{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"mnist/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train=mnist.train.num_examples\n",
    "num_val=mnist.validation.images.shape\n",
    "num_test=mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "img_size=28\n",
    "RNN_unit=img_size*img_size\n",
    "N_watch=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(dtype=tf.float32,shape=[None,img_size*img_size])\n",
    "y=tf.placeholder(dtype=tf.int64,shape=[None,10])\n",
    "\n",
    "predict_net=tf.layers.Dense(units=10)\n",
    "\n",
    "def get_next_input(output, i):\n",
    "    attention_weight=tf.nn.softmax(output)\n",
    "    weighted_graph=X*attention_weight\n",
    "    return weighted_graph\n",
    "\n",
    "lstm_cell = tf.nn.rnn_cell.LSTMCell(RNN_unit, state_is_tuple=True)\n",
    "init_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "inputs=[X]\n",
    "inputs.extend([0]*N_watch)\n",
    "outputs,_ = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, init_state, lstm_cell, loop_function=get_next_input)\n",
    "\n",
    "output=outputs[-1]\n",
    "score=predict_net(output)\n",
    "\n",
    "predictions = tf.argmax(score, 1)\n",
    "correct_predictions = tf.equal(predictions, tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "\n",
    "loss=tf.losses.softmax_cross_entropy(onehot_labels=y,logits=score)\n",
    "optimizier=tf.train.AdamOptimizer(learning_rate=1e-5)\n",
    "train_step = optimizier.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 16:51:12 start epoch 1/50:\n",
      "2018-04-13 16:51:12 iteration 1/859: current training loss = 2.301888\n",
      "2018-04-13 16:51:14 iteration 200/859: current training loss = 1.914014\n",
      "2018-04-13 16:51:16 iteration 400/859: current training loss = 0.965600\n",
      "2018-04-13 16:51:18 iteration 600/859: current training loss = 0.820855\n",
      "2018-04-13 16:51:20 iteration 800/859: current training loss = 0.303383\n",
      "2018-04-13 16:51:21 iteration 859/859: current training loss = 0.561343\n",
      "2018-04-13 16:51:36 end epoch 1/50: acc_train=84.823% acc_val=85.453% acc_test=85.469%\n",
      "2018-04-13 16:51:36 start epoch 2/50:\n",
      "2018-04-13 16:51:36 iteration 1/859: current training loss = 0.330166\n",
      "2018-04-13 16:51:38 iteration 200/859: current training loss = 0.483591\n",
      "2018-04-13 16:51:40 iteration 400/859: current training loss = 0.373152\n",
      "2018-04-13 16:51:42 iteration 600/859: current training loss = 0.546122\n",
      "2018-04-13 16:51:43 iteration 800/859: current training loss = 0.378881\n",
      "2018-04-13 16:51:44 iteration 859/859: current training loss = 0.316526\n",
      "2018-04-13 16:51:57 end epoch 2/50: acc_train=89.954% acc_val=90.322% acc_test=90.034%\n",
      "2018-04-13 16:51:57 start epoch 3/50:\n",
      "2018-04-13 16:51:57 iteration 1/859: current training loss = 0.319128\n",
      "2018-04-13 16:51:59 iteration 200/859: current training loss = 0.300726\n",
      "2018-04-13 16:52:01 iteration 400/859: current training loss = 0.276573\n",
      "2018-04-13 16:52:03 iteration 600/859: current training loss = 0.206598\n",
      "2018-04-13 16:52:06 iteration 800/859: current training loss = 0.102800\n",
      "2018-04-13 16:52:06 iteration 859/859: current training loss = 0.436467\n",
      "2018-04-13 16:52:20 end epoch 3/50: acc_train=91.760% acc_val=92.272% acc_test=91.816%\n",
      "2018-04-13 16:52:20 start epoch 4/50:\n",
      "2018-04-13 16:52:20 iteration 1/859: current training loss = 0.209397\n",
      "2018-04-13 16:52:22 iteration 200/859: current training loss = 0.250451\n",
      "2018-04-13 16:52:24 iteration 400/859: current training loss = 0.192215\n",
      "2018-04-13 16:52:26 iteration 600/859: current training loss = 0.280520\n",
      "2018-04-13 16:52:28 iteration 800/859: current training loss = 0.392432\n",
      "2018-04-13 16:52:29 iteration 859/859: current training loss = 0.119130\n",
      "2018-04-13 16:52:42 end epoch 4/50: acc_train=92.772% acc_val=93.034% acc_test=92.664%\n",
      "2018-04-13 16:52:42 start epoch 5/50:\n",
      "2018-04-13 16:52:42 iteration 1/859: current training loss = 0.365039\n",
      "2018-04-13 16:52:44 iteration 200/859: current training loss = 0.178489\n",
      "2018-04-13 16:52:46 iteration 400/859: current training loss = 0.211857\n",
      "2018-04-13 16:52:48 iteration 600/859: current training loss = 0.186500\n",
      "2018-04-13 16:52:50 iteration 800/859: current training loss = 0.301955\n",
      "2018-04-13 16:52:51 iteration 859/859: current training loss = 0.244455\n",
      "2018-04-13 16:53:04 end epoch 5/50: acc_train=93.463% acc_val=93.791% acc_test=93.189%\n",
      "2018-04-13 16:53:04 start epoch 6/50:\n",
      "2018-04-13 16:53:04 iteration 1/859: current training loss = 0.363011\n",
      "2018-04-13 16:53:06 iteration 200/859: current training loss = 0.148382\n",
      "2018-04-13 16:53:08 iteration 400/859: current training loss = 0.092140\n",
      "2018-04-13 16:53:10 iteration 600/859: current training loss = 0.315720\n",
      "2018-04-13 16:53:12 iteration 800/859: current training loss = 0.336614\n",
      "2018-04-13 16:53:12 iteration 859/859: current training loss = 0.451349\n",
      "2018-04-13 16:53:26 end epoch 6/50: acc_train=93.760% acc_val=93.953% acc_test=93.483%\n",
      "2018-04-13 16:53:26 start epoch 7/50:\n",
      "2018-04-13 16:53:26 iteration 1/859: current training loss = 0.260002\n",
      "2018-04-13 16:53:28 iteration 200/859: current training loss = 0.188231\n",
      "2018-04-13 16:53:30 iteration 400/859: current training loss = 0.061379\n",
      "2018-04-13 16:53:32 iteration 600/859: current training loss = 0.358783\n",
      "2018-04-13 16:53:34 iteration 800/859: current training loss = 0.254196\n",
      "2018-04-13 16:53:35 iteration 859/859: current training loss = 0.287069\n",
      "2018-04-13 16:53:49 end epoch 7/50: acc_train=94.397% acc_val=94.250% acc_test=93.955%\n",
      "2018-04-13 16:53:49 start epoch 8/50:\n",
      "2018-04-13 16:53:49 iteration 1/859: current training loss = 0.232513\n",
      "2018-04-13 16:53:51 iteration 200/859: current training loss = 0.132462\n",
      "2018-04-13 16:53:53 iteration 400/859: current training loss = 0.265642\n",
      "2018-04-13 16:53:55 iteration 600/859: current training loss = 0.102724\n",
      "2018-04-13 16:53:57 iteration 800/859: current training loss = 0.287660\n",
      "2018-04-13 16:53:57 iteration 859/859: current training loss = 0.265370\n",
      "2018-04-13 16:54:11 end epoch 8/50: acc_train=94.806% acc_val=94.853% acc_test=94.614%\n",
      "2018-04-13 16:54:11 start epoch 9/50:\n",
      "2018-04-13 16:54:11 iteration 1/859: current training loss = 0.191564\n",
      "2018-04-13 16:54:13 iteration 200/859: current training loss = 0.294698\n",
      "2018-04-13 16:54:15 iteration 400/859: current training loss = 0.045924\n",
      "2018-04-13 16:54:17 iteration 600/859: current training loss = 0.059943\n",
      "2018-04-13 16:54:19 iteration 800/859: current training loss = 0.060673\n",
      "2018-04-13 16:54:20 iteration 859/859: current training loss = 0.110194\n",
      "2018-04-13 16:54:34 end epoch 9/50: acc_train=95.171% acc_val=95.284% acc_test=94.784%\n",
      "2018-04-13 16:54:34 start epoch 10/50:\n",
      "2018-04-13 16:54:34 iteration 1/859: current training loss = 0.152227\n",
      "2018-04-13 16:54:36 iteration 200/859: current training loss = 0.071679\n",
      "2018-04-13 16:54:38 iteration 400/859: current training loss = 0.064757\n",
      "2018-04-13 16:54:40 iteration 600/859: current training loss = 0.149585\n",
      "2018-04-13 16:54:42 iteration 800/859: current training loss = 0.084968\n",
      "2018-04-13 16:54:43 iteration 859/859: current training loss = 0.168606\n",
      "2018-04-13 16:54:57 end epoch 10/50: acc_train=95.627% acc_val=95.509% acc_test=95.128%\n",
      "2018-04-13 16:54:57 start epoch 11/50:\n",
      "2018-04-13 16:54:57 iteration 1/859: current training loss = 0.154951\n",
      "2018-04-13 16:54:59 iteration 200/859: current training loss = 0.130963\n",
      "2018-04-13 16:55:01 iteration 400/859: current training loss = 0.146871\n",
      "2018-04-13 16:55:03 iteration 600/859: current training loss = 0.137230\n",
      "2018-04-13 16:55:05 iteration 800/859: current training loss = 0.086080\n",
      "2018-04-13 16:55:05 iteration 859/859: current training loss = 0.105370\n",
      "2018-04-13 16:55:18 end epoch 11/50: acc_train=95.893% acc_val=95.775% acc_test=95.522%\n",
      "2018-04-13 16:55:18 start epoch 12/50:\n",
      "2018-04-13 16:55:18 iteration 1/859: current training loss = 0.125242\n",
      "2018-04-13 16:55:19 iteration 200/859: current training loss = 0.212688\n",
      "2018-04-13 16:55:22 iteration 400/859: current training loss = 0.111282\n",
      "2018-04-13 16:55:23 iteration 600/859: current training loss = 0.275508\n",
      "2018-04-13 16:55:25 iteration 800/859: current training loss = 0.083915\n",
      "2018-04-13 16:55:26 iteration 859/859: current training loss = 0.063683\n",
      "2018-04-13 16:55:39 end epoch 12/50: acc_train=96.030% acc_val=95.487% acc_test=95.553%\n",
      "2018-04-13 16:55:39 start epoch 13/50:\n",
      "2018-04-13 16:55:39 iteration 1/859: current training loss = 0.148171\n",
      "2018-04-13 16:55:41 iteration 200/859: current training loss = 0.196520\n",
      "2018-04-13 16:55:43 iteration 400/859: current training loss = 0.187131\n",
      "2018-04-13 16:55:45 iteration 600/859: current training loss = 0.134720\n",
      "2018-04-13 16:55:47 iteration 800/859: current training loss = 0.281953\n",
      "2018-04-13 16:55:47 iteration 859/859: current training loss = 0.208940\n",
      "2018-04-13 16:56:00 end epoch 13/50: acc_train=96.496% acc_val=95.981% acc_test=95.852%\n",
      "2018-04-13 16:56:00 start epoch 14/50:\n",
      "2018-04-13 16:56:00 iteration 1/859: current training loss = 0.085983\n",
      "2018-04-13 16:56:02 iteration 200/859: current training loss = 0.075256\n",
      "2018-04-13 16:56:04 iteration 400/859: current training loss = 0.157332\n",
      "2018-04-13 16:56:06 iteration 600/859: current training loss = 0.102182\n",
      "2018-04-13 16:56:08 iteration 800/859: current training loss = 0.057647\n",
      "2018-04-13 16:56:09 iteration 859/859: current training loss = 0.043746\n",
      "2018-04-13 16:56:22 end epoch 14/50: acc_train=96.534% acc_val=96.056% acc_test=95.812%\n",
      "2018-04-13 16:56:22 start epoch 15/50:\n",
      "2018-04-13 16:56:22 iteration 1/859: current training loss = 0.094724\n",
      "2018-04-13 16:56:24 iteration 200/859: current training loss = 0.144720\n",
      "2018-04-13 16:56:26 iteration 400/859: current training loss = 0.178127\n",
      "2018-04-13 16:56:28 iteration 600/859: current training loss = 0.042338\n",
      "2018-04-13 16:56:30 iteration 800/859: current training loss = 0.118527\n",
      "2018-04-13 16:56:30 iteration 859/859: current training loss = 0.062773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 16:56:43 end epoch 15/50: acc_train=96.810% acc_val=96.194% acc_test=95.964%\n",
      "2018-04-13 16:56:43 start epoch 16/50:\n",
      "2018-04-13 16:56:43 iteration 1/859: current training loss = 0.090527\n",
      "2018-04-13 16:56:45 iteration 200/859: current training loss = 0.073734\n",
      "2018-04-13 16:56:47 iteration 400/859: current training loss = 0.100326\n",
      "2018-04-13 16:56:49 iteration 600/859: current training loss = 0.261580\n",
      "2018-04-13 16:56:51 iteration 800/859: current training loss = 0.286364\n",
      "2018-04-13 16:56:52 iteration 859/859: current training loss = 0.127040\n",
      "2018-04-13 16:57:04 end epoch 16/50: acc_train=96.996% acc_val=96.213% acc_test=96.231%\n",
      "2018-04-13 16:57:04 start epoch 17/50:\n",
      "2018-04-13 16:57:04 iteration 1/859: current training loss = 0.115799\n",
      "2018-04-13 16:57:06 iteration 200/859: current training loss = 0.180109\n",
      "2018-04-13 16:57:08 iteration 400/859: current training loss = 0.066585\n",
      "2018-04-13 16:57:09 iteration 600/859: current training loss = 0.056087\n",
      "2018-04-13 16:57:11 iteration 800/859: current training loss = 0.042638\n",
      "2018-04-13 16:57:12 iteration 859/859: current training loss = 0.321399\n",
      "2018-04-13 16:57:24 end epoch 17/50: acc_train=97.123% acc_val=96.319% acc_test=96.213%\n",
      "2018-04-13 16:57:24 start epoch 18/50:\n",
      "2018-04-13 16:57:24 iteration 1/859: current training loss = 0.093540\n",
      "2018-04-13 16:57:26 iteration 200/859: current training loss = 0.104686\n",
      "2018-04-13 16:57:28 iteration 400/859: current training loss = 0.176715\n",
      "2018-04-13 16:57:30 iteration 600/859: current training loss = 0.118789\n",
      "2018-04-13 16:57:32 iteration 800/859: current training loss = 0.030650\n",
      "2018-04-13 16:57:33 iteration 859/859: current training loss = 0.225726\n",
      "2018-04-13 16:57:45 end epoch 18/50: acc_train=97.263% acc_val=96.419% acc_test=96.316%\n",
      "2018-04-13 16:57:45 start epoch 19/50:\n",
      "2018-04-13 16:57:45 iteration 1/859: current training loss = 0.053860\n",
      "2018-04-13 16:57:47 iteration 200/859: current training loss = 0.056211\n",
      "2018-04-13 16:57:48 iteration 400/859: current training loss = 0.036359\n",
      "2018-04-13 16:57:50 iteration 600/859: current training loss = 0.079228\n",
      "2018-04-13 16:57:52 iteration 800/859: current training loss = 0.017940\n",
      "2018-04-13 16:57:52 iteration 859/859: current training loss = 0.043539\n",
      "2018-04-13 16:58:04 end epoch 19/50: acc_train=97.411% acc_val=96.722% acc_test=96.339%\n",
      "2018-04-13 16:58:04 start epoch 20/50:\n",
      "2018-04-13 16:58:04 iteration 1/859: current training loss = 0.157231\n",
      "2018-04-13 16:58:06 iteration 200/859: current training loss = 0.111721\n",
      "2018-04-13 16:58:08 iteration 400/859: current training loss = 0.124206\n",
      "2018-04-13 16:58:10 iteration 600/859: current training loss = 0.149138\n",
      "2018-04-13 16:58:12 iteration 800/859: current training loss = 0.102207\n",
      "2018-04-13 16:58:13 iteration 859/859: current training loss = 0.024930\n",
      "2018-04-13 16:58:24 end epoch 20/50: acc_train=97.554% acc_val=96.716% acc_test=96.344%\n",
      "2018-04-13 16:58:24 start epoch 21/50:\n",
      "2018-04-13 16:58:24 iteration 1/859: current training loss = 0.058380\n",
      "2018-04-13 16:58:26 iteration 200/859: current training loss = 0.036547\n",
      "2018-04-13 16:58:28 iteration 400/859: current training loss = 0.063444\n",
      "2018-04-13 16:58:30 iteration 600/859: current training loss = 0.198409\n",
      "2018-04-13 16:58:32 iteration 800/859: current training loss = 0.095351\n",
      "2018-04-13 16:58:33 iteration 859/859: current training loss = 0.035623\n",
      "2018-04-13 16:58:46 end epoch 21/50: acc_train=97.380% acc_val=96.656% acc_test=96.428%\n",
      "2018-04-13 16:58:46 start epoch 22/50:\n",
      "2018-04-13 16:58:46 iteration 1/859: current training loss = 0.130823\n",
      "2018-04-13 16:58:47 iteration 200/859: current training loss = 0.092587\n",
      "2018-04-13 16:58:49 iteration 400/859: current training loss = 0.103420\n",
      "2018-04-13 16:58:51 iteration 600/859: current training loss = 0.036021\n",
      "2018-04-13 16:58:53 iteration 800/859: current training loss = 0.150281\n",
      "2018-04-13 16:58:54 iteration 859/859: current training loss = 0.154640\n",
      "2018-04-13 16:59:06 end epoch 22/50: acc_train=97.778% acc_val=97.013% acc_test=96.419%\n",
      "2018-04-13 16:59:06 start epoch 23/50:\n",
      "2018-04-13 16:59:06 iteration 1/859: current training loss = 0.016486\n",
      "2018-04-13 16:59:08 iteration 200/859: current training loss = 0.112482\n",
      "2018-04-13 16:59:10 iteration 400/859: current training loss = 0.013382\n",
      "2018-04-13 16:59:12 iteration 600/859: current training loss = 0.045233\n",
      "2018-04-13 16:59:14 iteration 800/859: current training loss = 0.028382\n",
      "2018-04-13 16:59:14 iteration 859/859: current training loss = 0.127471\n",
      "2018-04-13 16:59:27 end epoch 23/50: acc_train=97.851% acc_val=96.938% acc_test=96.483%\n",
      "2018-04-13 16:59:27 start epoch 24/50:\n",
      "2018-04-13 16:59:27 iteration 1/859: current training loss = 0.098606\n",
      "2018-04-13 16:59:29 iteration 200/859: current training loss = 0.167554\n",
      "2018-04-13 16:59:31 iteration 400/859: current training loss = 0.015431\n",
      "2018-04-13 16:59:33 iteration 600/859: current training loss = 0.016102\n",
      "2018-04-13 16:59:35 iteration 800/859: current training loss = 0.060557\n",
      "2018-04-13 16:59:36 iteration 859/859: current training loss = 0.033270\n",
      "2018-04-13 16:59:48 end epoch 24/50: acc_train=97.949% acc_val=97.081% acc_test=96.536%\n",
      "2018-04-13 16:59:48 start epoch 25/50:\n",
      "2018-04-13 16:59:48 iteration 1/859: current training loss = 0.036983\n",
      "2018-04-13 16:59:49 iteration 200/859: current training loss = 0.010022\n",
      "2018-04-13 16:59:51 iteration 400/859: current training loss = 0.139048\n",
      "2018-04-13 16:59:53 iteration 600/859: current training loss = 0.027077\n",
      "2018-04-13 16:59:55 iteration 800/859: current training loss = 0.060535\n",
      "2018-04-13 16:59:56 iteration 859/859: current training loss = 0.104276\n",
      "2018-04-13 17:00:08 end epoch 25/50: acc_train=98.085% acc_val=97.300% acc_test=96.578%\n",
      "2018-04-13 17:00:08 start epoch 26/50:\n",
      "2018-04-13 17:00:08 iteration 1/859: current training loss = 0.064217\n",
      "2018-04-13 17:00:10 iteration 200/859: current training loss = 0.088853\n",
      "2018-04-13 17:00:12 iteration 400/859: current training loss = 0.052833\n",
      "2018-04-13 17:00:14 iteration 600/859: current training loss = 0.078770\n",
      "2018-04-13 17:00:16 iteration 800/859: current training loss = 0.127526\n",
      "2018-04-13 17:00:16 iteration 859/859: current training loss = 0.096108\n",
      "2018-04-13 17:00:28 end epoch 26/50: acc_train=98.043% acc_val=97.103% acc_test=96.675%\n",
      "2018-04-13 17:00:28 start epoch 27/50:\n",
      "2018-04-13 17:00:28 iteration 1/859: current training loss = 0.090007\n",
      "2018-04-13 17:00:30 iteration 200/859: current training loss = 0.027169\n",
      "2018-04-13 17:00:31 iteration 400/859: current training loss = 0.050888\n",
      "2018-04-13 17:00:33 iteration 600/859: current training loss = 0.111792\n",
      "2018-04-13 17:00:35 iteration 800/859: current training loss = 0.047104\n",
      "2018-04-13 17:00:35 iteration 859/859: current training loss = 0.107131\n",
      "2018-04-13 17:00:47 end epoch 27/50: acc_train=97.910% acc_val=96.762% acc_test=96.523%\n",
      "2018-04-13 17:00:47 start epoch 28/50:\n",
      "2018-04-13 17:00:47 iteration 1/859: current training loss = 0.068504\n",
      "2018-04-13 17:00:49 iteration 200/859: current training loss = 0.043129\n",
      "2018-04-13 17:00:51 iteration 400/859: current training loss = 0.118043\n",
      "2018-04-13 17:00:53 iteration 600/859: current training loss = 0.129254\n",
      "2018-04-13 17:00:55 iteration 800/859: current training loss = 0.048124\n",
      "2018-04-13 17:00:55 iteration 859/859: current training loss = 0.170280\n",
      "2018-04-13 17:01:08 end epoch 28/50: acc_train=98.259% acc_val=97.209% acc_test=96.711%\n",
      "2018-04-13 17:01:08 start epoch 29/50:\n",
      "2018-04-13 17:01:08 iteration 1/859: current training loss = 0.055405\n",
      "2018-04-13 17:01:10 iteration 200/859: current training loss = 0.017848\n",
      "2018-04-13 17:01:12 iteration 400/859: current training loss = 0.018481\n",
      "2018-04-13 17:01:14 iteration 600/859: current training loss = 0.003829\n",
      "2018-04-13 17:01:16 iteration 800/859: current training loss = 0.027992\n",
      "2018-04-13 17:01:17 iteration 859/859: current training loss = 0.176157\n",
      "2018-04-13 17:01:29 end epoch 29/50: acc_train=98.436% acc_val=97.297% acc_test=96.908%\n",
      "2018-04-13 17:01:29 start epoch 30/50:\n",
      "2018-04-13 17:01:29 iteration 1/859: current training loss = 0.030848\n",
      "2018-04-13 17:01:31 iteration 200/859: current training loss = 0.068215\n",
      "2018-04-13 17:01:33 iteration 400/859: current training loss = 0.026414\n",
      "2018-04-13 17:01:35 iteration 600/859: current training loss = 0.043519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 17:01:37 iteration 800/859: current training loss = 0.108231\n",
      "2018-04-13 17:01:37 iteration 859/859: current training loss = 0.070917\n",
      "2018-04-13 17:01:50 end epoch 30/50: acc_train=98.468% acc_val=97.278% acc_test=96.791%\n",
      "2018-04-13 17:01:50 start epoch 31/50:\n",
      "2018-04-13 17:01:50 iteration 1/859: current training loss = 0.017432\n",
      "2018-04-13 17:01:52 iteration 200/859: current training loss = 0.108070\n",
      "2018-04-13 17:01:54 iteration 400/859: current training loss = 0.065203\n",
      "2018-04-13 17:01:56 iteration 600/859: current training loss = 0.071679\n",
      "2018-04-13 17:01:58 iteration 800/859: current training loss = 0.058605\n",
      "2018-04-13 17:01:59 iteration 859/859: current training loss = 0.013860\n",
      "2018-04-13 17:02:12 end epoch 31/50: acc_train=98.312% acc_val=97.172% acc_test=96.873%\n",
      "2018-04-13 17:02:12 start epoch 32/50:\n",
      "2018-04-13 17:02:12 iteration 1/859: current training loss = 0.072733\n",
      "2018-04-13 17:02:14 iteration 200/859: current training loss = 0.020371\n",
      "2018-04-13 17:02:16 iteration 400/859: current training loss = 0.041250\n",
      "2018-04-13 17:02:17 iteration 600/859: current training loss = 0.019162\n",
      "2018-04-13 17:02:20 iteration 800/859: current training loss = 0.192794\n",
      "2018-04-13 17:02:20 iteration 859/859: current training loss = 0.010706\n",
      "2018-04-13 17:02:33 end epoch 32/50: acc_train=98.523% acc_val=97.431% acc_test=96.894%\n",
      "2018-04-13 17:02:33 start epoch 33/50:\n",
      "2018-04-13 17:02:33 iteration 1/859: current training loss = 0.020811\n",
      "2018-04-13 17:02:34 iteration 200/859: current training loss = 0.003097\n",
      "2018-04-13 17:02:36 iteration 400/859: current training loss = 0.077068\n",
      "2018-04-13 17:02:38 iteration 600/859: current training loss = 0.040642\n",
      "2018-04-13 17:02:40 iteration 800/859: current training loss = 0.026608\n",
      "2018-04-13 17:02:41 iteration 859/859: current training loss = 0.095723\n",
      "2018-04-13 17:02:54 end epoch 33/50: acc_train=98.733% acc_val=97.428% acc_test=96.991%\n",
      "2018-04-13 17:02:54 start epoch 34/50:\n",
      "2018-04-13 17:02:54 iteration 1/859: current training loss = 0.025079\n",
      "2018-04-13 17:02:56 iteration 200/859: current training loss = 0.022381\n",
      "2018-04-13 17:02:58 iteration 400/859: current training loss = 0.019215\n",
      "2018-04-13 17:03:00 iteration 600/859: current training loss = 0.024442\n",
      "2018-04-13 17:03:01 iteration 800/859: current training loss = 0.088846\n",
      "2018-04-13 17:03:02 iteration 859/859: current training loss = 0.063729\n",
      "2018-04-13 17:03:15 end epoch 34/50: acc_train=98.641% acc_val=97.197% acc_test=96.917%\n",
      "2018-04-13 17:03:15 start epoch 35/50:\n",
      "2018-04-13 17:03:15 iteration 1/859: current training loss = 0.006902\n",
      "2018-04-13 17:03:17 iteration 200/859: current training loss = 0.032953\n",
      "2018-04-13 17:03:19 iteration 400/859: current training loss = 0.038039\n",
      "2018-04-13 17:03:21 iteration 600/859: current training loss = 0.047084\n",
      "2018-04-13 17:03:23 iteration 800/859: current training loss = 0.009271\n",
      "2018-04-13 17:03:24 iteration 859/859: current training loss = 0.025606\n",
      "2018-04-13 17:03:37 end epoch 35/50: acc_train=98.848% acc_val=97.466% acc_test=97.155%\n",
      "2018-04-13 17:03:37 start epoch 36/50:\n",
      "2018-04-13 17:03:38 iteration 1/859: current training loss = 0.028393\n",
      "2018-04-13 17:03:39 iteration 200/859: current training loss = 0.013505\n",
      "2018-04-13 17:03:41 iteration 400/859: current training loss = 0.010488\n",
      "2018-04-13 17:03:43 iteration 600/859: current training loss = 0.094353\n",
      "2018-04-13 17:03:45 iteration 800/859: current training loss = 0.069842\n",
      "2018-04-13 17:03:46 iteration 859/859: current training loss = 0.015887\n",
      "2018-04-13 17:03:58 end epoch 36/50: acc_train=98.765% acc_val=97.491% acc_test=97.095%\n",
      "2018-04-13 17:03:58 start epoch 37/50:\n",
      "2018-04-13 17:03:58 iteration 1/859: current training loss = 0.011415\n",
      "2018-04-13 17:04:00 iteration 200/859: current training loss = 0.035829\n",
      "2018-04-13 17:04:02 iteration 400/859: current training loss = 0.026866\n",
      "2018-04-13 17:04:04 iteration 600/859: current training loss = 0.027569\n",
      "2018-04-13 17:04:06 iteration 800/859: current training loss = 0.032529\n",
      "2018-04-13 17:04:06 iteration 859/859: current training loss = 0.065076\n",
      "2018-04-13 17:04:18 end epoch 37/50: acc_train=98.728% acc_val=97.491% acc_test=97.089%\n",
      "2018-04-13 17:04:18 start epoch 38/50:\n",
      "2018-04-13 17:04:18 iteration 1/859: current training loss = 0.015225\n",
      "2018-04-13 17:04:20 iteration 200/859: current training loss = 0.068158\n",
      "2018-04-13 17:04:22 iteration 400/859: current training loss = 0.060499\n",
      "2018-04-13 17:04:23 iteration 600/859: current training loss = 0.046777\n",
      "2018-04-13 17:04:25 iteration 800/859: current training loss = 0.016587\n",
      "2018-04-13 17:04:26 iteration 859/859: current training loss = 0.013724\n",
      "2018-04-13 17:04:39 end epoch 38/50: acc_train=98.944% acc_val=97.331% acc_test=97.048%\n",
      "2018-04-13 17:04:39 start epoch 39/50:\n",
      "2018-04-13 17:04:39 iteration 1/859: current training loss = 0.008811\n",
      "2018-04-13 17:04:41 iteration 200/859: current training loss = 0.060148\n",
      "2018-04-13 17:04:43 iteration 400/859: current training loss = 0.008142\n",
      "2018-04-13 17:04:45 iteration 600/859: current training loss = 0.135712\n",
      "2018-04-13 17:04:48 iteration 800/859: current training loss = 0.003568\n",
      "2018-04-13 17:04:48 iteration 859/859: current training loss = 0.075919\n",
      "2018-04-13 17:05:02 end epoch 39/50: acc_train=99.076% acc_val=97.250% acc_test=97.281%\n",
      "2018-04-13 17:05:02 start epoch 40/50:\n",
      "2018-04-13 17:05:02 iteration 1/859: current training loss = 0.086551\n",
      "2018-04-13 17:05:03 iteration 200/859: current training loss = 0.078687\n",
      "2018-04-13 17:05:05 iteration 400/859: current training loss = 0.017403\n",
      "2018-04-13 17:05:07 iteration 600/859: current training loss = 0.030094\n",
      "2018-04-13 17:05:09 iteration 800/859: current training loss = 0.156596\n",
      "2018-04-13 17:05:10 iteration 859/859: current training loss = 0.004232\n",
      "2018-04-13 17:05:23 end epoch 40/50: acc_train=99.111% acc_val=97.609% acc_test=97.366%\n",
      "2018-04-13 17:05:23 start epoch 41/50:\n",
      "2018-04-13 17:05:23 iteration 1/859: current training loss = 0.003584\n",
      "2018-04-13 17:05:25 iteration 200/859: current training loss = 0.019519\n",
      "2018-04-13 17:05:26 iteration 400/859: current training loss = 0.001995\n",
      "2018-04-13 17:05:28 iteration 600/859: current training loss = 0.071827\n",
      "2018-04-13 17:05:30 iteration 800/859: current training loss = 0.020237\n",
      "2018-04-13 17:05:31 iteration 859/859: current training loss = 0.026814\n",
      "2018-04-13 17:05:43 end epoch 41/50: acc_train=99.159% acc_val=97.641% acc_test=97.314%\n",
      "2018-04-13 17:05:43 start epoch 42/50:\n",
      "2018-04-13 17:05:43 iteration 1/859: current training loss = 0.014158\n",
      "2018-04-13 17:05:45 iteration 200/859: current training loss = 0.002416\n",
      "2018-04-13 17:05:47 iteration 400/859: current training loss = 0.009751\n",
      "2018-04-13 17:05:49 iteration 600/859: current training loss = 0.027300\n",
      "2018-04-13 17:05:51 iteration 800/859: current training loss = 0.025231\n",
      "2018-04-13 17:05:51 iteration 859/859: current training loss = 0.005803\n",
      "2018-04-13 17:06:04 end epoch 42/50: acc_train=99.196% acc_val=97.531% acc_test=97.211%\n",
      "2018-04-13 17:06:04 start epoch 43/50:\n",
      "2018-04-13 17:06:04 iteration 1/859: current training loss = 0.016810\n",
      "2018-04-13 17:06:06 iteration 200/859: current training loss = 0.109966\n",
      "2018-04-13 17:06:08 iteration 400/859: current training loss = 0.051402\n",
      "2018-04-13 17:06:09 iteration 600/859: current training loss = 0.012984\n",
      "2018-04-13 17:06:11 iteration 800/859: current training loss = 0.066330\n",
      "2018-04-13 17:06:12 iteration 859/859: current training loss = 0.041081\n",
      "2018-04-13 17:06:25 end epoch 43/50: acc_train=99.205% acc_val=97.453% acc_test=97.208%\n",
      "2018-04-13 17:06:25 start epoch 44/50:\n",
      "2018-04-13 17:06:25 iteration 1/859: current training loss = 0.004985\n",
      "2018-04-13 17:06:27 iteration 200/859: current training loss = 0.005467\n",
      "2018-04-13 17:06:29 iteration 400/859: current training loss = 0.050074\n",
      "2018-04-13 17:06:31 iteration 600/859: current training loss = 0.086600\n",
      "2018-04-13 17:06:33 iteration 800/859: current training loss = 0.008824\n",
      "2018-04-13 17:06:33 iteration 859/859: current training loss = 0.009338\n",
      "2018-04-13 17:06:46 end epoch 44/50: acc_train=99.308% acc_val=97.594% acc_test=97.364%\n",
      "2018-04-13 17:06:46 start epoch 45/50:\n",
      "2018-04-13 17:06:46 iteration 1/859: current training loss = 0.140356\n",
      "2018-04-13 17:06:48 iteration 200/859: current training loss = 0.014109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 17:06:49 iteration 400/859: current training loss = 0.030285\n",
      "2018-04-13 17:06:52 iteration 600/859: current training loss = 0.009039\n",
      "2018-04-13 17:06:53 iteration 800/859: current training loss = 0.003283\n",
      "2018-04-13 17:06:54 iteration 859/859: current training loss = 0.014403\n",
      "2018-04-13 17:07:08 end epoch 45/50: acc_train=99.016% acc_val=97.597% acc_test=97.078%\n",
      "2018-04-13 17:07:08 start epoch 46/50:\n",
      "2018-04-13 17:07:08 iteration 1/859: current training loss = 0.011757\n",
      "2018-04-13 17:07:10 iteration 200/859: current training loss = 0.021205\n",
      "2018-04-13 17:07:12 iteration 400/859: current training loss = 0.090930\n",
      "2018-04-13 17:07:14 iteration 600/859: current training loss = 0.038537\n",
      "2018-04-13 17:07:16 iteration 800/859: current training loss = 0.019416\n",
      "2018-04-13 17:07:16 iteration 859/859: current training loss = 0.083729\n",
      "2018-04-13 17:07:30 end epoch 46/50: acc_train=99.335% acc_val=97.456% acc_test=97.256%\n",
      "2018-04-13 17:07:30 start epoch 47/50:\n",
      "2018-04-13 17:07:30 iteration 1/859: current training loss = 0.082589\n",
      "2018-04-13 17:07:32 iteration 200/859: current training loss = 0.004331\n",
      "2018-04-13 17:07:34 iteration 400/859: current training loss = 0.060177\n",
      "2018-04-13 17:07:36 iteration 600/859: current training loss = 0.035126\n",
      "2018-04-13 17:07:37 iteration 800/859: current training loss = 0.001768\n",
      "2018-04-13 17:07:38 iteration 859/859: current training loss = 0.032765\n",
      "2018-04-13 17:07:51 end epoch 47/50: acc_train=99.480% acc_val=97.728% acc_test=97.388%\n",
      "2018-04-13 17:07:51 start epoch 48/50:\n",
      "2018-04-13 17:07:51 iteration 1/859: current training loss = 0.004047\n",
      "2018-04-13 17:07:53 iteration 200/859: current training loss = 0.024345\n",
      "2018-04-13 17:07:55 iteration 400/859: current training loss = 0.013254\n",
      "2018-04-13 17:07:57 iteration 600/859: current training loss = 0.016814\n",
      "2018-04-13 17:07:59 iteration 800/859: current training loss = 0.028191\n",
      "2018-04-13 17:07:59 iteration 859/859: current training loss = 0.002083\n",
      "2018-04-13 17:08:12 end epoch 48/50: acc_train=99.509% acc_val=97.628% acc_test=97.516%\n",
      "2018-04-13 17:08:12 start epoch 49/50:\n",
      "2018-04-13 17:08:12 iteration 1/859: current training loss = 0.025685\n",
      "2018-04-13 17:08:13 iteration 200/859: current training loss = 0.002608\n",
      "2018-04-13 17:08:16 iteration 400/859: current training loss = 0.070057\n",
      "2018-04-13 17:08:17 iteration 600/859: current training loss = 0.003278\n",
      "2018-04-13 17:08:19 iteration 800/859: current training loss = 0.030417\n",
      "2018-04-13 17:08:20 iteration 859/859: current training loss = 0.004754\n",
      "2018-04-13 17:08:33 end epoch 49/50: acc_train=99.367% acc_val=97.547% acc_test=97.402%\n",
      "2018-04-13 17:08:33 start epoch 50/50:\n",
      "2018-04-13 17:08:33 iteration 1/859: current training loss = 0.016708\n",
      "2018-04-13 17:08:35 iteration 200/859: current training loss = 0.016105\n",
      "2018-04-13 17:08:37 iteration 400/859: current training loss = 0.004070\n",
      "2018-04-13 17:08:39 iteration 600/859: current training loss = 0.040452\n",
      "2018-04-13 17:08:41 iteration 800/859: current training loss = 0.050243\n",
      "2018-04-13 17:08:41 iteration 859/859: current training loss = 0.006667\n",
      "2018-04-13 17:08:54 end epoch 50/50: acc_train=99.426% acc_val=97.769% acc_test=97.413%\n"
     ]
    }
   ],
   "source": [
    "max_epoch=50\n",
    "print_every=200\n",
    "\n",
    "def train():\n",
    "    num_iteration=num_train//batch_size\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=mnist.train.next_batch(batch_size)\n",
    "        loss_num,_ = sess.run([loss,train_step],feed_dict={X:images,y:labels})\n",
    "        if it==0 or (it+1)%print_every==0 or it==num_iteration-1:\n",
    "            print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "                  'iteration %d/%d:' % (it+1,num_iteration),'current training loss = %f' % (loss_num))\n",
    "            \n",
    "def eval(dataset,num_iteration):\n",
    "    total_loss=0\n",
    "    total_accuracy=0\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=dataset.next_batch(batch_size)\n",
    "        loss_num,accuracy_num = sess.run([loss,accuracy],feed_dict={X:images,y:labels})\n",
    "        total_loss+=loss_num\n",
    "        total_accuracy+=accuracy_num\n",
    "    total_loss/=num_iteration\n",
    "    total_accuracy/=num_iteration\n",
    "    return total_loss,total_accuracy\n",
    "    \n",
    "acc_train_his=[]\n",
    "acc_val_his=[]\n",
    "acc_test_his=[]\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(max_epoch):\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'start epoch %d/%d:' % (epoch+1,max_epoch))\n",
    "        train()\n",
    "        loss_train,acc_train=eval(mnist.train,2000)\n",
    "        loss_val,acc_val=eval(mnist.validation,500)\n",
    "        loss_test,acc_test=eval(mnist.test,1000)\n",
    "        acc_train_his.append(acc_train)\n",
    "        acc_val_his.append(acc_val)\n",
    "        acc_test_his.append(acc_test)\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'end epoch %d/%d:' % (epoch+1,max_epoch),\n",
    "             'acc_train=%.3f%% acc_val=%.3f%% acc_test=%.3f%%' % (acc_train*100.0,acc_val*100.0,acc_test*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(2)\n",
    "ptr,=plt.plot(range(max_epoch),acc_train_his,'r-')\n",
    "pva,=plt.plot(range(max_epoch),acc_val_his,'b-')\n",
    "pte,=plt.plot(range(max_epoch),acc_test_his,'y-')\n",
    "plt.xlabel('training epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('accuracy on three sets')\n",
    "plt.legend((ptr,pva,pte),('train','validation','test'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
