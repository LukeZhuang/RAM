{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"mnist/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train=mnist.train.num_examples\n",
    "num_val=mnist.validation.images.shape\n",
    "num_test=mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "img_size=28\n",
    "RNN_unit=img_size*img_size\n",
    "N_watch=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(dtype=tf.float32,shape=[None,img_size*img_size])\n",
    "y=tf.placeholder(dtype=tf.int64,shape=[None,10])\n",
    "\n",
    "predict_net=tf.layers.Dense(units=10)\n",
    "\n",
    "def get_next_input(output, i):\n",
    "    attention_weight=tf.nn.softmax(output)\n",
    "    weighted_graph=X*attention_weight\n",
    "    return weighted_graph\n",
    "\n",
    "lstm_cell = tf.nn.rnn_cell.LSTMCell(RNN_unit, state_is_tuple=True)\n",
    "init_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "inputs=[X]\n",
    "inputs.extend([0]*N_watch)\n",
    "outputs,_ = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, init_state, lstm_cell, loop_function=get_next_input)\n",
    "\n",
    "output=outputs[-1]\n",
    "score=predict_net(output)\n",
    "\n",
    "predictions = tf.argmax(score, 1)\n",
    "correct_predictions = tf.equal(predictions, tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "\n",
    "loss=tf.losses.softmax_cross_entropy(onehot_labels=y,logits=score)\n",
    "optimizier=tf.train.AdamOptimizer(learning_rate=1e-5)\n",
    "train_step = optimizier.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16 14:38:17 start epoch 1/50:\n",
      "2018-04-16 14:38:19 iteration 1/859: current training loss = 2.304162\n",
      "2018-04-16 14:38:20 iteration 200/859: current training loss = 2.060483\n",
      "2018-04-16 14:38:22 iteration 400/859: current training loss = 0.852045\n",
      "2018-04-16 14:38:24 iteration 600/859: current training loss = 0.758529\n",
      "2018-04-16 14:38:26 iteration 800/859: current training loss = 0.398900\n",
      "2018-04-16 14:38:27 iteration 859/859: current training loss = 0.761768\n",
      "2018-04-16 14:38:42 end epoch 1/50: acc_train=83.753% acc_val=84.531% acc_test=83.948%\n",
      "2018-04-16 14:38:42 start epoch 2/50:\n",
      "2018-04-16 14:38:42 iteration 1/859: current training loss = 0.553279\n",
      "2018-04-16 14:38:44 iteration 200/859: current training loss = 0.649777\n",
      "2018-04-16 14:38:46 iteration 400/859: current training loss = 0.252843\n",
      "2018-04-16 14:38:49 iteration 600/859: current training loss = 0.257547\n",
      "2018-04-16 14:38:51 iteration 800/859: current training loss = 0.457916\n",
      "2018-04-16 14:38:52 iteration 859/859: current training loss = 0.283609\n",
      "2018-04-16 14:39:07 end epoch 2/50: acc_train=89.706% acc_val=90.188% acc_test=89.839%\n",
      "2018-04-16 14:39:07 start epoch 3/50:\n",
      "2018-04-16 14:39:07 iteration 1/859: current training loss = 0.373169\n",
      "2018-04-16 14:39:09 iteration 200/859: current training loss = 0.145202\n",
      "2018-04-16 14:39:11 iteration 400/859: current training loss = 0.374510\n",
      "2018-04-16 14:39:13 iteration 600/859: current training loss = 0.367569\n",
      "2018-04-16 14:39:15 iteration 800/859: current training loss = 0.232323\n",
      "2018-04-16 14:39:16 iteration 859/859: current training loss = 0.377084\n",
      "2018-04-16 14:39:30 end epoch 3/50: acc_train=91.561% acc_val=92.022% acc_test=91.505%\n",
      "2018-04-16 14:39:30 start epoch 4/50:\n",
      "2018-04-16 14:39:30 iteration 1/859: current training loss = 0.190094\n",
      "2018-04-16 14:39:32 iteration 200/859: current training loss = 0.160915\n",
      "2018-04-16 14:39:34 iteration 400/859: current training loss = 0.436934\n",
      "2018-04-16 14:39:36 iteration 600/859: current training loss = 0.253660\n",
      "2018-04-16 14:39:38 iteration 800/859: current training loss = 0.166054\n",
      "2018-04-16 14:39:38 iteration 859/859: current training loss = 0.115838\n",
      "2018-04-16 14:39:51 end epoch 4/50: acc_train=92.680% acc_val=92.906% acc_test=92.484%\n",
      "2018-04-16 14:39:51 start epoch 5/50:\n",
      "2018-04-16 14:39:51 iteration 1/859: current training loss = 0.327842\n",
      "2018-04-16 14:39:54 iteration 200/859: current training loss = 0.200211\n",
      "2018-04-16 14:39:56 iteration 400/859: current training loss = 0.359055\n",
      "2018-04-16 14:39:59 iteration 600/859: current training loss = 0.313231\n",
      "2018-04-16 14:40:01 iteration 800/859: current training loss = 0.138964\n",
      "2018-04-16 14:40:01 iteration 859/859: current training loss = 0.133465\n",
      "2018-04-16 14:40:14 end epoch 5/50: acc_train=93.397% acc_val=93.453% acc_test=93.114%\n",
      "2018-04-16 14:40:14 start epoch 6/50:\n",
      "2018-04-16 14:40:14 iteration 1/859: current training loss = 0.220033\n",
      "2018-04-16 14:40:16 iteration 200/859: current training loss = 0.163120\n",
      "2018-04-16 14:40:18 iteration 400/859: current training loss = 0.181490\n",
      "2018-04-16 14:40:20 iteration 600/859: current training loss = 0.337121\n",
      "2018-04-16 14:40:22 iteration 800/859: current training loss = 0.211513\n",
      "2018-04-16 14:40:23 iteration 859/859: current training loss = 0.186772\n",
      "2018-04-16 14:40:38 end epoch 6/50: acc_train=93.949% acc_val=94.003% acc_test=93.753%\n",
      "2018-04-16 14:40:38 start epoch 7/50:\n",
      "2018-04-16 14:40:38 iteration 1/859: current training loss = 0.140350\n",
      "2018-04-16 14:40:40 iteration 200/859: current training loss = 0.089892\n",
      "2018-04-16 14:40:43 iteration 400/859: current training loss = 0.249628\n",
      "2018-04-16 14:40:45 iteration 600/859: current training loss = 0.092817\n",
      "2018-04-16 14:40:47 iteration 800/859: current training loss = 0.174963\n",
      "2018-04-16 14:40:47 iteration 859/859: current training loss = 0.087783\n",
      "2018-04-16 14:41:01 end epoch 7/50: acc_train=94.493% acc_val=94.594% acc_test=94.163%\n",
      "2018-04-16 14:41:01 start epoch 8/50:\n",
      "2018-04-16 14:41:01 iteration 1/859: current training loss = 0.140643\n",
      "2018-04-16 14:41:03 iteration 200/859: current training loss = 0.262112\n",
      "2018-04-16 14:41:05 iteration 400/859: current training loss = 0.219516\n",
      "2018-04-16 14:41:07 iteration 600/859: current training loss = 0.206210\n",
      "2018-04-16 14:41:10 iteration 800/859: current training loss = 0.144118\n",
      "2018-04-16 14:41:11 iteration 859/859: current training loss = 0.071785\n",
      "2018-04-16 14:41:24 end epoch 8/50: acc_train=94.922% acc_val=95.009% acc_test=94.384%\n",
      "2018-04-16 14:41:24 start epoch 9/50:\n",
      "2018-04-16 14:41:24 iteration 1/859: current training loss = 0.176468\n",
      "2018-04-16 14:41:26 iteration 200/859: current training loss = 0.194276\n",
      "2018-04-16 14:41:28 iteration 400/859: current training loss = 0.116197\n",
      "2018-04-16 14:41:30 iteration 600/859: current training loss = 0.210474\n",
      "2018-04-16 14:41:33 iteration 800/859: current training loss = 0.113477\n",
      "2018-04-16 14:41:33 iteration 859/859: current training loss = 0.207636\n",
      "2018-04-16 14:41:46 end epoch 9/50: acc_train=95.245% acc_val=95.138% acc_test=94.903%\n",
      "2018-04-16 14:41:46 start epoch 10/50:\n",
      "2018-04-16 14:41:46 iteration 1/859: current training loss = 0.128485\n",
      "2018-04-16 14:41:49 iteration 200/859: current training loss = 0.201156\n",
      "2018-04-16 14:41:51 iteration 400/859: current training loss = 0.200725\n",
      "2018-04-16 14:41:52 iteration 600/859: current training loss = 0.069674\n",
      "2018-04-16 14:41:54 iteration 800/859: current training loss = 0.108471\n",
      "2018-04-16 14:41:55 iteration 859/859: current training loss = 0.306655\n",
      "2018-04-16 14:42:08 end epoch 10/50: acc_train=95.566% acc_val=95.422% acc_test=95.098%\n",
      "2018-04-16 14:42:08 start epoch 11/50:\n",
      "2018-04-16 14:42:08 iteration 1/859: current training loss = 0.206046\n",
      "2018-04-16 14:42:10 iteration 200/859: current training loss = 0.043875\n",
      "2018-04-16 14:42:12 iteration 400/859: current training loss = 0.258687\n",
      "2018-04-16 14:42:14 iteration 600/859: current training loss = 0.158437\n",
      "2018-04-16 14:42:17 iteration 800/859: current training loss = 0.045796\n",
      "2018-04-16 14:42:18 iteration 859/859: current training loss = 0.085499\n",
      "2018-04-16 14:42:31 end epoch 11/50: acc_train=95.749% acc_val=95.534% acc_test=95.184%\n",
      "2018-04-16 14:42:31 start epoch 12/50:\n",
      "2018-04-16 14:42:31 iteration 1/859: current training loss = 0.160084\n",
      "2018-04-16 14:42:33 iteration 200/859: current training loss = 0.092121\n",
      "2018-04-16 14:42:35 iteration 400/859: current training loss = 0.127773\n",
      "2018-04-16 14:42:38 iteration 600/859: current training loss = 0.114198\n",
      "2018-04-16 14:42:40 iteration 800/859: current training loss = 0.212370\n",
      "2018-04-16 14:42:40 iteration 859/859: current training loss = 0.131480\n",
      "2018-04-16 14:42:53 end epoch 12/50: acc_train=95.835% acc_val=95.453% acc_test=95.319%\n",
      "2018-04-16 14:42:53 start epoch 13/50:\n",
      "2018-04-16 14:42:53 iteration 1/859: current training loss = 0.066672\n",
      "2018-04-16 14:42:55 iteration 200/859: current training loss = 0.051435\n",
      "2018-04-16 14:42:57 iteration 400/859: current training loss = 0.088103\n",
      "2018-04-16 14:42:59 iteration 600/859: current training loss = 0.171868\n",
      "2018-04-16 14:43:01 iteration 800/859: current training loss = 0.088620\n",
      "2018-04-16 14:43:02 iteration 859/859: current training loss = 0.134353\n",
      "2018-04-16 14:43:15 end epoch 13/50: acc_train=96.410% acc_val=95.991% acc_test=95.811%\n",
      "2018-04-16 14:43:15 start epoch 14/50:\n",
      "2018-04-16 14:43:15 iteration 1/859: current training loss = 0.058880\n",
      "2018-04-16 14:43:17 iteration 200/859: current training loss = 0.244709\n",
      "2018-04-16 14:43:19 iteration 400/859: current training loss = 0.048372\n",
      "2018-04-16 14:43:21 iteration 600/859: current training loss = 0.240376\n",
      "2018-04-16 14:43:24 iteration 800/859: current training loss = 0.036784\n",
      "2018-04-16 14:43:24 iteration 859/859: current training loss = 0.124602\n",
      "2018-04-16 14:43:38 end epoch 14/50: acc_train=96.620% acc_val=95.972% acc_test=95.744%\n",
      "2018-04-16 14:43:38 start epoch 15/50:\n",
      "2018-04-16 14:43:38 iteration 1/859: current training loss = 0.126398\n",
      "2018-04-16 14:43:40 iteration 200/859: current training loss = 0.146863\n",
      "2018-04-16 14:43:42 iteration 400/859: current training loss = 0.167252\n",
      "2018-04-16 14:43:44 iteration 600/859: current training loss = 0.121626\n",
      "2018-04-16 14:43:46 iteration 800/859: current training loss = 0.053045\n",
      "2018-04-16 14:43:46 iteration 859/859: current training loss = 0.089091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16 14:43:58 end epoch 15/50: acc_train=96.855% acc_val=96.116% acc_test=96.116%\n",
      "2018-04-16 14:43:58 start epoch 16/50:\n",
      "2018-04-16 14:43:58 iteration 1/859: current training loss = 0.140693\n",
      "2018-04-16 14:44:01 iteration 200/859: current training loss = 0.067489\n",
      "2018-04-16 14:44:03 iteration 400/859: current training loss = 0.097634\n",
      "2018-04-16 14:44:04 iteration 600/859: current training loss = 0.092372\n",
      "2018-04-16 14:44:06 iteration 800/859: current training loss = 0.067582\n",
      "2018-04-16 14:44:07 iteration 859/859: current training loss = 0.117369\n",
      "2018-04-16 14:44:21 end epoch 16/50: acc_train=96.993% acc_val=96.272% acc_test=96.027%\n",
      "2018-04-16 14:44:21 start epoch 17/50:\n",
      "2018-04-16 14:44:21 iteration 1/859: current training loss = 0.142518\n",
      "2018-04-16 14:44:23 iteration 200/859: current training loss = 0.121414\n",
      "2018-04-16 14:44:24 iteration 400/859: current training loss = 0.025635\n",
      "2018-04-16 14:44:26 iteration 600/859: current training loss = 0.193744\n",
      "2018-04-16 14:44:29 iteration 800/859: current training loss = 0.140024\n",
      "2018-04-16 14:44:29 iteration 859/859: current training loss = 0.067243\n",
      "2018-04-16 14:44:41 end epoch 17/50: acc_train=97.184% acc_val=96.372% acc_test=96.208%\n",
      "2018-04-16 14:44:41 start epoch 18/50:\n",
      "2018-04-16 14:44:41 iteration 1/859: current training loss = 0.026020\n",
      "2018-04-16 14:44:43 iteration 200/859: current training loss = 0.051782\n",
      "2018-04-16 14:44:46 iteration 400/859: current training loss = 0.036810\n",
      "2018-04-16 14:44:48 iteration 600/859: current training loss = 0.062829\n",
      "2018-04-16 14:44:50 iteration 800/859: current training loss = 0.059742\n",
      "2018-04-16 14:44:50 iteration 859/859: current training loss = 0.037006\n",
      "2018-04-16 14:45:03 end epoch 18/50: acc_train=97.088% acc_val=96.281% acc_test=96.216%\n",
      "2018-04-16 14:45:03 start epoch 19/50:\n",
      "2018-04-16 14:45:03 iteration 1/859: current training loss = 0.129785\n",
      "2018-04-16 14:45:06 iteration 200/859: current training loss = 0.168257\n",
      "2018-04-16 14:45:07 iteration 400/859: current training loss = 0.034803\n",
      "2018-04-16 14:45:10 iteration 600/859: current training loss = 0.068748\n",
      "2018-04-16 14:45:12 iteration 800/859: current training loss = 0.111660\n",
      "2018-04-16 14:45:12 iteration 859/859: current training loss = 0.186763\n",
      "2018-04-16 14:45:29 end epoch 19/50: acc_train=97.250% acc_val=96.416% acc_test=96.306%\n",
      "2018-04-16 14:45:29 start epoch 20/50:\n",
      "2018-04-16 14:45:29 iteration 1/859: current training loss = 0.081969\n",
      "2018-04-16 14:45:31 iteration 200/859: current training loss = 0.094116\n",
      "2018-04-16 14:45:33 iteration 400/859: current training loss = 0.199007\n",
      "2018-04-16 14:45:34 iteration 600/859: current training loss = 0.058416\n",
      "2018-04-16 14:45:37 iteration 800/859: current training loss = 0.068609\n",
      "2018-04-16 14:45:38 iteration 859/859: current training loss = 0.061386\n",
      "2018-04-16 14:45:51 end epoch 20/50: acc_train=97.441% acc_val=96.759% acc_test=96.464%\n",
      "2018-04-16 14:45:51 start epoch 21/50:\n",
      "2018-04-16 14:45:51 iteration 1/859: current training loss = 0.075472\n",
      "2018-04-16 14:45:53 iteration 200/859: current training loss = 0.085173\n",
      "2018-04-16 14:45:56 iteration 400/859: current training loss = 0.071017\n",
      "2018-04-16 14:45:58 iteration 600/859: current training loss = 0.048048\n",
      "2018-04-16 14:46:00 iteration 800/859: current training loss = 0.092512\n",
      "2018-04-16 14:46:01 iteration 859/859: current training loss = 0.043453\n",
      "2018-04-16 14:46:13 end epoch 21/50: acc_train=97.623% acc_val=96.709% acc_test=96.455%\n",
      "2018-04-16 14:46:13 start epoch 22/50:\n",
      "2018-04-16 14:46:13 iteration 1/859: current training loss = 0.011461\n",
      "2018-04-16 14:46:15 iteration 200/859: current training loss = 0.189573\n",
      "2018-04-16 14:46:17 iteration 400/859: current training loss = 0.142996\n",
      "2018-04-16 14:46:19 iteration 600/859: current training loss = 0.058657\n",
      "2018-04-16 14:46:21 iteration 800/859: current training loss = 0.085920\n",
      "2018-04-16 14:46:21 iteration 859/859: current training loss = 0.037688\n",
      "2018-04-16 14:46:36 end epoch 22/50: acc_train=97.713% acc_val=96.678% acc_test=96.412%\n",
      "2018-04-16 14:46:36 start epoch 23/50:\n",
      "2018-04-16 14:46:36 iteration 1/859: current training loss = 0.024760\n",
      "2018-04-16 14:46:38 iteration 200/859: current training loss = 0.147435\n",
      "2018-04-16 14:46:40 iteration 400/859: current training loss = 0.144552\n",
      "2018-04-16 14:46:41 iteration 600/859: current training loss = 0.196872\n",
      "2018-04-16 14:46:44 iteration 800/859: current training loss = 0.025856\n",
      "2018-04-16 14:46:45 iteration 859/859: current training loss = 0.036063\n",
      "2018-04-16 14:46:59 end epoch 23/50: acc_train=97.695% acc_val=96.669% acc_test=96.789%\n",
      "2018-04-16 14:46:59 start epoch 24/50:\n",
      "2018-04-16 14:46:59 iteration 1/859: current training loss = 0.037752\n",
      "2018-04-16 14:47:01 iteration 200/859: current training loss = 0.176205\n",
      "2018-04-16 14:47:03 iteration 400/859: current training loss = 0.025033\n",
      "2018-04-16 14:47:06 iteration 600/859: current training loss = 0.041294\n",
      "2018-04-16 14:47:08 iteration 800/859: current training loss = 0.093338\n",
      "2018-04-16 14:47:09 iteration 859/859: current training loss = 0.074944\n",
      "2018-04-16 14:47:23 end epoch 24/50: acc_train=97.789% acc_val=97.078% acc_test=96.611%\n",
      "2018-04-16 14:47:23 start epoch 25/50:\n",
      "2018-04-16 14:47:23 iteration 1/859: current training loss = 0.033616\n",
      "2018-04-16 14:47:25 iteration 200/859: current training loss = 0.021362\n",
      "2018-04-16 14:47:27 iteration 400/859: current training loss = 0.139440\n",
      "2018-04-16 14:47:29 iteration 600/859: current training loss = 0.041224\n",
      "2018-04-16 14:47:31 iteration 800/859: current training loss = 0.012424\n",
      "2018-04-16 14:47:32 iteration 859/859: current training loss = 0.079670\n",
      "2018-04-16 14:47:45 end epoch 25/50: acc_train=98.037% acc_val=96.847% acc_test=96.650%\n",
      "2018-04-16 14:47:45 start epoch 26/50:\n",
      "2018-04-16 14:47:45 iteration 1/859: current training loss = 0.071503\n",
      "2018-04-16 14:47:47 iteration 200/859: current training loss = 0.078775\n",
      "2018-04-16 14:47:49 iteration 400/859: current training loss = 0.010618\n",
      "2018-04-16 14:47:51 iteration 600/859: current training loss = 0.118615\n",
      "2018-04-16 14:47:54 iteration 800/859: current training loss = 0.095978\n",
      "2018-04-16 14:47:54 iteration 859/859: current training loss = 0.039345\n",
      "2018-04-16 14:48:09 end epoch 26/50: acc_train=98.105% acc_val=96.819% acc_test=96.788%\n",
      "2018-04-16 14:48:09 start epoch 27/50:\n",
      "2018-04-16 14:48:09 iteration 1/859: current training loss = 0.035772\n",
      "2018-04-16 14:48:11 iteration 200/859: current training loss = 0.012470\n",
      "2018-04-16 14:48:13 iteration 400/859: current training loss = 0.028873\n",
      "2018-04-16 14:48:15 iteration 600/859: current training loss = 0.060605\n",
      "2018-04-16 14:48:16 iteration 800/859: current training loss = 0.011728\n",
      "2018-04-16 14:48:17 iteration 859/859: current training loss = 0.059081\n",
      "2018-04-16 14:48:31 end epoch 27/50: acc_train=98.263% acc_val=96.959% acc_test=96.831%\n",
      "2018-04-16 14:48:31 start epoch 28/50:\n",
      "2018-04-16 14:48:31 iteration 1/859: current training loss = 0.092400\n",
      "2018-04-16 14:48:34 iteration 200/859: current training loss = 0.066810\n",
      "2018-04-16 14:48:36 iteration 400/859: current training loss = 0.052436\n",
      "2018-04-16 14:48:38 iteration 600/859: current training loss = 0.047175\n",
      "2018-04-16 14:48:40 iteration 800/859: current training loss = 0.059586\n",
      "2018-04-16 14:48:41 iteration 859/859: current training loss = 0.114341\n",
      "2018-04-16 14:48:54 end epoch 28/50: acc_train=98.237% acc_val=97.159% acc_test=96.947%\n",
      "2018-04-16 14:48:54 start epoch 29/50:\n",
      "2018-04-16 14:48:54 iteration 1/859: current training loss = 0.090016\n",
      "2018-04-16 14:48:56 iteration 200/859: current training loss = 0.029476\n",
      "2018-04-16 14:48:59 iteration 400/859: current training loss = 0.060497\n",
      "2018-04-16 14:49:01 iteration 600/859: current training loss = 0.054840\n",
      "2018-04-16 14:49:03 iteration 800/859: current training loss = 0.020959\n",
      "2018-04-16 14:49:04 iteration 859/859: current training loss = 0.072513\n",
      "2018-04-16 14:49:18 end epoch 29/50: acc_train=98.310% acc_val=97.112% acc_test=97.006%\n",
      "2018-04-16 14:49:18 start epoch 30/50:\n",
      "2018-04-16 14:49:18 iteration 1/859: current training loss = 0.087220\n",
      "2018-04-16 14:49:20 iteration 200/859: current training loss = 0.085353\n",
      "2018-04-16 14:49:22 iteration 400/859: current training loss = 0.025294\n",
      "2018-04-16 14:49:24 iteration 600/859: current training loss = 0.038254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16 14:49:26 iteration 800/859: current training loss = 0.024762\n",
      "2018-04-16 14:49:26 iteration 859/859: current training loss = 0.012821\n",
      "2018-04-16 14:49:41 end epoch 30/50: acc_train=98.455% acc_val=97.159% acc_test=97.109%\n",
      "2018-04-16 14:49:41 start epoch 31/50:\n",
      "2018-04-16 14:49:41 iteration 1/859: current training loss = 0.029839\n",
      "2018-04-16 14:49:44 iteration 200/859: current training loss = 0.013444\n",
      "2018-04-16 14:49:46 iteration 400/859: current training loss = 0.133244\n",
      "2018-04-16 14:49:48 iteration 600/859: current training loss = 0.014293\n",
      "2018-04-16 14:49:50 iteration 800/859: current training loss = 0.031515\n",
      "2018-04-16 14:49:51 iteration 859/859: current training loss = 0.053485\n",
      "2018-04-16 14:50:05 end epoch 31/50: acc_train=98.455% acc_val=97.278% acc_test=97.033%\n",
      "2018-04-16 14:50:05 start epoch 32/50:\n",
      "2018-04-16 14:50:05 iteration 1/859: current training loss = 0.075821\n",
      "2018-04-16 14:50:07 iteration 200/859: current training loss = 0.014118\n",
      "2018-04-16 14:50:09 iteration 400/859: current training loss = 0.036674\n",
      "2018-04-16 14:50:11 iteration 600/859: current training loss = 0.033188\n",
      "2018-04-16 14:50:13 iteration 800/859: current training loss = 0.079735\n",
      "2018-04-16 14:50:14 iteration 859/859: current training loss = 0.013452\n",
      "2018-04-16 14:50:27 end epoch 32/50: acc_train=98.502% acc_val=97.162% acc_test=96.939%\n",
      "2018-04-16 14:50:27 start epoch 33/50:\n",
      "2018-04-16 14:50:27 iteration 1/859: current training loss = 0.019264\n",
      "2018-04-16 14:50:29 iteration 200/859: current training loss = 0.013101\n",
      "2018-04-16 14:50:31 iteration 400/859: current training loss = 0.022922\n",
      "2018-04-16 14:50:34 iteration 600/859: current training loss = 0.060624\n",
      "2018-04-16 14:50:36 iteration 800/859: current training loss = 0.140222\n",
      "2018-04-16 14:50:37 iteration 859/859: current training loss = 0.025624\n",
      "2018-04-16 14:50:51 end epoch 33/50: acc_train=98.523% acc_val=97.003% acc_test=97.062%\n",
      "2018-04-16 14:50:51 start epoch 34/50:\n",
      "2018-04-16 14:50:51 iteration 1/859: current training loss = 0.102146\n",
      "2018-04-16 14:50:54 iteration 200/859: current training loss = 0.097249\n",
      "2018-04-16 14:50:56 iteration 400/859: current training loss = 0.040397\n",
      "2018-04-16 14:50:58 iteration 600/859: current training loss = 0.043771\n",
      "2018-04-16 14:51:00 iteration 800/859: current training loss = 0.007493\n",
      "2018-04-16 14:51:00 iteration 859/859: current training loss = 0.020944\n",
      "2018-04-16 14:51:14 end epoch 34/50: acc_train=98.127% acc_val=96.938% acc_test=96.714%\n",
      "2018-04-16 14:51:14 start epoch 35/50:\n",
      "2018-04-16 14:51:14 iteration 1/859: current training loss = 0.073205\n",
      "2018-04-16 14:51:16 iteration 200/859: current training loss = 0.007660\n",
      "2018-04-16 14:51:18 iteration 400/859: current training loss = 0.073591\n",
      "2018-04-16 14:51:20 iteration 600/859: current training loss = 0.043386\n",
      "2018-04-16 14:51:23 iteration 800/859: current training loss = 0.008549\n",
      "2018-04-16 14:51:23 iteration 859/859: current training loss = 0.003877\n",
      "2018-04-16 14:51:37 end epoch 35/50: acc_train=98.741% acc_val=97.197% acc_test=97.138%\n",
      "2018-04-16 14:51:37 start epoch 36/50:\n",
      "2018-04-16 14:51:37 iteration 1/859: current training loss = 0.172296\n",
      "2018-04-16 14:51:39 iteration 200/859: current training loss = 0.058521\n",
      "2018-04-16 14:51:41 iteration 400/859: current training loss = 0.006375\n",
      "2018-04-16 14:51:43 iteration 600/859: current training loss = 0.008017\n",
      "2018-04-16 14:51:45 iteration 800/859: current training loss = 0.022692\n",
      "2018-04-16 14:51:46 iteration 859/859: current training loss = 0.023489\n",
      "2018-04-16 14:51:59 end epoch 36/50: acc_train=98.855% acc_val=97.325% acc_test=97.184%\n",
      "2018-04-16 14:51:59 start epoch 37/50:\n",
      "2018-04-16 14:51:59 iteration 1/859: current training loss = 0.019405\n",
      "2018-04-16 14:52:01 iteration 200/859: current training loss = 0.019323\n",
      "2018-04-16 14:52:04 iteration 400/859: current training loss = 0.013442\n",
      "2018-04-16 14:52:06 iteration 600/859: current training loss = 0.023049\n",
      "2018-04-16 14:52:08 iteration 800/859: current training loss = 0.025320\n",
      "2018-04-16 14:52:08 iteration 859/859: current training loss = 0.088567\n",
      "2018-04-16 14:52:21 end epoch 37/50: acc_train=98.699% acc_val=97.253% acc_test=97.041%\n",
      "2018-04-16 14:52:21 start epoch 38/50:\n",
      "2018-04-16 14:52:21 iteration 1/859: current training loss = 0.009967\n",
      "2018-04-16 14:52:23 iteration 200/859: current training loss = 0.143221\n",
      "2018-04-16 14:52:25 iteration 400/859: current training loss = 0.008030\n",
      "2018-04-16 14:52:27 iteration 600/859: current training loss = 0.043455\n",
      "2018-04-16 14:52:30 iteration 800/859: current training loss = 0.001835\n",
      "2018-04-16 14:52:30 iteration 859/859: current training loss = 0.024323\n",
      "2018-04-16 14:52:43 end epoch 38/50: acc_train=98.909% acc_val=97.281% acc_test=97.236%\n",
      "2018-04-16 14:52:43 start epoch 39/50:\n",
      "2018-04-16 14:52:43 iteration 1/859: current training loss = 0.037398\n",
      "2018-04-16 14:52:45 iteration 200/859: current training loss = 0.011061\n",
      "2018-04-16 14:52:47 iteration 400/859: current training loss = 0.007690\n",
      "2018-04-16 14:52:49 iteration 600/859: current training loss = 0.042862\n",
      "2018-04-16 14:52:51 iteration 800/859: current training loss = 0.075889\n",
      "2018-04-16 14:52:52 iteration 859/859: current training loss = 0.014615\n",
      "2018-04-16 14:53:03 end epoch 39/50: acc_train=98.987% acc_val=97.362% acc_test=97.217%\n",
      "2018-04-16 14:53:03 start epoch 40/50:\n",
      "2018-04-16 14:53:03 iteration 1/859: current training loss = 0.042037\n",
      "2018-04-16 14:53:05 iteration 200/859: current training loss = 0.063909\n",
      "2018-04-16 14:53:09 iteration 400/859: current training loss = 0.029219\n",
      "2018-04-16 14:53:11 iteration 600/859: current training loss = 0.050826\n",
      "2018-04-16 14:53:13 iteration 800/859: current training loss = 0.027864\n",
      "2018-04-16 14:53:13 iteration 859/859: current training loss = 0.080288\n",
      "2018-04-16 14:53:30 end epoch 40/50: acc_train=99.071% acc_val=97.416% acc_test=97.291%\n",
      "2018-04-16 14:53:30 start epoch 41/50:\n",
      "2018-04-16 14:53:30 iteration 1/859: current training loss = 0.011082\n",
      "2018-04-16 14:53:32 iteration 200/859: current training loss = 0.020038\n",
      "2018-04-16 14:53:33 iteration 400/859: current training loss = 0.037715\n",
      "2018-04-16 14:53:35 iteration 600/859: current training loss = 0.011611\n",
      "2018-04-16 14:53:38 iteration 800/859: current training loss = 0.008206\n",
      "2018-04-16 14:53:39 iteration 859/859: current training loss = 0.075063\n",
      "2018-04-16 14:53:53 end epoch 41/50: acc_train=98.766% acc_val=97.075% acc_test=96.925%\n",
      "2018-04-16 14:53:53 start epoch 42/50:\n",
      "2018-04-16 14:53:53 iteration 1/859: current training loss = 0.005073\n",
      "2018-04-16 14:53:55 iteration 200/859: current training loss = 0.014962\n",
      "2018-04-16 14:53:57 iteration 400/859: current training loss = 0.010189\n",
      "2018-04-16 14:53:59 iteration 600/859: current training loss = 0.062682\n",
      "2018-04-16 14:54:02 iteration 800/859: current training loss = 0.078466\n",
      "2018-04-16 14:54:02 iteration 859/859: current training loss = 0.006279\n",
      "2018-04-16 14:54:15 end epoch 42/50: acc_train=99.120% acc_val=97.366% acc_test=97.288%\n",
      "2018-04-16 14:54:15 start epoch 43/50:\n",
      "2018-04-16 14:54:15 iteration 1/859: current training loss = 0.077641\n",
      "2018-04-16 14:54:17 iteration 200/859: current training loss = 0.025383\n",
      "2018-04-16 14:54:20 iteration 400/859: current training loss = 0.004612\n",
      "2018-04-16 14:54:22 iteration 600/859: current training loss = 0.021857\n",
      "2018-04-16 14:54:24 iteration 800/859: current training loss = 0.100913\n",
      "2018-04-16 14:54:24 iteration 859/859: current training loss = 0.010530\n",
      "2018-04-16 14:54:38 end epoch 43/50: acc_train=98.755% acc_val=97.031% acc_test=96.952%\n",
      "2018-04-16 14:54:38 start epoch 44/50:\n",
      "2018-04-16 14:54:38 iteration 1/859: current training loss = 0.012036\n",
      "2018-04-16 14:54:40 iteration 200/859: current training loss = 0.005301\n",
      "2018-04-16 14:54:42 iteration 400/859: current training loss = 0.015080\n",
      "2018-04-16 14:54:44 iteration 600/859: current training loss = 0.005699\n",
      "2018-04-16 14:54:46 iteration 800/859: current training loss = 0.032534\n",
      "2018-04-16 14:54:47 iteration 859/859: current training loss = 0.008312\n",
      "2018-04-16 14:55:02 end epoch 44/50: acc_train=99.275% acc_val=97.359% acc_test=97.263%\n",
      "2018-04-16 14:55:02 start epoch 45/50:\n",
      "2018-04-16 14:55:02 iteration 1/859: current training loss = 0.008190\n",
      "2018-04-16 14:55:04 iteration 200/859: current training loss = 0.004141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16 14:55:06 iteration 400/859: current training loss = 0.023062\n",
      "2018-04-16 14:55:08 iteration 600/859: current training loss = 0.040162\n",
      "2018-04-16 14:55:10 iteration 800/859: current training loss = 0.020081\n",
      "2018-04-16 14:55:11 iteration 859/859: current training loss = 0.043714\n",
      "2018-04-16 14:55:24 end epoch 45/50: acc_train=99.283% acc_val=97.297% acc_test=97.428%\n",
      "2018-04-16 14:55:24 start epoch 46/50:\n",
      "2018-04-16 14:55:24 iteration 1/859: current training loss = 0.059119\n",
      "2018-04-16 14:55:26 iteration 200/859: current training loss = 0.030551\n",
      "2018-04-16 14:55:29 iteration 400/859: current training loss = 0.020510\n",
      "2018-04-16 14:55:31 iteration 600/859: current training loss = 0.018962\n",
      "2018-04-16 14:55:33 iteration 800/859: current training loss = 0.002500\n",
      "2018-04-16 14:55:34 iteration 859/859: current training loss = 0.010331\n",
      "2018-04-16 14:55:47 end epoch 46/50: acc_train=99.097% acc_val=97.356% acc_test=96.978%\n",
      "2018-04-16 14:55:47 start epoch 47/50:\n",
      "2018-04-16 14:55:47 iteration 1/859: current training loss = 0.024220\n",
      "2018-04-16 14:55:49 iteration 200/859: current training loss = 0.067518\n",
      "2018-04-16 14:55:51 iteration 400/859: current training loss = 0.006842\n",
      "2018-04-16 14:55:53 iteration 600/859: current training loss = 0.015946\n",
      "2018-04-16 14:55:55 iteration 800/859: current training loss = 0.003945\n",
      "2018-04-16 14:55:57 iteration 859/859: current training loss = 0.058250\n",
      "2018-04-16 14:56:10 end epoch 47/50: acc_train=99.369% acc_val=97.350% acc_test=97.289%\n",
      "2018-04-16 14:56:10 start epoch 48/50:\n",
      "2018-04-16 14:56:10 iteration 1/859: current training loss = 0.015264\n",
      "2018-04-16 14:56:12 iteration 200/859: current training loss = 0.015880\n",
      "2018-04-16 14:56:14 iteration 400/859: current training loss = 0.008244\n",
      "2018-04-16 14:56:17 iteration 600/859: current training loss = 0.005628\n",
      "2018-04-16 14:56:18 iteration 800/859: current training loss = 0.001380\n",
      "2018-04-16 14:56:19 iteration 859/859: current training loss = 0.044093\n",
      "2018-04-16 14:56:32 end epoch 48/50: acc_train=99.177% acc_val=97.400% acc_test=97.127%\n",
      "2018-04-16 14:56:32 start epoch 49/50:\n",
      "2018-04-16 14:56:32 iteration 1/859: current training loss = 0.008586\n",
      "2018-04-16 14:56:34 iteration 200/859: current training loss = 0.140560\n",
      "2018-04-16 14:56:37 iteration 400/859: current training loss = 0.036951\n",
      "2018-04-16 14:56:39 iteration 600/859: current training loss = 0.001187\n",
      "2018-04-16 14:56:41 iteration 800/859: current training loss = 0.003606\n",
      "2018-04-16 14:56:41 iteration 859/859: current training loss = 0.017403\n",
      "2018-04-16 14:56:56 end epoch 49/50: acc_train=99.399% acc_val=97.488% acc_test=97.330%\n",
      "2018-04-16 14:56:56 start epoch 50/50:\n",
      "2018-04-16 14:56:56 iteration 1/859: current training loss = 0.008643\n",
      "2018-04-16 14:56:57 iteration 200/859: current training loss = 0.010692\n",
      "2018-04-16 14:56:59 iteration 400/859: current training loss = 0.036908\n",
      "2018-04-16 14:57:01 iteration 600/859: current training loss = 0.006951\n",
      "2018-04-16 14:57:02 iteration 800/859: current training loss = 0.041693\n",
      "2018-04-16 14:57:04 iteration 859/859: current training loss = 0.009320\n",
      "2018-04-16 14:57:18 end epoch 50/50: acc_train=99.395% acc_val=97.300% acc_test=97.409%\n",
      "Model saved in path: model/SRAM.ckpt\n"
     ]
    }
   ],
   "source": [
    "max_epoch=50\n",
    "print_every=200\n",
    "\n",
    "def train():\n",
    "    num_iteration=num_train//batch_size\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=mnist.train.next_batch(batch_size)\n",
    "        loss_num,_ = sess.run([loss,train_step],feed_dict={X:images,y:labels})\n",
    "        if it==0 or (it+1)%print_every==0 or it==num_iteration-1:\n",
    "            print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "                  'iteration %d/%d:' % (it+1,num_iteration),'current training loss = %f' % (loss_num))\n",
    "            \n",
    "def eval(dataset,num_iteration):\n",
    "    total_loss=0\n",
    "    total_accuracy=0\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=dataset.next_batch(batch_size)\n",
    "        loss_num,accuracy_num = sess.run([loss,accuracy],feed_dict={X:images,y:labels})\n",
    "        total_loss+=loss_num\n",
    "        total_accuracy+=accuracy_num\n",
    "    total_loss/=num_iteration\n",
    "    total_accuracy/=num_iteration\n",
    "    return total_loss,total_accuracy\n",
    "    \n",
    "acc_train_his=[]\n",
    "acc_val_his=[]\n",
    "acc_test_his=[]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(max_epoch):\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'start epoch %d/%d:' % (epoch+1,max_epoch))\n",
    "        train()\n",
    "        loss_train,acc_train=eval(mnist.train,2000)\n",
    "        loss_val,acc_val=eval(mnist.validation,500)\n",
    "        loss_test,acc_test=eval(mnist.test,1000)\n",
    "        acc_train_his.append(acc_train)\n",
    "        acc_val_his.append(acc_val)\n",
    "        acc_test_his.append(acc_test)\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'end epoch %d/%d:' % (epoch+1,max_epoch),\n",
    "             'acc_train=%.3f%% acc_val=%.3f%% acc_test=%.3f%%' % (acc_train*100.0,acc_val*100.0,acc_test*100.0))\n",
    "    save_path = saver.save(sess, \"model/SRAM/SRAM.ckpt\")\n",
    "    print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(2)\n",
    "ptr,=plt.plot(range(max_epoch),acc_train_his,'r-')\n",
    "pva,=plt.plot(range(max_epoch),acc_val_his,'b-')\n",
    "pte,=plt.plot(range(max_epoch),acc_test_his,'y-')\n",
    "plt.xlabel('training epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('accuracy on three sets')\n",
    "plt.legend((ptr,pva,pte),('train','validation','test'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
