{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"mnist/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train=mnist.train.num_examples\n",
    "num_val=mnist.validation.images.shape\n",
    "num_test=mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baseline: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(dtype=tf.float32,shape=[None,28,28,1],name='X')\n",
    "y=tf.placeholder(dtype=tf.int64,shape=[None,10],name='y')\n",
    "is_training=tf.placeholder(dtype=tf.bool,name='is_training')\n",
    "cnn_out1=tf.layers.conv2d(X,128,kernel_size=3,strides=(1, 1),padding='same')\n",
    "bn_out1=tf.layers.batch_normalization(cnn_out1,axis=3,training=is_training)\n",
    "relu_out1=tf.nn.relu(bn_out1)\n",
    "pool_out1=tf.layers.max_pooling2d(relu_out1,[2,2],[2,2])\n",
    "cnn_out2=tf.layers.conv2d(pool_out1,128,kernel_size=3,strides=(1, 1),padding='same')\n",
    "bn_out2=tf.layers.batch_normalization(cnn_out2,axis=3,training=is_training)\n",
    "relu_out1=tf.nn.relu(bn_out2)\n",
    "pool_out2=tf.layers.max_pooling2d(relu_out1,[2,2],[2,2])\n",
    "flt=tf.layers.flatten(pool_out2)\n",
    "out1=tf.layers.dense(flt,1024)\n",
    "# bn_out3=tf.layers.batch_normalization(out1,axis=1,training=is_training)\n",
    "out2=tf.nn.relu(out1)\n",
    "score=tf.layers.dense(out2,10)\n",
    "predictions = tf.argmax(score, 1)\n",
    "correct_predictions = tf.equal(predictions, tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "\n",
    "loss=tf.losses.softmax_cross_entropy(onehot_labels=y,logits=score)\n",
    "optimizier=tf.train.AdamOptimizer(learning_rate=1e-6)\n",
    "train_step = optimizier.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 14:22:25 start epoch 1/50:\n",
      "2018-04-13 14:22:27 iteration 1/6875: current training loss = 2.496820\n",
      "2018-04-13 14:22:29 iteration 625/6875: current training loss = 1.081113\n",
      "2018-04-13 14:22:32 iteration 1250/6875: current training loss = 0.475402\n",
      "2018-04-13 14:22:34 iteration 1875/6875: current training loss = 0.558688\n",
      "2018-04-13 14:22:37 iteration 2500/6875: current training loss = 0.203625\n",
      "2018-04-13 14:22:39 iteration 3125/6875: current training loss = 0.416182\n",
      "2018-04-13 14:22:42 iteration 3750/6875: current training loss = 0.148532\n",
      "2018-04-13 14:22:44 iteration 4375/6875: current training loss = 0.100706\n",
      "2018-04-13 14:22:47 iteration 5000/6875: current training loss = 0.697870\n",
      "2018-04-13 14:22:49 iteration 5625/6875: current training loss = 0.105114\n",
      "2018-04-13 14:22:52 iteration 6250/6875: current training loss = 0.174963\n",
      "2018-04-13 14:22:55 iteration 6875/6875: current training loss = 0.521361\n",
      "2018-04-13 14:23:02 end epoch 1/50: acc_train=81.794% acc_val=83.175% acc_test=81.475%\n",
      "2018-04-13 14:23:02 start epoch 2/50:\n",
      "2018-04-13 14:23:02 iteration 1/6875: current training loss = 0.109956\n",
      "2018-04-13 14:23:04 iteration 625/6875: current training loss = 0.033830\n",
      "2018-04-13 14:23:07 iteration 1250/6875: current training loss = 0.152430\n",
      "2018-04-13 14:23:09 iteration 1875/6875: current training loss = 0.226125\n",
      "2018-04-13 14:23:12 iteration 2500/6875: current training loss = 0.032565\n",
      "2018-04-13 14:23:14 iteration 3125/6875: current training loss = 0.006975\n",
      "2018-04-13 14:23:17 iteration 3750/6875: current training loss = 0.359356\n",
      "2018-04-13 14:23:20 iteration 4375/6875: current training loss = 0.035752\n",
      "2018-04-13 14:23:22 iteration 5000/6875: current training loss = 0.018260\n",
      "2018-04-13 14:23:25 iteration 5625/6875: current training loss = 0.233672\n",
      "2018-04-13 14:23:27 iteration 6250/6875: current training loss = 0.010752\n",
      "2018-04-13 14:23:30 iteration 6875/6875: current training loss = 0.162136\n",
      "2018-04-13 14:23:35 end epoch 2/50: acc_train=86.887% acc_val=87.800% acc_test=86.825%\n",
      "2018-04-13 14:23:35 start epoch 3/50:\n",
      "2018-04-13 14:23:35 iteration 1/6875: current training loss = 0.123984\n",
      "2018-04-13 14:23:38 iteration 625/6875: current training loss = 0.169640\n",
      "2018-04-13 14:23:40 iteration 1250/6875: current training loss = 0.135098\n",
      "2018-04-13 14:23:43 iteration 1875/6875: current training loss = 0.012431\n",
      "2018-04-13 14:23:45 iteration 2500/6875: current training loss = 0.171301\n",
      "2018-04-13 14:23:48 iteration 3125/6875: current training loss = 0.079958\n",
      "2018-04-13 14:23:50 iteration 3750/6875: current training loss = 0.053020\n",
      "2018-04-13 14:23:53 iteration 4375/6875: current training loss = 0.017703\n",
      "2018-04-13 14:23:55 iteration 5000/6875: current training loss = 0.093616\n",
      "2018-04-13 14:23:58 iteration 5625/6875: current training loss = 0.070504\n",
      "2018-04-13 14:24:00 iteration 6250/6875: current training loss = 0.103323\n",
      "2018-04-13 14:24:03 iteration 6875/6875: current training loss = 0.024217\n",
      "2018-04-13 14:24:09 end epoch 3/50: acc_train=90.281% acc_val=90.550% acc_test=90.650%\n",
      "2018-04-13 14:24:09 start epoch 4/50:\n",
      "2018-04-13 14:24:09 iteration 1/6875: current training loss = 0.161592\n",
      "2018-04-13 14:24:12 iteration 625/6875: current training loss = 0.002635\n",
      "2018-04-13 14:24:15 iteration 1250/6875: current training loss = 0.101866\n",
      "2018-04-13 14:24:17 iteration 1875/6875: current training loss = 0.021446\n",
      "2018-04-13 14:24:20 iteration 2500/6875: current training loss = 0.315985\n",
      "2018-04-13 14:24:22 iteration 3125/6875: current training loss = 0.117639\n",
      "2018-04-13 14:24:24 iteration 3750/6875: current training loss = 0.115085\n",
      "2018-04-13 14:24:27 iteration 4375/6875: current training loss = 0.313051\n",
      "2018-04-13 14:24:29 iteration 5000/6875: current training loss = 0.054281\n",
      "2018-04-13 14:24:32 iteration 5625/6875: current training loss = 0.006585\n",
      "2018-04-13 14:24:34 iteration 6250/6875: current training loss = 0.008552\n",
      "2018-04-13 14:24:36 iteration 6875/6875: current training loss = 0.050761\n",
      "2018-04-13 14:24:42 end epoch 4/50: acc_train=93.594% acc_val=94.025% acc_test=93.425%\n",
      "2018-04-13 14:24:42 start epoch 5/50:\n",
      "2018-04-13 14:24:42 iteration 1/6875: current training loss = 0.017166\n",
      "2018-04-13 14:24:44 iteration 625/6875: current training loss = 0.042919\n",
      "2018-04-13 14:24:46 iteration 1250/6875: current training loss = 0.011888\n",
      "2018-04-13 14:24:49 iteration 1875/6875: current training loss = 0.011876\n",
      "2018-04-13 14:24:51 iteration 2500/6875: current training loss = 0.010689\n",
      "2018-04-13 14:24:54 iteration 3125/6875: current training loss = 0.079386\n",
      "2018-04-13 14:24:56 iteration 3750/6875: current training loss = 0.015713\n",
      "2018-04-13 14:24:59 iteration 4375/6875: current training loss = 0.254329\n",
      "2018-04-13 14:25:01 iteration 5000/6875: current training loss = 0.020388\n",
      "2018-04-13 14:25:04 iteration 5625/6875: current training loss = 0.016005\n",
      "2018-04-13 14:25:06 iteration 6250/6875: current training loss = 0.009838\n",
      "2018-04-13 14:25:09 iteration 6875/6875: current training loss = 0.029941\n",
      "2018-04-13 14:25:14 end epoch 5/50: acc_train=93.925% acc_val=94.125% acc_test=93.987%\n",
      "2018-04-13 14:25:14 start epoch 6/50:\n",
      "2018-04-13 14:25:14 iteration 1/6875: current training loss = 0.032253\n",
      "2018-04-13 14:25:17 iteration 625/6875: current training loss = 0.007926\n",
      "2018-04-13 14:25:19 iteration 1250/6875: current training loss = 0.008855\n",
      "2018-04-13 14:25:21 iteration 1875/6875: current training loss = 0.012248\n",
      "2018-04-13 14:25:24 iteration 2500/6875: current training loss = 0.029677\n",
      "2018-04-13 14:25:26 iteration 3125/6875: current training loss = 0.014230\n",
      "2018-04-13 14:25:28 iteration 3750/6875: current training loss = 0.001982\n",
      "2018-04-13 14:25:31 iteration 4375/6875: current training loss = 0.042080\n",
      "2018-04-13 14:25:33 iteration 5000/6875: current training loss = 0.240940\n",
      "2018-04-13 14:25:35 iteration 5625/6875: current training loss = 0.003221\n",
      "2018-04-13 14:25:38 iteration 6250/6875: current training loss = 0.004944\n",
      "2018-04-13 14:25:40 iteration 6875/6875: current training loss = 0.043404\n",
      "2018-04-13 14:25:45 end epoch 6/50: acc_train=95.462% acc_val=95.325% acc_test=95.600%\n",
      "2018-04-13 14:25:45 start epoch 7/50:\n",
      "2018-04-13 14:25:45 iteration 1/6875: current training loss = 0.059027\n",
      "2018-04-13 14:25:48 iteration 625/6875: current training loss = 0.055277\n",
      "2018-04-13 14:25:50 iteration 1250/6875: current training loss = 0.127900\n",
      "2018-04-13 14:25:52 iteration 1875/6875: current training loss = 0.045914\n",
      "2018-04-13 14:25:55 iteration 2500/6875: current training loss = 0.039571\n",
      "2018-04-13 14:25:57 iteration 3125/6875: current training loss = 0.026565\n",
      "2018-04-13 14:25:59 iteration 3750/6875: current training loss = 0.039780\n",
      "2018-04-13 14:26:02 iteration 4375/6875: current training loss = 0.003487\n",
      "2018-04-13 14:26:04 iteration 5000/6875: current training loss = 0.025993\n",
      "2018-04-13 14:26:07 iteration 5625/6875: current training loss = 0.032846\n",
      "2018-04-13 14:26:09 iteration 6250/6875: current training loss = 0.002036\n",
      "2018-04-13 14:26:12 iteration 6875/6875: current training loss = 0.021652\n",
      "2018-04-13 14:26:19 end epoch 7/50: acc_train=95.519% acc_val=95.725% acc_test=95.675%\n",
      "2018-04-13 14:26:19 start epoch 8/50:\n",
      "2018-04-13 14:26:19 iteration 1/6875: current training loss = 0.048647\n",
      "2018-04-13 14:26:21 iteration 625/6875: current training loss = 0.012723\n",
      "2018-04-13 14:26:24 iteration 1250/6875: current training loss = 0.008547\n",
      "2018-04-13 14:26:26 iteration 1875/6875: current training loss = 0.001297\n",
      "2018-04-13 14:26:29 iteration 2500/6875: current training loss = 0.001237\n",
      "2018-04-13 14:26:31 iteration 3125/6875: current training loss = 0.094849\n",
      "2018-04-13 14:26:34 iteration 3750/6875: current training loss = 0.136437\n",
      "2018-04-13 14:26:37 iteration 4375/6875: current training loss = 0.184627\n",
      "2018-04-13 14:26:39 iteration 5000/6875: current training loss = 0.004426\n",
      "2018-04-13 14:26:42 iteration 5625/6875: current training loss = 0.010295\n",
      "2018-04-13 14:26:44 iteration 6250/6875: current training loss = 0.012168\n",
      "2018-04-13 14:26:46 iteration 6875/6875: current training loss = 0.007562\n",
      "2018-04-13 14:26:51 end epoch 8/50: acc_train=96.431% acc_val=96.650% acc_test=96.350%\n",
      "2018-04-13 14:26:51 start epoch 9/50:\n",
      "2018-04-13 14:26:51 iteration 1/6875: current training loss = 0.000565\n",
      "2018-04-13 14:26:54 iteration 625/6875: current training loss = 0.000909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 14:26:56 iteration 1250/6875: current training loss = 0.005832\n",
      "2018-04-13 14:26:58 iteration 1875/6875: current training loss = 0.017695\n",
      "2018-04-13 14:27:01 iteration 2500/6875: current training loss = 0.009931\n",
      "2018-04-13 14:27:03 iteration 3125/6875: current training loss = 0.014724\n",
      "2018-04-13 14:27:05 iteration 3750/6875: current training loss = 0.004046\n",
      "2018-04-13 14:27:08 iteration 4375/6875: current training loss = 0.003921\n",
      "2018-04-13 14:27:10 iteration 5000/6875: current training loss = 0.035541\n",
      "2018-04-13 14:27:12 iteration 5625/6875: current training loss = 0.005573\n",
      "2018-04-13 14:27:15 iteration 6250/6875: current training loss = 0.044476\n",
      "2018-04-13 14:27:17 iteration 6875/6875: current training loss = 0.004140\n",
      "2018-04-13 14:27:23 end epoch 9/50: acc_train=96.506% acc_val=96.675% acc_test=96.250%\n",
      "2018-04-13 14:27:23 start epoch 10/50:\n",
      "2018-04-13 14:27:23 iteration 1/6875: current training loss = 0.172007\n",
      "2018-04-13 14:27:25 iteration 625/6875: current training loss = 0.039585\n",
      "2018-04-13 14:27:28 iteration 1250/6875: current training loss = 0.003160\n",
      "2018-04-13 14:27:30 iteration 1875/6875: current training loss = 0.000492\n",
      "2018-04-13 14:27:33 iteration 2500/6875: current training loss = 0.003846\n",
      "2018-04-13 14:27:35 iteration 3125/6875: current training loss = 0.002563\n",
      "2018-04-13 14:27:38 iteration 3750/6875: current training loss = 0.023741\n",
      "2018-04-13 14:27:40 iteration 4375/6875: current training loss = 0.147726\n",
      "2018-04-13 14:27:43 iteration 5000/6875: current training loss = 0.006036\n",
      "2018-04-13 14:27:45 iteration 5625/6875: current training loss = 0.044693\n",
      "2018-04-13 14:27:48 iteration 6250/6875: current training loss = 0.002113\n",
      "2018-04-13 14:27:50 iteration 6875/6875: current training loss = 0.010134\n",
      "2018-04-13 14:27:56 end epoch 10/50: acc_train=96.844% acc_val=96.875% acc_test=96.938%\n",
      "2018-04-13 14:27:56 start epoch 11/50:\n",
      "2018-04-13 14:27:56 iteration 1/6875: current training loss = 0.041796\n",
      "2018-04-13 14:27:58 iteration 625/6875: current training loss = 0.005944\n",
      "2018-04-13 14:28:01 iteration 1250/6875: current training loss = 0.000998\n",
      "2018-04-13 14:28:03 iteration 1875/6875: current training loss = 0.059201\n",
      "2018-04-13 14:28:06 iteration 2500/6875: current training loss = 0.028718\n",
      "2018-04-13 14:28:08 iteration 3125/6875: current training loss = 0.040567\n",
      "2018-04-13 14:28:11 iteration 3750/6875: current training loss = 0.082524\n",
      "2018-04-13 14:28:13 iteration 4375/6875: current training loss = 0.055442\n",
      "2018-04-13 14:28:16 iteration 5000/6875: current training loss = 0.038830\n",
      "2018-04-13 14:28:18 iteration 5625/6875: current training loss = 0.000995\n",
      "2018-04-13 14:28:21 iteration 6250/6875: current training loss = 0.024912\n",
      "2018-04-13 14:28:24 iteration 6875/6875: current training loss = 0.005291\n",
      "2018-04-13 14:28:30 end epoch 11/50: acc_train=97.094% acc_val=96.850% acc_test=96.888%\n",
      "2018-04-13 14:28:30 start epoch 12/50:\n",
      "2018-04-13 14:28:30 iteration 1/6875: current training loss = 0.126730\n",
      "2018-04-13 14:28:32 iteration 625/6875: current training loss = 0.004736\n",
      "2018-04-13 14:28:35 iteration 1250/6875: current training loss = 0.001280\n",
      "2018-04-13 14:28:37 iteration 1875/6875: current training loss = 0.162670\n",
      "2018-04-13 14:28:40 iteration 2500/6875: current training loss = 0.003582\n",
      "2018-04-13 14:28:43 iteration 3125/6875: current training loss = 0.002046\n",
      "2018-04-13 14:28:45 iteration 3750/6875: current training loss = 0.068772\n",
      "2018-04-13 14:28:48 iteration 4375/6875: current training loss = 0.004926\n",
      "2018-04-13 14:28:50 iteration 5000/6875: current training loss = 0.056452\n",
      "2018-04-13 14:28:53 iteration 5625/6875: current training loss = 0.005906\n",
      "2018-04-13 14:28:55 iteration 6250/6875: current training loss = 0.114400\n",
      "2018-04-13 14:28:58 iteration 6875/6875: current training loss = 0.054726\n",
      "2018-04-13 14:29:03 end epoch 12/50: acc_train=97.362% acc_val=97.575% acc_test=96.675%\n",
      "2018-04-13 14:29:03 start epoch 13/50:\n",
      "2018-04-13 14:29:03 iteration 1/6875: current training loss = 0.002811\n",
      "2018-04-13 14:29:05 iteration 625/6875: current training loss = 0.018728\n",
      "2018-04-13 14:29:08 iteration 1250/6875: current training loss = 0.003184\n",
      "2018-04-13 14:29:10 iteration 1875/6875: current training loss = 0.000356\n",
      "2018-04-13 14:29:13 iteration 2500/6875: current training loss = 0.023347\n",
      "2018-04-13 14:29:15 iteration 3125/6875: current training loss = 0.000759\n",
      "2018-04-13 14:29:18 iteration 3750/6875: current training loss = 0.086148\n",
      "2018-04-13 14:29:20 iteration 4375/6875: current training loss = 0.015007\n",
      "2018-04-13 14:29:22 iteration 5000/6875: current training loss = 0.004417\n",
      "2018-04-13 14:29:25 iteration 5625/6875: current training loss = 0.018064\n",
      "2018-04-13 14:29:27 iteration 6250/6875: current training loss = 0.001808\n",
      "2018-04-13 14:29:30 iteration 6875/6875: current training loss = 0.004038\n",
      "2018-04-13 14:29:36 end epoch 13/50: acc_train=97.700% acc_val=97.100% acc_test=97.650%\n",
      "2018-04-13 14:29:36 start epoch 14/50:\n",
      "2018-04-13 14:29:36 iteration 1/6875: current training loss = 0.002098\n",
      "2018-04-13 14:29:38 iteration 625/6875: current training loss = 0.003548\n",
      "2018-04-13 14:29:40 iteration 1250/6875: current training loss = 0.002284\n",
      "2018-04-13 14:29:43 iteration 1875/6875: current training loss = 0.002954\n",
      "2018-04-13 14:29:46 iteration 2500/6875: current training loss = 0.021400\n",
      "2018-04-13 14:29:48 iteration 3125/6875: current training loss = 0.019120\n",
      "2018-04-13 14:29:51 iteration 3750/6875: current training loss = 0.007377\n",
      "2018-04-13 14:29:53 iteration 4375/6875: current training loss = 0.004880\n",
      "2018-04-13 14:29:56 iteration 5000/6875: current training loss = 0.006301\n",
      "2018-04-13 14:29:58 iteration 5625/6875: current training loss = 0.002162\n",
      "2018-04-13 14:30:01 iteration 6250/6875: current training loss = 0.022294\n",
      "2018-04-13 14:30:03 iteration 6875/6875: current training loss = 0.003572\n",
      "2018-04-13 14:30:10 end epoch 14/50: acc_train=97.906% acc_val=97.525% acc_test=97.438%\n",
      "2018-04-13 14:30:10 start epoch 15/50:\n",
      "2018-04-13 14:30:10 iteration 1/6875: current training loss = 0.012045\n",
      "2018-04-13 14:30:13 iteration 625/6875: current training loss = 0.002552\n",
      "2018-04-13 14:30:15 iteration 1250/6875: current training loss = 0.006539\n",
      "2018-04-13 14:30:18 iteration 1875/6875: current training loss = 0.003809\n",
      "2018-04-13 14:30:20 iteration 2500/6875: current training loss = 0.266040\n",
      "2018-04-13 14:30:23 iteration 3125/6875: current training loss = 0.022687\n",
      "2018-04-13 14:30:25 iteration 3750/6875: current training loss = 0.004896\n",
      "2018-04-13 14:30:28 iteration 4375/6875: current training loss = 0.003732\n",
      "2018-04-13 14:30:30 iteration 5000/6875: current training loss = 0.015049\n",
      "2018-04-13 14:30:33 iteration 5625/6875: current training loss = 0.010757\n",
      "2018-04-13 14:30:36 iteration 6250/6875: current training loss = 0.079908\n",
      "2018-04-13 14:30:38 iteration 6875/6875: current training loss = 0.001202\n",
      "2018-04-13 14:30:45 end epoch 15/50: acc_train=97.275% acc_val=97.250% acc_test=96.988%\n",
      "2018-04-13 14:30:45 start epoch 16/50:\n",
      "2018-04-13 14:30:45 iteration 1/6875: current training loss = 0.003158\n",
      "2018-04-13 14:30:47 iteration 625/6875: current training loss = 0.000351\n",
      "2018-04-13 14:30:50 iteration 1250/6875: current training loss = 0.124329\n",
      "2018-04-13 14:30:53 iteration 1875/6875: current training loss = 0.006592\n",
      "2018-04-13 14:30:55 iteration 2500/6875: current training loss = 0.051595\n",
      "2018-04-13 14:30:58 iteration 3125/6875: current training loss = 0.000651\n",
      "2018-04-13 14:31:00 iteration 3750/6875: current training loss = 0.002285\n",
      "2018-04-13 14:31:03 iteration 4375/6875: current training loss = 0.145140\n",
      "2018-04-13 14:31:05 iteration 5000/6875: current training loss = 0.011014\n",
      "2018-04-13 14:31:08 iteration 5625/6875: current training loss = 0.004382\n",
      "2018-04-13 14:31:10 iteration 6250/6875: current training loss = 0.001376\n",
      "2018-04-13 14:31:13 iteration 6875/6875: current training loss = 0.002196\n",
      "2018-04-13 14:31:19 end epoch 16/50: acc_train=97.706% acc_val=97.300% acc_test=97.725%\n",
      "2018-04-13 14:31:19 start epoch 17/50:\n",
      "2018-04-13 14:31:19 iteration 1/6875: current training loss = 0.009326\n",
      "2018-04-13 14:31:21 iteration 625/6875: current training loss = 0.059270\n",
      "2018-04-13 14:31:24 iteration 1250/6875: current training loss = 0.004014\n",
      "2018-04-13 14:31:26 iteration 1875/6875: current training loss = 0.017647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 14:31:29 iteration 2500/6875: current training loss = 0.001194\n",
      "2018-04-13 14:31:32 iteration 3125/6875: current training loss = 0.064488\n",
      "2018-04-13 14:31:34 iteration 3750/6875: current training loss = 0.012540\n",
      "2018-04-13 14:31:37 iteration 4375/6875: current training loss = 0.002128\n",
      "2018-04-13 14:31:40 iteration 5000/6875: current training loss = 0.006273\n",
      "2018-04-13 14:31:42 iteration 5625/6875: current training loss = 0.027493\n",
      "2018-04-13 14:31:45 iteration 6250/6875: current training loss = 0.000646\n",
      "2018-04-13 14:31:47 iteration 6875/6875: current training loss = 0.028626\n",
      "2018-04-13 14:31:53 end epoch 17/50: acc_train=97.756% acc_val=97.700% acc_test=97.513%\n",
      "2018-04-13 14:31:53 start epoch 18/50:\n",
      "2018-04-13 14:31:53 iteration 1/6875: current training loss = 0.000357\n",
      "2018-04-13 14:31:55 iteration 625/6875: current training loss = 0.003706\n",
      "2018-04-13 14:31:58 iteration 1250/6875: current training loss = 0.000887\n",
      "2018-04-13 14:32:00 iteration 1875/6875: current training loss = 0.004909\n",
      "2018-04-13 14:32:03 iteration 2500/6875: current training loss = 0.030086\n",
      "2018-04-13 14:32:05 iteration 3125/6875: current training loss = 0.000576\n",
      "2018-04-13 14:32:08 iteration 3750/6875: current training loss = 0.011729\n",
      "2018-04-13 14:32:10 iteration 4375/6875: current training loss = 0.098664\n",
      "2018-04-13 14:32:13 iteration 5000/6875: current training loss = 0.000796\n",
      "2018-04-13 14:32:15 iteration 5625/6875: current training loss = 0.000241\n",
      "2018-04-13 14:32:18 iteration 6250/6875: current training loss = 0.017029\n",
      "2018-04-13 14:32:20 iteration 6875/6875: current training loss = 0.014702\n",
      "2018-04-13 14:32:27 end epoch 18/50: acc_train=98.394% acc_val=98.025% acc_test=98.112%\n",
      "2018-04-13 14:32:27 start epoch 19/50:\n",
      "2018-04-13 14:32:27 iteration 1/6875: current training loss = 0.007424\n",
      "2018-04-13 14:32:29 iteration 625/6875: current training loss = 0.377690\n",
      "2018-04-13 14:32:32 iteration 1250/6875: current training loss = 0.021715\n",
      "2018-04-13 14:32:34 iteration 1875/6875: current training loss = 0.001982\n",
      "2018-04-13 14:32:37 iteration 2500/6875: current training loss = 0.010668\n",
      "2018-04-13 14:32:39 iteration 3125/6875: current training loss = 0.004582\n",
      "2018-04-13 14:32:41 iteration 3750/6875: current training loss = 0.021125\n",
      "2018-04-13 14:32:44 iteration 4375/6875: current training loss = 0.041527\n",
      "2018-04-13 14:32:46 iteration 5000/6875: current training loss = 0.002262\n",
      "2018-04-13 14:32:48 iteration 5625/6875: current training loss = 0.171686\n",
      "2018-04-13 14:32:51 iteration 6250/6875: current training loss = 0.014736\n",
      "2018-04-13 14:32:53 iteration 6875/6875: current training loss = 0.019684\n",
      "2018-04-13 14:32:59 end epoch 19/50: acc_train=97.844% acc_val=97.500% acc_test=97.462%\n",
      "2018-04-13 14:32:59 start epoch 20/50:\n",
      "2018-04-13 14:32:59 iteration 1/6875: current training loss = 0.000244\n",
      "2018-04-13 14:33:01 iteration 625/6875: current training loss = 0.014997\n",
      "2018-04-13 14:33:03 iteration 1250/6875: current training loss = 0.030570\n",
      "2018-04-13 14:33:06 iteration 1875/6875: current training loss = 0.000725\n",
      "2018-04-13 14:33:08 iteration 2500/6875: current training loss = 0.000668\n",
      "2018-04-13 14:33:10 iteration 3125/6875: current training loss = 0.099321\n",
      "2018-04-13 14:33:13 iteration 3750/6875: current training loss = 0.005670\n",
      "2018-04-13 14:33:15 iteration 4375/6875: current training loss = 0.002575\n",
      "2018-04-13 14:33:18 iteration 5000/6875: current training loss = 0.003248\n",
      "2018-04-13 14:33:20 iteration 5625/6875: current training loss = 0.000894\n",
      "2018-04-13 14:33:23 iteration 6250/6875: current training loss = 0.042849\n",
      "2018-04-13 14:33:25 iteration 6875/6875: current training loss = 0.002323\n",
      "2018-04-13 14:33:31 end epoch 20/50: acc_train=97.988% acc_val=97.925% acc_test=97.788%\n",
      "2018-04-13 14:33:31 start epoch 21/50:\n",
      "2018-04-13 14:33:31 iteration 1/6875: current training loss = 0.000923\n",
      "2018-04-13 14:33:34 iteration 625/6875: current training loss = 0.004073\n",
      "2018-04-13 14:33:36 iteration 1250/6875: current training loss = 0.007208\n",
      "2018-04-13 14:33:39 iteration 1875/6875: current training loss = 0.002289\n",
      "2018-04-13 14:33:41 iteration 2500/6875: current training loss = 0.034899\n",
      "2018-04-13 14:33:43 iteration 3125/6875: current training loss = 0.000871\n",
      "2018-04-13 14:33:45 iteration 3750/6875: current training loss = 0.008858\n",
      "2018-04-13 14:33:48 iteration 4375/6875: current training loss = 0.005841\n",
      "2018-04-13 14:33:50 iteration 5000/6875: current training loss = 0.002154\n",
      "2018-04-13 14:33:52 iteration 5625/6875: current training loss = 0.015549\n",
      "2018-04-13 14:33:55 iteration 6250/6875: current training loss = 0.000373\n",
      "2018-04-13 14:33:57 iteration 6875/6875: current training loss = 0.010935\n",
      "2018-04-13 14:34:03 end epoch 21/50: acc_train=98.194% acc_val=97.975% acc_test=97.788%\n",
      "2018-04-13 14:34:03 start epoch 22/50:\n",
      "2018-04-13 14:34:03 iteration 1/6875: current training loss = 0.010589\n",
      "2018-04-13 14:34:05 iteration 625/6875: current training loss = 0.008304\n",
      "2018-04-13 14:34:07 iteration 1250/6875: current training loss = 0.010550\n",
      "2018-04-13 14:34:10 iteration 1875/6875: current training loss = 0.000548\n",
      "2018-04-13 14:34:12 iteration 2500/6875: current training loss = 0.008102\n",
      "2018-04-13 14:34:15 iteration 3125/6875: current training loss = 0.005944\n",
      "2018-04-13 14:34:17 iteration 3750/6875: current training loss = 0.032598\n",
      "2018-04-13 14:34:19 iteration 4375/6875: current training loss = 0.129307\n",
      "2018-04-13 14:34:22 iteration 5000/6875: current training loss = 0.007211\n",
      "2018-04-13 14:34:24 iteration 5625/6875: current training loss = 0.003054\n",
      "2018-04-13 14:34:26 iteration 6250/6875: current training loss = 0.001541\n",
      "2018-04-13 14:34:29 iteration 6875/6875: current training loss = 0.002294\n",
      "2018-04-13 14:34:35 end epoch 22/50: acc_train=97.856% acc_val=97.950% acc_test=97.300%\n",
      "2018-04-13 14:34:35 start epoch 23/50:\n",
      "2018-04-13 14:34:35 iteration 1/6875: current training loss = 0.000586\n",
      "2018-04-13 14:34:37 iteration 625/6875: current training loss = 0.058890\n",
      "2018-04-13 14:34:40 iteration 1250/6875: current training loss = 0.016810\n",
      "2018-04-13 14:34:42 iteration 1875/6875: current training loss = 0.000829\n",
      "2018-04-13 14:34:44 iteration 2500/6875: current training loss = 0.002058\n",
      "2018-04-13 14:34:46 iteration 3125/6875: current training loss = 0.000877\n",
      "2018-04-13 14:34:48 iteration 3750/6875: current training loss = 0.002824\n",
      "2018-04-13 14:34:51 iteration 4375/6875: current training loss = 0.000047\n",
      "2018-04-13 14:34:53 iteration 5000/6875: current training loss = 0.025312\n",
      "2018-04-13 14:34:55 iteration 5625/6875: current training loss = 0.005125\n",
      "2018-04-13 14:34:57 iteration 6250/6875: current training loss = 0.004190\n",
      "2018-04-13 14:35:00 iteration 6875/6875: current training loss = 0.291474\n",
      "2018-04-13 14:35:05 end epoch 23/50: acc_train=97.775% acc_val=97.675% acc_test=97.150%\n",
      "2018-04-13 14:35:05 start epoch 24/50:\n",
      "2018-04-13 14:35:05 iteration 1/6875: current training loss = 0.020338\n",
      "2018-04-13 14:35:07 iteration 625/6875: current training loss = 0.011879\n",
      "2018-04-13 14:35:10 iteration 1250/6875: current training loss = 0.049713\n",
      "2018-04-13 14:35:12 iteration 1875/6875: current training loss = 0.000200\n",
      "2018-04-13 14:35:15 iteration 2500/6875: current training loss = 0.005986\n",
      "2018-04-13 14:35:17 iteration 3125/6875: current training loss = 0.008314\n",
      "2018-04-13 14:35:20 iteration 3750/6875: current training loss = 0.002440\n",
      "2018-04-13 14:35:22 iteration 4375/6875: current training loss = 0.000328\n",
      "2018-04-13 14:35:24 iteration 5000/6875: current training loss = 0.000697\n",
      "2018-04-13 14:35:26 iteration 5625/6875: current training loss = 0.000494\n",
      "2018-04-13 14:35:29 iteration 6250/6875: current training loss = 0.001137\n",
      "2018-04-13 14:35:31 iteration 6875/6875: current training loss = 0.002219\n",
      "2018-04-13 14:35:37 end epoch 24/50: acc_train=98.206% acc_val=97.825% acc_test=97.650%\n",
      "2018-04-13 14:35:37 start epoch 25/50:\n",
      "2018-04-13 14:35:37 iteration 1/6875: current training loss = 0.004037\n",
      "2018-04-13 14:35:39 iteration 625/6875: current training loss = 0.004091\n",
      "2018-04-13 14:35:41 iteration 1250/6875: current training loss = 0.003136\n",
      "2018-04-13 14:35:44 iteration 1875/6875: current training loss = 0.001698\n",
      "2018-04-13 14:35:46 iteration 2500/6875: current training loss = 0.027673\n",
      "2018-04-13 14:35:49 iteration 3125/6875: current training loss = 0.000913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 14:35:51 iteration 3750/6875: current training loss = 0.002139\n",
      "2018-04-13 14:35:54 iteration 4375/6875: current training loss = 0.004677\n",
      "2018-04-13 14:35:56 iteration 5000/6875: current training loss = 0.000437\n",
      "2018-04-13 14:35:58 iteration 5625/6875: current training loss = 0.013360\n",
      "2018-04-13 14:36:01 iteration 6250/6875: current training loss = 0.041612\n",
      "2018-04-13 14:36:03 iteration 6875/6875: current training loss = 0.004738\n",
      "2018-04-13 14:36:09 end epoch 25/50: acc_train=98.188% acc_val=98.125% acc_test=97.862%\n",
      "2018-04-13 14:36:09 start epoch 26/50:\n",
      "2018-04-13 14:36:09 iteration 1/6875: current training loss = 0.000549\n",
      "2018-04-13 14:36:12 iteration 625/6875: current training loss = 0.012672\n",
      "2018-04-13 14:36:14 iteration 1250/6875: current training loss = 0.001151\n",
      "2018-04-13 14:36:17 iteration 1875/6875: current training loss = 0.004615\n",
      "2018-04-13 14:36:19 iteration 2500/6875: current training loss = 0.000249\n",
      "2018-04-13 14:36:22 iteration 3125/6875: current training loss = 0.010432\n",
      "2018-04-13 14:36:24 iteration 3750/6875: current training loss = 0.010129\n",
      "2018-04-13 14:36:26 iteration 4375/6875: current training loss = 0.067428\n",
      "2018-04-13 14:36:29 iteration 5000/6875: current training loss = 0.001003\n",
      "2018-04-13 14:36:31 iteration 5625/6875: current training loss = 0.013244\n",
      "2018-04-13 14:36:34 iteration 6250/6875: current training loss = 0.019482\n",
      "2018-04-13 14:36:36 iteration 6875/6875: current training loss = 0.010711\n",
      "2018-04-13 14:36:42 end epoch 26/50: acc_train=97.750% acc_val=97.800% acc_test=97.288%\n",
      "2018-04-13 14:36:42 start epoch 27/50:\n",
      "2018-04-13 14:36:42 iteration 1/6875: current training loss = 0.001578\n",
      "2018-04-13 14:36:44 iteration 625/6875: current training loss = 0.000263\n",
      "2018-04-13 14:36:47 iteration 1250/6875: current training loss = 0.000294\n",
      "2018-04-13 14:36:49 iteration 1875/6875: current training loss = 0.000840\n",
      "2018-04-13 14:36:52 iteration 2500/6875: current training loss = 0.008285\n",
      "2018-04-13 14:36:54 iteration 3125/6875: current training loss = 0.000060\n",
      "2018-04-13 14:36:57 iteration 3750/6875: current training loss = 0.009754\n",
      "2018-04-13 14:36:59 iteration 4375/6875: current training loss = 0.032412\n",
      "2018-04-13 14:37:01 iteration 5000/6875: current training loss = 0.000060\n",
      "2018-04-13 14:37:04 iteration 5625/6875: current training loss = 0.003131\n",
      "2018-04-13 14:37:06 iteration 6250/6875: current training loss = 0.005211\n",
      "2018-04-13 14:37:09 iteration 6875/6875: current training loss = 0.021766\n",
      "2018-04-13 14:37:15 end epoch 27/50: acc_train=97.388% acc_val=97.800% acc_test=97.188%\n",
      "2018-04-13 14:37:15 start epoch 28/50:\n",
      "2018-04-13 14:37:15 iteration 1/6875: current training loss = 0.012694\n",
      "2018-04-13 14:37:17 iteration 625/6875: current training loss = 0.002308\n",
      "2018-04-13 14:37:20 iteration 1250/6875: current training loss = 0.000473\n",
      "2018-04-13 14:37:22 iteration 1875/6875: current training loss = 0.008702\n",
      "2018-04-13 14:37:25 iteration 2500/6875: current training loss = 0.053691\n",
      "2018-04-13 14:37:27 iteration 3125/6875: current training loss = 0.000274\n",
      "2018-04-13 14:37:30 iteration 3750/6875: current training loss = 0.006374\n",
      "2018-04-13 14:37:32 iteration 4375/6875: current training loss = 0.013220\n",
      "2018-04-13 14:37:35 iteration 5000/6875: current training loss = 0.000108\n",
      "2018-04-13 14:37:37 iteration 5625/6875: current training loss = 0.006749\n",
      "2018-04-13 14:37:40 iteration 6250/6875: current training loss = 0.087760\n",
      "2018-04-13 14:37:42 iteration 6875/6875: current training loss = 0.002087\n",
      "2018-04-13 14:37:48 end epoch 28/50: acc_train=97.869% acc_val=97.600% acc_test=97.325%\n",
      "2018-04-13 14:37:48 start epoch 29/50:\n",
      "2018-04-13 14:37:48 iteration 1/6875: current training loss = 0.011485\n",
      "2018-04-13 14:37:51 iteration 625/6875: current training loss = 0.003997\n",
      "2018-04-13 14:37:54 iteration 1250/6875: current training loss = 0.030377\n",
      "2018-04-13 14:37:56 iteration 1875/6875: current training loss = 0.007934\n",
      "2018-04-13 14:37:58 iteration 2500/6875: current training loss = 0.000555\n",
      "2018-04-13 14:38:01 iteration 3125/6875: current training loss = 0.006262\n",
      "2018-04-13 14:38:03 iteration 3750/6875: current training loss = 0.000250\n",
      "2018-04-13 14:38:06 iteration 4375/6875: current training loss = 0.012070\n",
      "2018-04-13 14:38:08 iteration 5000/6875: current training loss = 0.001051\n",
      "2018-04-13 14:38:11 iteration 5625/6875: current training loss = 0.000087\n",
      "2018-04-13 14:38:13 iteration 6250/6875: current training loss = 0.020461\n",
      "2018-04-13 14:38:15 iteration 6875/6875: current training loss = 0.006108\n",
      "2018-04-13 14:38:21 end epoch 29/50: acc_train=97.288% acc_val=97.150% acc_test=96.962%\n",
      "2018-04-13 14:38:21 start epoch 30/50:\n",
      "2018-04-13 14:38:21 iteration 1/6875: current training loss = 0.001255\n",
      "2018-04-13 14:38:24 iteration 625/6875: current training loss = 0.001180\n",
      "2018-04-13 14:38:26 iteration 1250/6875: current training loss = 0.001631\n",
      "2018-04-13 14:38:28 iteration 1875/6875: current training loss = 0.001611\n",
      "2018-04-13 14:38:31 iteration 2500/6875: current training loss = 0.013810\n",
      "2018-04-13 14:38:33 iteration 3125/6875: current training loss = 0.000151\n",
      "2018-04-13 14:38:35 iteration 3750/6875: current training loss = 0.000052\n",
      "2018-04-13 14:38:38 iteration 4375/6875: current training loss = 0.003055\n",
      "2018-04-13 14:38:40 iteration 5000/6875: current training loss = 0.000265\n",
      "2018-04-13 14:38:42 iteration 5625/6875: current training loss = 0.000251\n",
      "2018-04-13 14:38:45 iteration 6250/6875: current training loss = 0.000332\n",
      "2018-04-13 14:38:47 iteration 6875/6875: current training loss = 0.000622\n",
      "2018-04-13 14:38:54 end epoch 30/50: acc_train=98.081% acc_val=97.825% acc_test=97.575%\n",
      "2018-04-13 14:38:54 start epoch 31/50:\n",
      "2018-04-13 14:38:54 iteration 1/6875: current training loss = 0.000285\n",
      "2018-04-13 14:38:56 iteration 625/6875: current training loss = 0.032548\n",
      "2018-04-13 14:38:58 iteration 1250/6875: current training loss = 0.000423\n",
      "2018-04-13 14:39:01 iteration 1875/6875: current training loss = 0.000983\n",
      "2018-04-13 14:39:03 iteration 2500/6875: current training loss = 0.005965\n",
      "2018-04-13 14:39:05 iteration 3125/6875: current training loss = 0.000331\n",
      "2018-04-13 14:39:08 iteration 3750/6875: current training loss = 0.001644\n",
      "2018-04-13 14:39:10 iteration 4375/6875: current training loss = 0.002447\n",
      "2018-04-13 14:39:12 iteration 5000/6875: current training loss = 0.001095\n",
      "2018-04-13 14:39:15 iteration 5625/6875: current training loss = 0.002055\n",
      "2018-04-13 14:39:17 iteration 6250/6875: current training loss = 0.001630\n",
      "2018-04-13 14:39:20 iteration 6875/6875: current training loss = 0.014909\n",
      "2018-04-13 14:39:25 end epoch 31/50: acc_train=97.388% acc_val=97.450% acc_test=96.713%\n",
      "2018-04-13 14:39:25 start epoch 32/50:\n",
      "2018-04-13 14:39:25 iteration 1/6875: current training loss = 0.000484\n",
      "2018-04-13 14:39:27 iteration 625/6875: current training loss = 0.002589\n",
      "2018-04-13 14:39:30 iteration 1250/6875: current training loss = 0.000382\n",
      "2018-04-13 14:39:32 iteration 1875/6875: current training loss = 0.000249\n",
      "2018-04-13 14:39:35 iteration 2500/6875: current training loss = 0.000989\n",
      "2018-04-13 14:39:37 iteration 3125/6875: current training loss = 0.000473\n",
      "2018-04-13 14:39:40 iteration 3750/6875: current training loss = 0.002079\n",
      "2018-04-13 14:39:42 iteration 4375/6875: current training loss = 0.000583\n",
      "2018-04-13 14:39:45 iteration 5000/6875: current training loss = 0.000053\n",
      "2018-04-13 14:39:47 iteration 5625/6875: current training loss = 0.000226\n",
      "2018-04-13 14:39:49 iteration 6250/6875: current training loss = 0.012718\n",
      "2018-04-13 14:39:52 iteration 6875/6875: current training loss = 0.005962\n",
      "2018-04-13 14:39:57 end epoch 32/50: acc_train=97.750% acc_val=97.525% acc_test=97.062%\n",
      "2018-04-13 14:39:57 start epoch 33/50:\n",
      "2018-04-13 14:39:57 iteration 1/6875: current training loss = 0.000633\n",
      "2018-04-13 14:39:59 iteration 625/6875: current training loss = 0.002903\n",
      "2018-04-13 14:40:02 iteration 1250/6875: current training loss = 0.012089\n",
      "2018-04-13 14:40:04 iteration 1875/6875: current training loss = 0.006951\n",
      "2018-04-13 14:40:07 iteration 2500/6875: current training loss = 0.007440\n",
      "2018-04-13 14:40:09 iteration 3125/6875: current training loss = 0.000404\n",
      "2018-04-13 14:40:11 iteration 3750/6875: current training loss = 0.000192\n",
      "2018-04-13 14:40:14 iteration 4375/6875: current training loss = 0.000067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 14:40:16 iteration 5000/6875: current training loss = 0.000394\n",
      "2018-04-13 14:40:19 iteration 5625/6875: current training loss = 0.000115\n",
      "2018-04-13 14:40:21 iteration 6250/6875: current training loss = 0.001934\n",
      "2018-04-13 14:40:24 iteration 6875/6875: current training loss = 0.003934\n",
      "2018-04-13 14:40:30 end epoch 33/50: acc_train=97.662% acc_val=97.475% acc_test=97.375%\n",
      "2018-04-13 14:40:30 start epoch 34/50:\n",
      "2018-04-13 14:40:30 iteration 1/6875: current training loss = 0.024032\n",
      "2018-04-13 14:40:32 iteration 625/6875: current training loss = 0.011175\n",
      "2018-04-13 14:40:34 iteration 1250/6875: current training loss = 0.001976\n",
      "2018-04-13 14:40:37 iteration 1875/6875: current training loss = 0.001416\n",
      "2018-04-13 14:40:39 iteration 2500/6875: current training loss = 0.001149\n",
      "2018-04-13 14:40:41 iteration 3125/6875: current training loss = 0.067955\n",
      "2018-04-13 14:40:44 iteration 3750/6875: current training loss = 0.001565\n",
      "2018-04-13 14:40:46 iteration 4375/6875: current training loss = 0.000026\n",
      "2018-04-13 14:40:48 iteration 5000/6875: current training loss = 0.000223\n",
      "2018-04-13 14:40:51 iteration 5625/6875: current training loss = 0.003023\n",
      "2018-04-13 14:40:53 iteration 6250/6875: current training loss = 0.001322\n",
      "2018-04-13 14:40:55 iteration 6875/6875: current training loss = 0.002952\n",
      "2018-04-13 14:41:00 end epoch 34/50: acc_train=97.619% acc_val=97.100% acc_test=96.588%\n",
      "2018-04-13 14:41:00 start epoch 35/50:\n",
      "2018-04-13 14:41:00 iteration 1/6875: current training loss = 0.003436\n",
      "2018-04-13 14:41:02 iteration 625/6875: current training loss = 0.003717\n",
      "2018-04-13 14:41:05 iteration 1250/6875: current training loss = 0.000438\n",
      "2018-04-13 14:41:07 iteration 1875/6875: current training loss = 0.001885\n",
      "2018-04-13 14:41:10 iteration 2500/6875: current training loss = 0.000566\n",
      "2018-04-13 14:41:12 iteration 3125/6875: current training loss = 0.039820\n",
      "2018-04-13 14:41:14 iteration 3750/6875: current training loss = 0.005370\n",
      "2018-04-13 14:41:17 iteration 4375/6875: current training loss = 0.000550\n",
      "2018-04-13 14:41:19 iteration 5000/6875: current training loss = 0.001490\n",
      "2018-04-13 14:41:22 iteration 5625/6875: current training loss = 0.000090\n",
      "2018-04-13 14:41:24 iteration 6250/6875: current training loss = 0.004668\n",
      "2018-04-13 14:41:27 iteration 6875/6875: current training loss = 0.000213\n",
      "2018-04-13 14:41:32 end epoch 35/50: acc_train=97.862% acc_val=97.525% acc_test=97.288%\n",
      "2018-04-13 14:41:32 start epoch 36/50:\n",
      "2018-04-13 14:41:32 iteration 1/6875: current training loss = 0.038048\n",
      "2018-04-13 14:41:35 iteration 625/6875: current training loss = 0.000368\n",
      "2018-04-13 14:41:37 iteration 1250/6875: current training loss = 0.005868\n",
      "2018-04-13 14:41:40 iteration 1875/6875: current training loss = 0.001542\n",
      "2018-04-13 14:41:42 iteration 2500/6875: current training loss = 0.002922\n",
      "2018-04-13 14:41:45 iteration 3125/6875: current training loss = 0.000877\n",
      "2018-04-13 14:41:47 iteration 3750/6875: current training loss = 0.002779\n",
      "2018-04-13 14:41:50 iteration 4375/6875: current training loss = 0.000571\n",
      "2018-04-13 14:41:52 iteration 5000/6875: current training loss = 0.001937\n",
      "2018-04-13 14:41:55 iteration 5625/6875: current training loss = 0.000975\n",
      "2018-04-13 14:41:57 iteration 6250/6875: current training loss = 0.000096\n",
      "2018-04-13 14:42:00 iteration 6875/6875: current training loss = 0.000531\n",
      "2018-04-13 14:42:06 end epoch 36/50: acc_train=96.588% acc_val=96.575% acc_test=96.162%\n",
      "2018-04-13 14:42:06 start epoch 37/50:\n",
      "2018-04-13 14:42:06 iteration 1/6875: current training loss = 0.001895\n",
      "2018-04-13 14:42:08 iteration 625/6875: current training loss = 0.000140\n",
      "2018-04-13 14:42:11 iteration 1250/6875: current training loss = 0.000196\n",
      "2018-04-13 14:42:13 iteration 1875/6875: current training loss = 0.000031\n",
      "2018-04-13 14:42:16 iteration 2500/6875: current training loss = 0.000465\n",
      "2018-04-13 14:42:18 iteration 3125/6875: current training loss = 0.021232\n",
      "2018-04-13 14:42:21 iteration 3750/6875: current training loss = 0.000443\n",
      "2018-04-13 14:42:23 iteration 4375/6875: current training loss = 0.000077\n",
      "2018-04-13 14:42:26 iteration 5000/6875: current training loss = 0.000374\n",
      "2018-04-13 14:42:29 iteration 5625/6875: current training loss = 0.001690\n",
      "2018-04-13 14:42:31 iteration 6250/6875: current training loss = 0.001265\n",
      "2018-04-13 14:42:34 iteration 6875/6875: current training loss = 0.008532\n",
      "2018-04-13 14:42:41 end epoch 37/50: acc_train=96.825% acc_val=96.850% acc_test=95.837%\n",
      "2018-04-13 14:42:41 start epoch 38/50:\n",
      "2018-04-13 14:42:41 iteration 1/6875: current training loss = 0.003036\n",
      "2018-04-13 14:42:43 iteration 625/6875: current training loss = 0.000179\n",
      "2018-04-13 14:42:46 iteration 1250/6875: current training loss = 0.001090\n",
      "2018-04-13 14:42:48 iteration 1875/6875: current training loss = 0.000800\n",
      "2018-04-13 14:42:51 iteration 2500/6875: current training loss = 0.001103\n",
      "2018-04-13 14:42:53 iteration 3125/6875: current training loss = 0.000437\n",
      "2018-04-13 14:42:56 iteration 3750/6875: current training loss = 0.001981\n",
      "2018-04-13 14:42:58 iteration 4375/6875: current training loss = 0.000165\n",
      "2018-04-13 14:43:01 iteration 5000/6875: current training loss = 0.019242\n",
      "2018-04-13 14:43:04 iteration 5625/6875: current training loss = 0.000214\n",
      "2018-04-13 14:43:06 iteration 6250/6875: current training loss = 0.000782\n",
      "2018-04-13 14:43:09 iteration 6875/6875: current training loss = 0.000335\n",
      "2018-04-13 14:43:15 end epoch 38/50: acc_train=96.662% acc_val=96.975% acc_test=96.025%\n",
      "2018-04-13 14:43:15 start epoch 39/50:\n",
      "2018-04-13 14:43:15 iteration 1/6875: current training loss = 0.001990\n",
      "2018-04-13 14:43:18 iteration 625/6875: current training loss = 0.002575\n",
      "2018-04-13 14:43:20 iteration 1250/6875: current training loss = 0.000012\n",
      "2018-04-13 14:43:23 iteration 1875/6875: current training loss = 0.000188\n",
      "2018-04-13 14:43:25 iteration 2500/6875: current training loss = 0.000381\n",
      "2018-04-13 14:43:28 iteration 3125/6875: current training loss = 0.000084\n",
      "2018-04-13 14:43:30 iteration 3750/6875: current training loss = 0.001102\n",
      "2018-04-13 14:43:33 iteration 4375/6875: current training loss = 0.002090\n",
      "2018-04-13 14:43:35 iteration 5000/6875: current training loss = 0.007207\n",
      "2018-04-13 14:43:38 iteration 5625/6875: current training loss = 0.000407\n",
      "2018-04-13 14:43:40 iteration 6250/6875: current training loss = 0.000078\n",
      "2018-04-13 14:43:43 iteration 6875/6875: current training loss = 0.000005\n",
      "2018-04-13 14:43:49 end epoch 39/50: acc_train=97.562% acc_val=97.075% acc_test=96.725%\n",
      "2018-04-13 14:43:49 start epoch 40/50:\n",
      "2018-04-13 14:43:49 iteration 1/6875: current training loss = 0.001353\n",
      "2018-04-13 14:43:52 iteration 625/6875: current training loss = 0.004484\n",
      "2018-04-13 14:43:54 iteration 1250/6875: current training loss = 0.000079\n",
      "2018-04-13 14:43:57 iteration 1875/6875: current training loss = 0.006022\n",
      "2018-04-13 14:43:59 iteration 2500/6875: current training loss = 0.001888\n",
      "2018-04-13 14:44:02 iteration 3125/6875: current training loss = 0.000245\n",
      "2018-04-13 14:44:04 iteration 3750/6875: current training loss = 0.003213\n",
      "2018-04-13 14:44:07 iteration 4375/6875: current training loss = 0.000263\n",
      "2018-04-13 14:44:10 iteration 5000/6875: current training loss = 0.000108\n",
      "2018-04-13 14:44:12 iteration 5625/6875: current training loss = 0.003701\n",
      "2018-04-13 14:44:15 iteration 6250/6875: current training loss = 0.000161\n",
      "2018-04-13 14:44:17 iteration 6875/6875: current training loss = 0.000060\n",
      "2018-04-13 14:44:24 end epoch 40/50: acc_train=96.181% acc_val=96.400% acc_test=95.487%\n",
      "2018-04-13 14:44:24 start epoch 41/50:\n",
      "2018-04-13 14:44:24 iteration 1/6875: current training loss = 0.000203\n",
      "2018-04-13 14:44:26 iteration 625/6875: current training loss = 0.000072\n",
      "2018-04-13 14:44:29 iteration 1250/6875: current training loss = 0.001655\n",
      "2018-04-13 14:44:31 iteration 1875/6875: current training loss = 0.000756\n",
      "2018-04-13 14:44:34 iteration 2500/6875: current training loss = 0.021864\n",
      "2018-04-13 14:44:37 iteration 3125/6875: current training loss = 0.000165\n",
      "2018-04-13 14:44:39 iteration 3750/6875: current training loss = 0.000559\n",
      "2018-04-13 14:44:42 iteration 4375/6875: current training loss = 0.003980\n",
      "2018-04-13 14:44:44 iteration 5000/6875: current training loss = 0.002037\n",
      "2018-04-13 14:44:47 iteration 5625/6875: current training loss = 0.000154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 14:44:50 iteration 6250/6875: current training loss = 0.000060\n",
      "2018-04-13 14:44:52 iteration 6875/6875: current training loss = 0.000382\n",
      "2018-04-13 14:44:59 end epoch 41/50: acc_train=96.956% acc_val=96.900% acc_test=96.350%\n",
      "2018-04-13 14:44:59 start epoch 42/50:\n",
      "2018-04-13 14:44:59 iteration 1/6875: current training loss = 0.000237\n",
      "2018-04-13 14:45:01 iteration 625/6875: current training loss = 0.001052\n",
      "2018-04-13 14:45:04 iteration 1250/6875: current training loss = 0.000064\n",
      "2018-04-13 14:45:07 iteration 1875/6875: current training loss = 0.000188\n",
      "2018-04-13 14:45:09 iteration 2500/6875: current training loss = 0.000979\n",
      "2018-04-13 14:45:12 iteration 3125/6875: current training loss = 0.000924\n",
      "2018-04-13 14:45:14 iteration 3750/6875: current training loss = 0.000034\n",
      "2018-04-13 14:45:17 iteration 4375/6875: current training loss = 0.003968\n",
      "2018-04-13 14:45:19 iteration 5000/6875: current training loss = 0.000298\n",
      "2018-04-13 14:45:22 iteration 5625/6875: current training loss = 0.000362\n",
      "2018-04-13 14:45:25 iteration 6250/6875: current training loss = 0.000112\n",
      "2018-04-13 14:45:27 iteration 6875/6875: current training loss = 0.004398\n",
      "2018-04-13 14:45:34 end epoch 42/50: acc_train=95.544% acc_val=96.000% acc_test=94.863%\n",
      "2018-04-13 14:45:34 start epoch 43/50:\n",
      "2018-04-13 14:45:34 iteration 1/6875: current training loss = 0.003124\n",
      "2018-04-13 14:45:36 iteration 625/6875: current training loss = 0.003418\n",
      "2018-04-13 14:45:39 iteration 1250/6875: current training loss = 0.004812\n",
      "2018-04-13 14:45:41 iteration 1875/6875: current training loss = 0.000070\n",
      "2018-04-13 14:45:44 iteration 2500/6875: current training loss = 0.000046\n",
      "2018-04-13 14:45:46 iteration 3125/6875: current training loss = 0.003314\n",
      "2018-04-13 14:45:49 iteration 3750/6875: current training loss = 0.000007\n",
      "2018-04-13 14:45:51 iteration 4375/6875: current training loss = 0.000060\n",
      "2018-04-13 14:45:54 iteration 5000/6875: current training loss = 0.004694\n",
      "2018-04-13 14:45:56 iteration 5625/6875: current training loss = 0.011237\n",
      "2018-04-13 14:45:59 iteration 6250/6875: current training loss = 0.000015\n",
      "2018-04-13 14:46:01 iteration 6875/6875: current training loss = 0.000812\n",
      "2018-04-13 14:46:08 end epoch 43/50: acc_train=96.856% acc_val=97.150% acc_test=96.188%\n",
      "2018-04-13 14:46:08 start epoch 44/50:\n",
      "2018-04-13 14:46:08 iteration 1/6875: current training loss = 0.005711\n",
      "2018-04-13 14:46:10 iteration 625/6875: current training loss = 0.000063\n",
      "2018-04-13 14:46:13 iteration 1250/6875: current training loss = 0.000089\n",
      "2018-04-13 14:46:15 iteration 1875/6875: current training loss = 0.000508\n",
      "2018-04-13 14:46:18 iteration 2500/6875: current training loss = 0.000366\n",
      "2018-04-13 14:46:20 iteration 3125/6875: current training loss = 0.002561\n",
      "2018-04-13 14:46:23 iteration 3750/6875: current training loss = 0.000027\n",
      "2018-04-13 14:46:26 iteration 4375/6875: current training loss = 0.000039\n",
      "2018-04-13 14:46:28 iteration 5000/6875: current training loss = 0.000007\n",
      "2018-04-13 14:46:31 iteration 5625/6875: current training loss = 0.000075\n",
      "2018-04-13 14:46:33 iteration 6250/6875: current training loss = 0.001104\n",
      "2018-04-13 14:46:36 iteration 6875/6875: current training loss = 0.000001\n",
      "2018-04-13 14:46:43 end epoch 44/50: acc_train=95.612% acc_val=95.575% acc_test=95.100%\n",
      "2018-04-13 14:46:43 start epoch 45/50:\n",
      "2018-04-13 14:46:43 iteration 1/6875: current training loss = 0.000413\n",
      "2018-04-13 14:46:45 iteration 625/6875: current training loss = 0.000045\n",
      "2018-04-13 14:46:48 iteration 1250/6875: current training loss = 0.000430\n",
      "2018-04-13 14:46:50 iteration 1875/6875: current training loss = 0.004463\n",
      "2018-04-13 14:46:53 iteration 2500/6875: current training loss = 0.001332\n",
      "2018-04-13 14:46:55 iteration 3125/6875: current training loss = 0.001437\n",
      "2018-04-13 14:46:58 iteration 3750/6875: current training loss = 0.002416\n",
      "2018-04-13 14:47:00 iteration 4375/6875: current training loss = 0.000054\n",
      "2018-04-13 14:47:03 iteration 5000/6875: current training loss = 0.000014\n",
      "2018-04-13 14:47:06 iteration 5625/6875: current training loss = 0.000011\n",
      "2018-04-13 14:47:08 iteration 6250/6875: current training loss = 0.002597\n",
      "2018-04-13 14:47:11 iteration 6875/6875: current training loss = 0.000146\n",
      "2018-04-13 14:47:17 end epoch 45/50: acc_train=95.412% acc_val=95.850% acc_test=94.537%\n",
      "2018-04-13 14:47:17 start epoch 46/50:\n",
      "2018-04-13 14:47:17 iteration 1/6875: current training loss = 0.005018\n",
      "2018-04-13 14:47:20 iteration 625/6875: current training loss = 0.000059\n",
      "2018-04-13 14:47:22 iteration 1250/6875: current training loss = 0.000163\n",
      "2018-04-13 14:47:25 iteration 1875/6875: current training loss = 0.004875\n",
      "2018-04-13 14:47:27 iteration 2500/6875: current training loss = 0.000012\n",
      "2018-04-13 14:47:30 iteration 3125/6875: current training loss = 0.000034\n",
      "2018-04-13 14:47:33 iteration 3750/6875: current training loss = 0.000034\n",
      "2018-04-13 14:47:35 iteration 4375/6875: current training loss = 0.000020\n",
      "2018-04-13 14:47:38 iteration 5000/6875: current training loss = 0.000016\n",
      "2018-04-13 14:47:40 iteration 5625/6875: current training loss = 0.000054\n",
      "2018-04-13 14:47:43 iteration 6250/6875: current training loss = 0.001241\n",
      "2018-04-13 14:47:45 iteration 6875/6875: current training loss = 0.000026\n",
      "2018-04-13 14:47:51 end epoch 46/50: acc_train=93.637% acc_val=94.250% acc_test=92.925%\n",
      "2018-04-13 14:47:51 start epoch 47/50:\n",
      "2018-04-13 14:47:51 iteration 1/6875: current training loss = 0.000370\n",
      "2018-04-13 14:47:54 iteration 625/6875: current training loss = 0.000010\n",
      "2018-04-13 14:47:56 iteration 1250/6875: current training loss = 0.000320\n",
      "2018-04-13 14:47:59 iteration 1875/6875: current training loss = 0.000043\n",
      "2018-04-13 14:48:01 iteration 2500/6875: current training loss = 0.000422\n",
      "2018-04-13 14:48:04 iteration 3125/6875: current training loss = 0.002106\n",
      "2018-04-13 14:48:07 iteration 3750/6875: current training loss = 0.002522\n",
      "2018-04-13 14:48:09 iteration 4375/6875: current training loss = 0.000606\n",
      "2018-04-13 14:48:12 iteration 5000/6875: current training loss = 0.000050\n",
      "2018-04-13 14:48:14 iteration 5625/6875: current training loss = 0.000135\n",
      "2018-04-13 14:48:17 iteration 6250/6875: current training loss = 0.000544\n",
      "2018-04-13 14:48:19 iteration 6875/6875: current training loss = 0.000341\n",
      "2018-04-13 14:48:26 end epoch 47/50: acc_train=94.569% acc_val=94.850% acc_test=94.075%\n",
      "2018-04-13 14:48:26 start epoch 48/50:\n",
      "2018-04-13 14:48:26 iteration 1/6875: current training loss = 0.000632\n",
      "2018-04-13 14:48:28 iteration 625/6875: current training loss = 0.000305\n",
      "2018-04-13 14:48:31 iteration 1250/6875: current training loss = 0.000027\n",
      "2018-04-13 14:48:33 iteration 1875/6875: current training loss = 0.004057\n",
      "2018-04-13 14:48:36 iteration 2500/6875: current training loss = 0.000048\n",
      "2018-04-13 14:48:38 iteration 3125/6875: current training loss = 0.013880\n",
      "2018-04-13 14:48:41 iteration 3750/6875: current training loss = 0.000017\n",
      "2018-04-13 14:48:43 iteration 4375/6875: current training loss = 0.000029\n",
      "2018-04-13 14:48:46 iteration 5000/6875: current training loss = 0.001272\n",
      "2018-04-13 14:48:48 iteration 5625/6875: current training loss = 0.000023\n",
      "2018-04-13 14:48:51 iteration 6250/6875: current training loss = 0.000504\n",
      "2018-04-13 14:48:53 iteration 6875/6875: current training loss = 0.000250\n",
      "2018-04-13 14:49:00 end epoch 48/50: acc_train=94.050% acc_val=95.150% acc_test=93.312%\n",
      "2018-04-13 14:49:00 start epoch 49/50:\n",
      "2018-04-13 14:49:00 iteration 1/6875: current training loss = 0.001169\n",
      "2018-04-13 14:49:03 iteration 625/6875: current training loss = 0.000317\n",
      "2018-04-13 14:49:05 iteration 1250/6875: current training loss = 0.005564\n",
      "2018-04-13 14:49:08 iteration 1875/6875: current training loss = 0.000004\n",
      "2018-04-13 14:49:10 iteration 2500/6875: current training loss = 0.000846\n",
      "2018-04-13 14:49:13 iteration 3125/6875: current training loss = 0.000217\n",
      "2018-04-13 14:49:15 iteration 3750/6875: current training loss = 0.000109\n",
      "2018-04-13 14:49:18 iteration 4375/6875: current training loss = 0.000598\n",
      "2018-04-13 14:49:21 iteration 5000/6875: current training loss = 0.000043\n",
      "2018-04-13 14:49:23 iteration 5625/6875: current training loss = 0.000013\n",
      "2018-04-13 14:49:26 iteration 6250/6875: current training loss = 0.000830\n",
      "2018-04-13 14:49:28 iteration 6875/6875: current training loss = 0.000555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 14:49:34 end epoch 49/50: acc_train=94.819% acc_val=94.975% acc_test=94.350%\n",
      "2018-04-13 14:49:34 start epoch 50/50:\n",
      "2018-04-13 14:49:34 iteration 1/6875: current training loss = 0.000124\n",
      "2018-04-13 14:49:37 iteration 625/6875: current training loss = 0.000009\n",
      "2018-04-13 14:49:39 iteration 1250/6875: current training loss = 0.000018\n",
      "2018-04-13 14:49:42 iteration 1875/6875: current training loss = 0.000174\n",
      "2018-04-13 14:49:44 iteration 2500/6875: current training loss = 0.000576\n",
      "2018-04-13 14:49:47 iteration 3125/6875: current training loss = 0.000214\n",
      "2018-04-13 14:49:49 iteration 3750/6875: current training loss = 0.000029\n",
      "2018-04-13 14:49:51 iteration 4375/6875: current training loss = 0.001677\n",
      "2018-04-13 14:49:54 iteration 5000/6875: current training loss = 0.003341\n",
      "2018-04-13 14:49:57 iteration 5625/6875: current training loss = 0.001035\n",
      "2018-04-13 14:49:59 iteration 6250/6875: current training loss = 0.000051\n",
      "2018-04-13 14:50:02 iteration 6875/6875: current training loss = 0.000758\n",
      "2018-04-13 14:50:08 end epoch 50/50: acc_train=94.713% acc_val=95.025% acc_test=93.812%\n"
     ]
    }
   ],
   "source": [
    "max_epoch=50\n",
    "batch_size=8\n",
    "print_every=625\n",
    "\n",
    "def train():\n",
    "    num_iteration=num_train//batch_size\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=mnist.train.next_batch(batch_size)\n",
    "        loss_num,_ = sess.run([loss,train_step],feed_dict={X:images.reshape(-1,28,28,1),y:labels,is_training:True})\n",
    "        if it==0 or (it+1)%print_every==0 or it==num_iteration-1:\n",
    "            print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "                  'iteration %d/%d:' % (it+1,num_iteration),'current training loss = %f' % (loss_num))\n",
    "            \n",
    "def eval(dataset,num_iteration):\n",
    "    total_loss=0\n",
    "    total_accuracy=0\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=dataset.next_batch(batch_size)\n",
    "        loss_num,accuracy_num = sess.run([loss,accuracy],feed_dict={X:images.reshape(-1,28,28,1),y:labels,is_training:False})\n",
    "        total_loss+=loss_num\n",
    "        total_accuracy+=accuracy_num\n",
    "    total_loss/=num_iteration\n",
    "    total_accuracy/=num_iteration\n",
    "    return total_loss,total_accuracy\n",
    "    \n",
    "acc_train_his=[]\n",
    "acc_val_his=[]\n",
    "acc_test_his=[]\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(max_epoch):\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'start epoch %d/%d:' % (epoch+1,max_epoch))\n",
    "        train()\n",
    "        loss_train,acc_train=eval(mnist.train,2000)\n",
    "        loss_val,acc_val=eval(mnist.validation,500)\n",
    "        loss_test,acc_test=eval(mnist.test,1000)\n",
    "        acc_train_his.append(acc_train)\n",
    "        acc_val_his.append(acc_val)\n",
    "        acc_test_his.append(acc_test)\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'end epoch %d/%d:' % (epoch+1,max_epoch),\n",
    "             'acc_train=%.3f%% acc_val=%.3f%% acc_test=%.3f%%' % (acc_train*100.0,acc_val*100.0,acc_test*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "ptr,=plt.plot(range(max_epoch),acc_train_his,'r-')\n",
    "pva,=plt.plot(range(max_epoch),acc_val_his,'b-')\n",
    "pte,=plt.plot(range(max_epoch),acc_test_his,'y-')\n",
    "plt.xlabel('training epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('accuracy on three sets')\n",
    "plt.legend((ptr,pva,pte),('train','validation','test'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Soft attention method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "img_size=28\n",
    "RNN_unit=img_size*img_size\n",
    "N_watch=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(dtype=tf.float32,shape=[None,img_size*img_size])\n",
    "y=tf.placeholder(dtype=tf.int64,shape=[None,10])\n",
    "\n",
    "predict_net=tf.layers.Dense(units=10)\n",
    "\n",
    "def get_next_input(output, i):\n",
    "    attention_weight=tf.nn.softmax(output)\n",
    "    weighted_graph=X*attention_weight\n",
    "    return weighted_graph\n",
    "\n",
    "lstm_cell = tf.nn.rnn_cell.LSTMCell(RNN_unit, state_is_tuple=True)\n",
    "init_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "inputs=[X]\n",
    "inputs.extend([0]*N_watch)\n",
    "outputs,_ = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, init_state, lstm_cell, loop_function=get_next_input)\n",
    "\n",
    "output=outputs[-1]\n",
    "score=predict_net(output)\n",
    "\n",
    "predictions = tf.argmax(score, 1)\n",
    "correct_predictions = tf.equal(predictions, tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "\n",
    "loss=tf.losses.softmax_cross_entropy(onehot_labels=y,logits=score)\n",
    "optimizier=tf.train.AdamOptimizer(learning_rate=1e-5)\n",
    "train_step = optimizier.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 14:50:09 start epoch 1/50:\n",
      "2018-04-13 14:50:09 iteration 1/859: current training loss = 2.302991\n",
      "2018-04-13 14:50:11 iteration 200/859: current training loss = 1.958197\n",
      "2018-04-13 14:50:13 iteration 400/859: current training loss = 0.734439\n",
      "2018-04-13 14:50:15 iteration 600/859: current training loss = 0.487307\n",
      "2018-04-13 14:50:17 iteration 800/859: current training loss = 0.396553\n",
      "2018-04-13 14:50:18 iteration 859/859: current training loss = 0.583968\n",
      "2018-04-13 14:50:32 end epoch 1/50: acc_train=86.005% acc_val=86.941% acc_test=86.416%\n",
      "2018-04-13 14:50:32 start epoch 2/50:\n",
      "2018-04-13 14:50:32 iteration 1/859: current training loss = 0.454555\n",
      "2018-04-13 14:50:35 iteration 200/859: current training loss = 0.498903\n",
      "2018-04-13 14:50:37 iteration 400/859: current training loss = 0.231672\n",
      "2018-04-13 14:50:39 iteration 600/859: current training loss = 0.328378\n",
      "2018-04-13 14:50:41 iteration 800/859: current training loss = 0.143264\n",
      "2018-04-13 14:50:41 iteration 859/859: current training loss = 0.307050\n",
      "2018-04-13 14:50:55 end epoch 2/50: acc_train=90.261% acc_val=90.931% acc_test=90.409%\n",
      "2018-04-13 14:50:55 start epoch 3/50:\n",
      "2018-04-13 14:50:55 iteration 1/859: current training loss = 0.382305\n",
      "2018-04-13 14:50:57 iteration 200/859: current training loss = 0.519005\n",
      "2018-04-13 14:50:59 iteration 400/859: current training loss = 0.377961\n",
      "2018-04-13 14:51:00 iteration 600/859: current training loss = 0.273476\n",
      "2018-04-13 14:51:03 iteration 800/859: current training loss = 0.187729\n",
      "2018-04-13 14:51:03 iteration 859/859: current training loss = 0.331414\n",
      "2018-04-13 14:51:17 end epoch 3/50: acc_train=91.792% acc_val=92.209% acc_test=92.038%\n",
      "2018-04-13 14:51:17 start epoch 4/50:\n",
      "2018-04-13 14:51:17 iteration 1/859: current training loss = 0.269534\n",
      "2018-04-13 14:51:19 iteration 200/859: current training loss = 0.159894\n",
      "2018-04-13 14:51:21 iteration 400/859: current training loss = 0.204768\n",
      "2018-04-13 14:51:23 iteration 600/859: current training loss = 0.141616\n",
      "2018-04-13 14:51:25 iteration 800/859: current training loss = 0.086708\n",
      "2018-04-13 14:51:26 iteration 859/859: current training loss = 0.126386\n",
      "2018-04-13 14:51:39 end epoch 4/50: acc_train=92.792% acc_val=93.041% acc_test=92.905%\n",
      "2018-04-13 14:51:39 start epoch 5/50:\n",
      "2018-04-13 14:51:39 iteration 1/859: current training loss = 0.333589\n",
      "2018-04-13 14:51:41 iteration 200/859: current training loss = 0.124479\n",
      "2018-04-13 14:51:43 iteration 400/859: current training loss = 0.151962\n",
      "2018-04-13 14:51:45 iteration 600/859: current training loss = 0.304553\n",
      "2018-04-13 14:51:47 iteration 800/859: current training loss = 0.372117\n",
      "2018-04-13 14:51:47 iteration 859/859: current training loss = 0.127149\n",
      "2018-04-13 14:51:59 end epoch 5/50: acc_train=93.737% acc_val=93.903% acc_test=93.297%\n",
      "2018-04-13 14:51:59 start epoch 6/50:\n",
      "2018-04-13 14:51:59 iteration 1/859: current training loss = 0.433277\n",
      "2018-04-13 14:52:01 iteration 200/859: current training loss = 0.076201\n",
      "2018-04-13 14:52:03 iteration 400/859: current training loss = 0.211479\n",
      "2018-04-13 14:52:05 iteration 600/859: current training loss = 0.120657\n",
      "2018-04-13 14:52:07 iteration 800/859: current training loss = 0.248616\n",
      "2018-04-13 14:52:08 iteration 859/859: current training loss = 0.377153\n",
      "2018-04-13 14:52:21 end epoch 6/50: acc_train=94.246% acc_val=94.509% acc_test=93.784%\n",
      "2018-04-13 14:52:21 start epoch 7/50:\n",
      "2018-04-13 14:52:21 iteration 1/859: current training loss = 0.023454\n",
      "2018-04-13 14:52:23 iteration 200/859: current training loss = 0.197287\n",
      "2018-04-13 14:52:25 iteration 400/859: current training loss = 0.104496\n",
      "2018-04-13 14:52:27 iteration 600/859: current training loss = 0.232072\n",
      "2018-04-13 14:52:29 iteration 800/859: current training loss = 0.185178\n",
      "2018-04-13 14:52:30 iteration 859/859: current training loss = 0.176439\n",
      "2018-04-13 14:52:42 end epoch 7/50: acc_train=94.559% acc_val=94.772% acc_test=94.316%\n",
      "2018-04-13 14:52:42 start epoch 8/50:\n",
      "2018-04-13 14:52:42 iteration 1/859: current training loss = 0.079101\n",
      "2018-04-13 14:52:44 iteration 200/859: current training loss = 0.074578\n",
      "2018-04-13 14:52:46 iteration 400/859: current training loss = 0.123563\n",
      "2018-04-13 14:52:48 iteration 600/859: current training loss = 0.209457\n",
      "2018-04-13 14:52:50 iteration 800/859: current training loss = 0.066756\n",
      "2018-04-13 14:52:51 iteration 859/859: current training loss = 0.387732\n",
      "2018-04-13 14:53:03 end epoch 8/50: acc_train=95.225% acc_val=95.159% acc_test=94.703%\n",
      "2018-04-13 14:53:03 start epoch 9/50:\n",
      "2018-04-13 14:53:03 iteration 1/859: current training loss = 0.091828\n",
      "2018-04-13 14:53:04 iteration 200/859: current training loss = 0.176807\n",
      "2018-04-13 14:53:06 iteration 400/859: current training loss = 0.272869\n",
      "2018-04-13 14:53:08 iteration 600/859: current training loss = 0.135756\n",
      "2018-04-13 14:53:10 iteration 800/859: current training loss = 0.305639\n",
      "2018-04-13 14:53:10 iteration 859/859: current training loss = 0.176760\n",
      "2018-04-13 14:53:23 end epoch 9/50: acc_train=95.455% acc_val=95.253% acc_test=94.811%\n",
      "2018-04-13 14:53:23 start epoch 10/50:\n",
      "2018-04-13 14:53:23 iteration 1/859: current training loss = 0.064245\n",
      "2018-04-13 14:53:24 iteration 200/859: current training loss = 0.097692\n",
      "2018-04-13 14:53:26 iteration 400/859: current training loss = 0.146471\n",
      "2018-04-13 14:53:28 iteration 600/859: current training loss = 0.112273\n",
      "2018-04-13 14:53:30 iteration 800/859: current training loss = 0.270256\n",
      "2018-04-13 14:53:31 iteration 859/859: current training loss = 0.135037\n",
      "2018-04-13 14:53:43 end epoch 10/50: acc_train=95.611% acc_val=95.469% acc_test=95.139%\n",
      "2018-04-13 14:53:43 start epoch 11/50:\n",
      "2018-04-13 14:53:43 iteration 1/859: current training loss = 0.200523\n",
      "2018-04-13 14:53:44 iteration 200/859: current training loss = 0.053022\n",
      "2018-04-13 14:53:46 iteration 400/859: current training loss = 0.083317\n",
      "2018-04-13 14:53:48 iteration 600/859: current training loss = 0.380005\n",
      "2018-04-13 14:53:50 iteration 800/859: current training loss = 0.138787\n",
      "2018-04-13 14:53:50 iteration 859/859: current training loss = 0.250486\n",
      "2018-04-13 14:54:02 end epoch 11/50: acc_train=95.845% acc_val=95.628% acc_test=95.389%\n",
      "2018-04-13 14:54:02 start epoch 12/50:\n",
      "2018-04-13 14:54:02 iteration 1/859: current training loss = 0.213556\n",
      "2018-04-13 14:54:04 iteration 200/859: current training loss = 0.056055\n",
      "2018-04-13 14:54:06 iteration 400/859: current training loss = 0.050787\n",
      "2018-04-13 14:54:08 iteration 600/859: current training loss = 0.101741\n",
      "2018-04-13 14:54:10 iteration 800/859: current training loss = 0.131147\n",
      "2018-04-13 14:54:10 iteration 859/859: current training loss = 0.119591\n",
      "2018-04-13 14:54:22 end epoch 12/50: acc_train=96.096% acc_val=95.875% acc_test=95.500%\n",
      "2018-04-13 14:54:22 start epoch 13/50:\n",
      "2018-04-13 14:54:22 iteration 1/859: current training loss = 0.222170\n",
      "2018-04-13 14:54:24 iteration 200/859: current training loss = 0.210920\n",
      "2018-04-13 14:54:26 iteration 400/859: current training loss = 0.155579\n",
      "2018-04-13 14:54:28 iteration 600/859: current training loss = 0.212203\n",
      "2018-04-13 14:54:30 iteration 800/859: current training loss = 0.055983\n",
      "2018-04-13 14:54:30 iteration 859/859: current training loss = 0.054517\n",
      "2018-04-13 14:54:43 end epoch 13/50: acc_train=96.275% acc_val=95.900% acc_test=95.661%\n",
      "2018-04-13 14:54:43 start epoch 14/50:\n",
      "2018-04-13 14:54:43 iteration 1/859: current training loss = 0.064722\n",
      "2018-04-13 14:54:45 iteration 200/859: current training loss = 0.114340\n",
      "2018-04-13 14:54:47 iteration 400/859: current training loss = 0.053177\n",
      "2018-04-13 14:54:48 iteration 600/859: current training loss = 0.180862\n",
      "2018-04-13 14:54:50 iteration 800/859: current training loss = 0.057921\n",
      "2018-04-13 14:54:51 iteration 859/859: current training loss = 0.064994\n",
      "2018-04-13 14:55:04 end epoch 14/50: acc_train=96.416% acc_val=96.188% acc_test=95.864%\n",
      "2018-04-13 14:55:04 start epoch 15/50:\n",
      "2018-04-13 14:55:04 iteration 1/859: current training loss = 0.141439\n",
      "2018-04-13 14:55:06 iteration 200/859: current training loss = 0.025865\n",
      "2018-04-13 14:55:08 iteration 400/859: current training loss = 0.060306\n",
      "2018-04-13 14:55:09 iteration 600/859: current training loss = 0.022542\n",
      "2018-04-13 14:55:11 iteration 800/859: current training loss = 0.069367\n",
      "2018-04-13 14:55:12 iteration 859/859: current training loss = 0.082772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 14:55:23 end epoch 15/50: acc_train=96.934% acc_val=96.562% acc_test=96.119%\n",
      "2018-04-13 14:55:23 start epoch 16/50:\n",
      "2018-04-13 14:55:23 iteration 1/859: current training loss = 0.122094\n",
      "2018-04-13 14:55:25 iteration 200/859: current training loss = 0.183912\n",
      "2018-04-13 14:55:27 iteration 400/859: current training loss = 0.030242\n",
      "2018-04-13 14:55:29 iteration 600/859: current training loss = 0.077744\n",
      "2018-04-13 14:55:30 iteration 800/859: current training loss = 0.075538\n",
      "2018-04-13 14:55:31 iteration 859/859: current training loss = 0.031497\n",
      "2018-04-13 14:55:42 end epoch 16/50: acc_train=96.975% acc_val=96.544% acc_test=96.125%\n",
      "2018-04-13 14:55:42 start epoch 17/50:\n",
      "2018-04-13 14:55:42 iteration 1/859: current training loss = 0.165149\n",
      "2018-04-13 14:55:44 iteration 200/859: current training loss = 0.184565\n",
      "2018-04-13 14:55:46 iteration 400/859: current training loss = 0.127650\n",
      "2018-04-13 14:55:48 iteration 600/859: current training loss = 0.124863\n",
      "2018-04-13 14:55:49 iteration 800/859: current training loss = 0.105109\n",
      "2018-04-13 14:55:50 iteration 859/859: current training loss = 0.039132\n",
      "2018-04-13 14:56:03 end epoch 17/50: acc_train=97.237% acc_val=96.456% acc_test=96.369%\n",
      "2018-04-13 14:56:03 start epoch 18/50:\n",
      "2018-04-13 14:56:03 iteration 1/859: current training loss = 0.167120\n",
      "2018-04-13 14:56:05 iteration 200/859: current training loss = 0.014951\n",
      "2018-04-13 14:56:07 iteration 400/859: current training loss = 0.116834\n",
      "2018-04-13 14:56:09 iteration 600/859: current training loss = 0.048329\n",
      "2018-04-13 14:56:10 iteration 800/859: current training loss = 0.149147\n",
      "2018-04-13 14:56:11 iteration 859/859: current training loss = 0.041290\n",
      "2018-04-13 14:56:24 end epoch 18/50: acc_train=97.321% acc_val=96.831% acc_test=96.347%\n",
      "2018-04-13 14:56:24 start epoch 19/50:\n",
      "2018-04-13 14:56:24 iteration 1/859: current training loss = 0.047455\n",
      "2018-04-13 14:56:26 iteration 200/859: current training loss = 0.052583\n",
      "2018-04-13 14:56:27 iteration 400/859: current training loss = 0.082792\n",
      "2018-04-13 14:56:29 iteration 600/859: current training loss = 0.148223\n",
      "2018-04-13 14:56:31 iteration 800/859: current training loss = 0.143153\n",
      "2018-04-13 14:56:31 iteration 859/859: current training loss = 0.091490\n",
      "2018-04-13 14:56:44 end epoch 19/50: acc_train=97.353% acc_val=96.838% acc_test=96.372%\n",
      "2018-04-13 14:56:44 start epoch 20/50:\n",
      "2018-04-13 14:56:44 iteration 1/859: current training loss = 0.085378\n",
      "2018-04-13 14:56:45 iteration 200/859: current training loss = 0.057024\n",
      "2018-04-13 14:56:47 iteration 400/859: current training loss = 0.040548\n",
      "2018-04-13 14:56:49 iteration 600/859: current training loss = 0.095276\n",
      "2018-04-13 14:56:51 iteration 800/859: current training loss = 0.053451\n",
      "2018-04-13 14:56:52 iteration 859/859: current training loss = 0.038526\n",
      "2018-04-13 14:57:03 end epoch 20/50: acc_train=97.613% acc_val=96.666% acc_test=96.509%\n",
      "2018-04-13 14:57:03 start epoch 21/50:\n",
      "2018-04-13 14:57:03 iteration 1/859: current training loss = 0.119444\n",
      "2018-04-13 14:57:05 iteration 200/859: current training loss = 0.102913\n",
      "2018-04-13 14:57:07 iteration 400/859: current training loss = 0.030477\n",
      "2018-04-13 14:57:09 iteration 600/859: current training loss = 0.108245\n",
      "2018-04-13 14:57:11 iteration 800/859: current training loss = 0.072100\n",
      "2018-04-13 14:57:12 iteration 859/859: current training loss = 0.103762\n",
      "2018-04-13 14:57:24 end epoch 21/50: acc_train=97.652% acc_val=96.844% acc_test=96.495%\n",
      "2018-04-13 14:57:24 start epoch 22/50:\n",
      "2018-04-13 14:57:24 iteration 1/859: current training loss = 0.006617\n",
      "2018-04-13 14:57:26 iteration 200/859: current training loss = 0.022481\n",
      "2018-04-13 14:57:28 iteration 400/859: current training loss = 0.036733\n",
      "2018-04-13 14:57:30 iteration 600/859: current training loss = 0.120391\n",
      "2018-04-13 14:57:32 iteration 800/859: current training loss = 0.199232\n",
      "2018-04-13 14:57:32 iteration 859/859: current training loss = 0.177097\n",
      "2018-04-13 14:57:46 end epoch 22/50: acc_train=97.179% acc_val=96.294% acc_test=96.109%\n",
      "2018-04-13 14:57:46 start epoch 23/50:\n",
      "2018-04-13 14:57:46 iteration 1/859: current training loss = 0.047857\n",
      "2018-04-13 14:57:48 iteration 200/859: current training loss = 0.126695\n",
      "2018-04-13 14:57:50 iteration 400/859: current training loss = 0.082521\n",
      "2018-04-13 14:57:52 iteration 600/859: current training loss = 0.038165\n",
      "2018-04-13 14:57:54 iteration 800/859: current training loss = 0.052767\n",
      "2018-04-13 14:57:54 iteration 859/859: current training loss = 0.026031\n",
      "2018-04-13 14:58:07 end epoch 23/50: acc_train=97.775% acc_val=96.838% acc_test=96.505%\n",
      "2018-04-13 14:58:07 start epoch 24/50:\n",
      "2018-04-13 14:58:07 iteration 1/859: current training loss = 0.016867\n",
      "2018-04-13 14:58:09 iteration 200/859: current training loss = 0.101271\n",
      "2018-04-13 14:58:11 iteration 400/859: current training loss = 0.093028\n",
      "2018-04-13 14:58:13 iteration 600/859: current training loss = 0.021665\n",
      "2018-04-13 14:58:15 iteration 800/859: current training loss = 0.122639\n",
      "2018-04-13 14:58:15 iteration 859/859: current training loss = 0.128566\n",
      "2018-04-13 14:58:27 end epoch 24/50: acc_train=98.009% acc_val=96.938% acc_test=96.542%\n",
      "2018-04-13 14:58:27 start epoch 25/50:\n",
      "2018-04-13 14:58:27 iteration 1/859: current training loss = 0.076972\n",
      "2018-04-13 14:58:29 iteration 200/859: current training loss = 0.247189\n",
      "2018-04-13 14:58:31 iteration 400/859: current training loss = 0.057437\n",
      "2018-04-13 14:58:33 iteration 600/859: current training loss = 0.043858\n",
      "2018-04-13 14:58:35 iteration 800/859: current training loss = 0.070482\n",
      "2018-04-13 14:58:36 iteration 859/859: current training loss = 0.054301\n",
      "2018-04-13 14:58:49 end epoch 25/50: acc_train=97.932% acc_val=96.994% acc_test=96.773%\n",
      "2018-04-13 14:58:49 start epoch 26/50:\n",
      "2018-04-13 14:58:49 iteration 1/859: current training loss = 0.025906\n",
      "2018-04-13 14:58:51 iteration 200/859: current training loss = 0.100008\n",
      "2018-04-13 14:58:53 iteration 400/859: current training loss = 0.038869\n",
      "2018-04-13 14:58:55 iteration 600/859: current training loss = 0.035867\n",
      "2018-04-13 14:58:57 iteration 800/859: current training loss = 0.057051\n",
      "2018-04-13 14:58:57 iteration 859/859: current training loss = 0.037900\n",
      "2018-04-13 14:59:10 end epoch 26/50: acc_train=98.098% acc_val=97.228% acc_test=96.845%\n",
      "2018-04-13 14:59:10 start epoch 27/50:\n",
      "2018-04-13 14:59:10 iteration 1/859: current training loss = 0.114768\n",
      "2018-04-13 14:59:12 iteration 200/859: current training loss = 0.049848\n",
      "2018-04-13 14:59:14 iteration 400/859: current training loss = 0.033181\n",
      "2018-04-13 14:59:16 iteration 600/859: current training loss = 0.027434\n",
      "2018-04-13 14:59:18 iteration 800/859: current training loss = 0.059841\n",
      "2018-04-13 14:59:19 iteration 859/859: current training loss = 0.065829\n",
      "2018-04-13 14:59:31 end epoch 27/50: acc_train=98.127% acc_val=96.778% acc_test=96.686%\n",
      "2018-04-13 14:59:31 start epoch 28/50:\n",
      "2018-04-13 14:59:31 iteration 1/859: current training loss = 0.078912\n",
      "2018-04-13 14:59:32 iteration 200/859: current training loss = 0.017083\n",
      "2018-04-13 14:59:35 iteration 400/859: current training loss = 0.074436\n",
      "2018-04-13 14:59:37 iteration 600/859: current training loss = 0.143625\n",
      "2018-04-13 14:59:39 iteration 800/859: current training loss = 0.117550\n",
      "2018-04-13 14:59:39 iteration 859/859: current training loss = 0.044147\n",
      "2018-04-13 14:59:51 end epoch 28/50: acc_train=98.174% acc_val=97.081% acc_test=96.806%\n",
      "2018-04-13 14:59:51 start epoch 29/50:\n",
      "2018-04-13 14:59:51 iteration 1/859: current training loss = 0.013915\n",
      "2018-04-13 14:59:52 iteration 200/859: current training loss = 0.016949\n",
      "2018-04-13 14:59:54 iteration 400/859: current training loss = 0.141772\n",
      "2018-04-13 14:59:56 iteration 600/859: current training loss = 0.114522\n",
      "2018-04-13 14:59:58 iteration 800/859: current training loss = 0.039437\n",
      "2018-04-13 14:59:58 iteration 859/859: current training loss = 0.022935\n",
      "2018-04-13 15:00:12 end epoch 29/50: acc_train=98.359% acc_val=96.981% acc_test=97.088%\n",
      "2018-04-13 15:00:12 start epoch 30/50:\n",
      "2018-04-13 15:00:12 iteration 1/859: current training loss = 0.083362\n",
      "2018-04-13 15:00:13 iteration 200/859: current training loss = 0.038329\n",
      "2018-04-13 15:00:15 iteration 400/859: current training loss = 0.020520\n",
      "2018-04-13 15:00:17 iteration 600/859: current training loss = 0.113427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 15:00:19 iteration 800/859: current training loss = 0.144707\n",
      "2018-04-13 15:00:19 iteration 859/859: current training loss = 0.018286\n",
      "2018-04-13 15:00:31 end epoch 30/50: acc_train=98.448% acc_val=97.047% acc_test=96.988%\n",
      "2018-04-13 15:00:31 start epoch 31/50:\n",
      "2018-04-13 15:00:31 iteration 1/859: current training loss = 0.138352\n",
      "2018-04-13 15:00:33 iteration 200/859: current training loss = 0.026562\n",
      "2018-04-13 15:00:35 iteration 400/859: current training loss = 0.052733\n",
      "2018-04-13 15:00:37 iteration 600/859: current training loss = 0.049689\n",
      "2018-04-13 15:00:39 iteration 800/859: current training loss = 0.007093\n",
      "2018-04-13 15:00:39 iteration 859/859: current training loss = 0.017757\n",
      "2018-04-13 15:00:51 end epoch 31/50: acc_train=98.452% acc_val=96.869% acc_test=97.006%\n",
      "2018-04-13 15:00:51 start epoch 32/50:\n",
      "2018-04-13 15:00:51 iteration 1/859: current training loss = 0.057303\n",
      "2018-04-13 15:00:53 iteration 200/859: current training loss = 0.055581\n",
      "2018-04-13 15:00:54 iteration 400/859: current training loss = 0.013000\n",
      "2018-04-13 15:00:56 iteration 600/859: current training loss = 0.053124\n",
      "2018-04-13 15:00:58 iteration 800/859: current training loss = 0.016854\n",
      "2018-04-13 15:00:59 iteration 859/859: current training loss = 0.084079\n",
      "2018-04-13 15:01:12 end epoch 32/50: acc_train=98.655% acc_val=97.278% acc_test=96.994%\n",
      "2018-04-13 15:01:12 start epoch 33/50:\n",
      "2018-04-13 15:01:12 iteration 1/859: current training loss = 0.039350\n",
      "2018-04-13 15:01:13 iteration 200/859: current training loss = 0.065787\n",
      "2018-04-13 15:01:15 iteration 400/859: current training loss = 0.040878\n",
      "2018-04-13 15:01:17 iteration 600/859: current training loss = 0.066406\n",
      "2018-04-13 15:01:19 iteration 800/859: current training loss = 0.030667\n",
      "2018-04-13 15:01:20 iteration 859/859: current training loss = 0.062143\n",
      "2018-04-13 15:01:32 end epoch 33/50: acc_train=98.504% acc_val=97.088% acc_test=96.886%\n",
      "2018-04-13 15:01:32 start epoch 34/50:\n",
      "2018-04-13 15:01:32 iteration 1/859: current training loss = 0.015556\n",
      "2018-04-13 15:01:34 iteration 200/859: current training loss = 0.028665\n",
      "2018-04-13 15:01:36 iteration 400/859: current training loss = 0.103008\n",
      "2018-04-13 15:01:38 iteration 600/859: current training loss = 0.023030\n",
      "2018-04-13 15:01:40 iteration 800/859: current training loss = 0.024586\n",
      "2018-04-13 15:01:40 iteration 859/859: current training loss = 0.051348\n",
      "2018-04-13 15:01:53 end epoch 34/50: acc_train=98.632% acc_val=97.256% acc_test=97.042%\n",
      "2018-04-13 15:01:53 start epoch 35/50:\n",
      "2018-04-13 15:01:53 iteration 1/859: current training loss = 0.006818\n",
      "2018-04-13 15:01:55 iteration 200/859: current training loss = 0.029105\n",
      "2018-04-13 15:01:56 iteration 400/859: current training loss = 0.012485\n",
      "2018-04-13 15:01:58 iteration 600/859: current training loss = 0.034082\n",
      "2018-04-13 15:02:00 iteration 800/859: current training loss = 0.057228\n",
      "2018-04-13 15:02:00 iteration 859/859: current training loss = 0.009588\n",
      "2018-04-13 15:02:12 end epoch 35/50: acc_train=98.660% acc_val=97.078% acc_test=97.080%\n",
      "2018-04-13 15:02:12 start epoch 36/50:\n",
      "2018-04-13 15:02:12 iteration 1/859: current training loss = 0.033874\n",
      "2018-04-13 15:02:14 iteration 200/859: current training loss = 0.102027\n",
      "2018-04-13 15:02:16 iteration 400/859: current training loss = 0.113438\n",
      "2018-04-13 15:02:18 iteration 600/859: current training loss = 0.082519\n",
      "2018-04-13 15:02:19 iteration 800/859: current training loss = 0.017714\n",
      "2018-04-13 15:02:20 iteration 859/859: current training loss = 0.125083\n",
      "2018-04-13 15:02:33 end epoch 36/50: acc_train=98.841% acc_val=97.225% acc_test=97.269%\n",
      "2018-04-13 15:02:33 start epoch 37/50:\n",
      "2018-04-13 15:02:33 iteration 1/859: current training loss = 0.019323\n",
      "2018-04-13 15:02:35 iteration 200/859: current training loss = 0.035988\n",
      "2018-04-13 15:02:37 iteration 400/859: current training loss = 0.022929\n",
      "2018-04-13 15:02:39 iteration 600/859: current training loss = 0.032614\n",
      "2018-04-13 15:02:41 iteration 800/859: current training loss = 0.017191\n",
      "2018-04-13 15:02:41 iteration 859/859: current training loss = 0.032897\n",
      "2018-04-13 15:02:54 end epoch 37/50: acc_train=98.809% acc_val=97.000% acc_test=97.088%\n",
      "2018-04-13 15:02:54 start epoch 38/50:\n",
      "2018-04-13 15:02:54 iteration 1/859: current training loss = 0.019524\n",
      "2018-04-13 15:02:55 iteration 200/859: current training loss = 0.019244\n",
      "2018-04-13 15:02:57 iteration 400/859: current training loss = 0.082499\n",
      "2018-04-13 15:02:59 iteration 600/859: current training loss = 0.115717\n",
      "2018-04-13 15:03:01 iteration 800/859: current training loss = 0.016766\n",
      "2018-04-13 15:03:01 iteration 859/859: current training loss = 0.026879\n",
      "2018-04-13 15:03:13 end epoch 38/50: acc_train=98.948% acc_val=97.284% acc_test=97.161%\n",
      "2018-04-13 15:03:13 start epoch 39/50:\n",
      "2018-04-13 15:03:13 iteration 1/859: current training loss = 0.050906\n",
      "2018-04-13 15:03:15 iteration 200/859: current training loss = 0.093238\n",
      "2018-04-13 15:03:17 iteration 400/859: current training loss = 0.014760\n",
      "2018-04-13 15:03:19 iteration 600/859: current training loss = 0.031473\n",
      "2018-04-13 15:03:21 iteration 800/859: current training loss = 0.012254\n",
      "2018-04-13 15:03:21 iteration 859/859: current training loss = 0.007505\n",
      "2018-04-13 15:03:33 end epoch 39/50: acc_train=98.973% acc_val=97.125% acc_test=97.147%\n",
      "2018-04-13 15:03:33 start epoch 40/50:\n",
      "2018-04-13 15:03:33 iteration 1/859: current training loss = 0.066053\n",
      "2018-04-13 15:03:35 iteration 200/859: current training loss = 0.004550\n",
      "2018-04-13 15:03:37 iteration 400/859: current training loss = 0.061696\n",
      "2018-04-13 15:03:38 iteration 600/859: current training loss = 0.032655\n",
      "2018-04-13 15:03:40 iteration 800/859: current training loss = 0.010685\n",
      "2018-04-13 15:03:41 iteration 859/859: current training loss = 0.006901\n",
      "2018-04-13 15:03:53 end epoch 40/50: acc_train=99.035% acc_val=97.284% acc_test=97.258%\n",
      "2018-04-13 15:03:53 start epoch 41/50:\n",
      "2018-04-13 15:03:53 iteration 1/859: current training loss = 0.038091\n",
      "2018-04-13 15:03:55 iteration 200/859: current training loss = 0.018319\n",
      "2018-04-13 15:03:57 iteration 400/859: current training loss = 0.036159\n",
      "2018-04-13 15:03:59 iteration 600/859: current training loss = 0.016933\n",
      "2018-04-13 15:04:01 iteration 800/859: current training loss = 0.070680\n",
      "2018-04-13 15:04:01 iteration 859/859: current training loss = 0.103166\n",
      "2018-04-13 15:04:15 end epoch 41/50: acc_train=99.064% acc_val=97.384% acc_test=97.264%\n",
      "2018-04-13 15:04:15 start epoch 42/50:\n",
      "2018-04-13 15:04:15 iteration 1/859: current training loss = 0.034594\n",
      "2018-04-13 15:04:18 iteration 200/859: current training loss = 0.094218\n",
      "2018-04-13 15:04:20 iteration 400/859: current training loss = 0.061764\n",
      "2018-04-13 15:04:21 iteration 600/859: current training loss = 0.014258\n",
      "2018-04-13 15:04:23 iteration 800/859: current training loss = 0.039758\n",
      "2018-04-13 15:04:24 iteration 859/859: current training loss = 0.002133\n",
      "2018-04-13 15:04:38 end epoch 42/50: acc_train=98.797% acc_val=96.962% acc_test=97.036%\n",
      "2018-04-13 15:04:38 start epoch 43/50:\n",
      "2018-04-13 15:04:38 iteration 1/859: current training loss = 0.004562\n",
      "2018-04-13 15:04:40 iteration 200/859: current training loss = 0.075352\n",
      "2018-04-13 15:04:41 iteration 400/859: current training loss = 0.020621\n",
      "2018-04-13 15:04:43 iteration 600/859: current training loss = 0.010205\n",
      "2018-04-13 15:04:45 iteration 800/859: current training loss = 0.008152\n",
      "2018-04-13 15:04:46 iteration 859/859: current training loss = 0.011427\n",
      "2018-04-13 15:04:58 end epoch 43/50: acc_train=99.166% acc_val=97.231% acc_test=97.339%\n",
      "2018-04-13 15:04:58 start epoch 44/50:\n",
      "2018-04-13 15:04:58 iteration 1/859: current training loss = 0.045956\n",
      "2018-04-13 15:05:00 iteration 200/859: current training loss = 0.062049\n",
      "2018-04-13 15:05:02 iteration 400/859: current training loss = 0.016904\n",
      "2018-04-13 15:05:04 iteration 600/859: current training loss = 0.072460\n",
      "2018-04-13 15:05:06 iteration 800/859: current training loss = 0.045160\n",
      "2018-04-13 15:05:06 iteration 859/859: current training loss = 0.098258\n",
      "2018-04-13 15:05:18 end epoch 44/50: acc_train=99.231% acc_val=97.306% acc_test=97.434%\n",
      "2018-04-13 15:05:18 start epoch 45/50:\n",
      "2018-04-13 15:05:18 iteration 1/859: current training loss = 0.006383\n",
      "2018-04-13 15:05:20 iteration 200/859: current training loss = 0.025000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 15:05:22 iteration 400/859: current training loss = 0.001618\n",
      "2018-04-13 15:05:24 iteration 600/859: current training loss = 0.003769\n",
      "2018-04-13 15:05:26 iteration 800/859: current training loss = 0.022566\n",
      "2018-04-13 15:05:27 iteration 859/859: current training loss = 0.012924\n",
      "2018-04-13 15:05:39 end epoch 45/50: acc_train=99.008% acc_val=97.266% acc_test=96.945%\n",
      "2018-04-13 15:05:39 start epoch 46/50:\n",
      "2018-04-13 15:05:39 iteration 1/859: current training loss = 0.008885\n",
      "2018-04-13 15:05:41 iteration 200/859: current training loss = 0.022592\n",
      "2018-04-13 15:05:43 iteration 400/859: current training loss = 0.020459\n",
      "2018-04-13 15:05:45 iteration 600/859: current training loss = 0.005420\n",
      "2018-04-13 15:05:47 iteration 800/859: current training loss = 0.006988\n",
      "2018-04-13 15:05:47 iteration 859/859: current training loss = 0.010045\n",
      "2018-04-13 15:05:59 end epoch 46/50: acc_train=99.294% acc_val=97.488% acc_test=97.527%\n",
      "2018-04-13 15:05:59 start epoch 47/50:\n",
      "2018-04-13 15:05:59 iteration 1/859: current training loss = 0.006394\n",
      "2018-04-13 15:06:01 iteration 200/859: current training loss = 0.017306\n",
      "2018-04-13 15:06:03 iteration 400/859: current training loss = 0.010708\n",
      "2018-04-13 15:06:05 iteration 600/859: current training loss = 0.021829\n",
      "2018-04-13 15:06:06 iteration 800/859: current training loss = 0.022737\n",
      "2018-04-13 15:06:07 iteration 859/859: current training loss = 0.028459\n",
      "2018-04-13 15:06:19 end epoch 47/50: acc_train=99.287% acc_val=97.469% acc_test=97.523%\n",
      "2018-04-13 15:06:19 start epoch 48/50:\n",
      "2018-04-13 15:06:19 iteration 1/859: current training loss = 0.019982\n",
      "2018-04-13 15:06:21 iteration 200/859: current training loss = 0.011138\n",
      "2018-04-13 15:06:23 iteration 400/859: current training loss = 0.007592\n",
      "2018-04-13 15:06:25 iteration 600/859: current training loss = 0.019722\n",
      "2018-04-13 15:06:27 iteration 800/859: current training loss = 0.025107\n",
      "2018-04-13 15:06:28 iteration 859/859: current training loss = 0.007920\n",
      "2018-04-13 15:06:40 end epoch 48/50: acc_train=99.093% acc_val=97.288% acc_test=97.188%\n",
      "2018-04-13 15:06:40 start epoch 49/50:\n",
      "2018-04-13 15:06:40 iteration 1/859: current training loss = 0.051772\n",
      "2018-04-13 15:06:42 iteration 200/859: current training loss = 0.015478\n",
      "2018-04-13 15:06:44 iteration 400/859: current training loss = 0.029185\n",
      "2018-04-13 15:06:46 iteration 600/859: current training loss = 0.011714\n",
      "2018-04-13 15:06:48 iteration 800/859: current training loss = 0.012297\n",
      "2018-04-13 15:06:48 iteration 859/859: current training loss = 0.006311\n",
      "2018-04-13 15:07:01 end epoch 49/50: acc_train=99.328% acc_val=97.381% acc_test=97.159%\n",
      "2018-04-13 15:07:01 start epoch 50/50:\n",
      "2018-04-13 15:07:01 iteration 1/859: current training loss = 0.010418\n",
      "2018-04-13 15:07:03 iteration 200/859: current training loss = 0.021798\n",
      "2018-04-13 15:07:06 iteration 400/859: current training loss = 0.103206\n",
      "2018-04-13 15:07:07 iteration 600/859: current training loss = 0.020760\n",
      "2018-04-13 15:07:09 iteration 800/859: current training loss = 0.017467\n",
      "2018-04-13 15:07:10 iteration 859/859: current training loss = 0.024845\n",
      "2018-04-13 15:07:23 end epoch 50/50: acc_train=99.155% acc_val=97.138% acc_test=97.277%\n"
     ]
    }
   ],
   "source": [
    "max_epoch=50\n",
    "print_every=200\n",
    "\n",
    "def train():\n",
    "    num_iteration=num_train//batch_size\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=mnist.train.next_batch(batch_size)\n",
    "        loss_num,_ = sess.run([loss,train_step],feed_dict={X:images,y:labels})\n",
    "        if it==0 or (it+1)%print_every==0 or it==num_iteration-1:\n",
    "            print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "                  'iteration %d/%d:' % (it+1,num_iteration),'current training loss = %f' % (loss_num))\n",
    "            \n",
    "def eval(dataset,num_iteration):\n",
    "    total_loss=0\n",
    "    total_accuracy=0\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=dataset.next_batch(batch_size)\n",
    "        loss_num,accuracy_num = sess.run([loss,accuracy],feed_dict={X:images,y:labels})\n",
    "        total_loss+=loss_num\n",
    "        total_accuracy+=accuracy_num\n",
    "    total_loss/=num_iteration\n",
    "    total_accuracy/=num_iteration\n",
    "    return total_loss,total_accuracy\n",
    "    \n",
    "acc_train_his=[]\n",
    "acc_val_his=[]\n",
    "acc_test_his=[]\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(max_epoch):\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'start epoch %d/%d:' % (epoch+1,max_epoch))\n",
    "        train()\n",
    "        loss_train,acc_train=eval(mnist.train,2000)\n",
    "        loss_val,acc_val=eval(mnist.validation,500)\n",
    "        loss_test,acc_test=eval(mnist.test,1000)\n",
    "        acc_train_his.append(acc_train)\n",
    "        acc_val_his.append(acc_val)\n",
    "        acc_test_his.append(acc_test)\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'end epoch %d/%d:' % (epoch+1,max_epoch),\n",
    "             'acc_train=%.3f%% acc_val=%.3f%% acc_test=%.3f%%' % (acc_train*100.0,acc_val*100.0,acc_test*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(2)\n",
    "ptr,=plt.plot(range(max_epoch),acc_train_his,'r-')\n",
    "pva,=plt.plot(range(max_epoch),acc_val_his,'b-')\n",
    "pte,=plt.plot(range(max_epoch),acc_test_his,'y-')\n",
    "plt.xlabel('training epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('accuracy on three sets')\n",
    "plt.legend((ptr,pva,pte),('train','validation','test'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hard attention REINFORCE method (Glimpse Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size=64\n",
    "img_size=28\n",
    "sensor_unit=256\n",
    "lstm_size=256\n",
    "N_glimpse=10\n",
    "MC_test=128\n",
    "loc_std=0.2\n",
    "tot_size=batch_size*MC_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glimpse_Network():\n",
    "    def __init__(self):\n",
    "        self.glimspe_size=[5,7,9]\n",
    "        self.concat_size=9\n",
    "        self.img_net=tf.layers.Dense(units=sensor_unit,name='glimpse_net/img_net')\n",
    "        self.loc_net=tf.layers.Dense(units=sensor_unit,name='glimpse_net/loc_net')\n",
    "        \n",
    "    def glimpse_sensor(self,image,loc):\n",
    "        glimpses_list=[tf.image.extract_glimpse(input=image,size=[gs,gs],offsets=loc) for gs in self.glimspe_size]\n",
    "        glimpses_norm=[tf.image.resize_bilinear(g,[self.concat_size,self.concat_size]) for g in glimpses_list]\n",
    "        glimpses=tf.concat(values=glimpses_norm,axis=3)  # batch_size*concat_size*concat_size*3\n",
    "        return glimpses\n",
    "    \n",
    "    def forward(self,image,loc):\n",
    "        glimpses=self.glimpse_sensor(image,loc) # tot_size*concat_size*concat_size*3\n",
    "        g_image=self.img_net(inputs=tf.layers.flatten(glimpses))\n",
    "        g_loc=self.loc_net(inputs=loc)\n",
    "        g_out=tf.nn.relu(g_image+g_loc)\n",
    "        return g_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(dtype=tf.float32,shape=[None,28,28,1])\n",
    "y=tf.placeholder(dtype=tf.int64,shape=[None,10])\n",
    "start_location=tf.random_uniform(shape=[tot_size,2],minval=-1.0,maxval=1.0)\n",
    "gNet=Glimpse_Network()\n",
    "\n",
    "lstm_cell = tf.contrib.rnn.LSTMCell(lstm_size)\n",
    "state = lstm_cell.zero_state(tot_size, tf.float32)\n",
    "\n",
    "emission_net=tf.layers.Dense(units=2,name='emission_net')\n",
    "baseline_net=tf.layers.Dense(units=1,name='baseline_net')\n",
    "predict_net=tf.layers.Dense(units=10,name='predict_net')\n",
    "\n",
    "def loglikelihood(sample,mean):\n",
    "    gaussian=tf.distributions.Normal(loc=mean,scale=tf.constant([loc_std,loc_std]))\n",
    "    llh=-gaussian.log_prob(sample)\n",
    "    return tf.reduce_sum(llh,axis=1)\n",
    "    \n",
    "loc_his=[]\n",
    "loglikelihood_his=[]\n",
    "baseline_his=[]\n",
    "normalized_loc=start_location\n",
    "for ng in range(N_glimpse):\n",
    "    loc_his.append(normalized_loc)\n",
    "    \n",
    "    # extract glimpse\n",
    "    glimpses_out=gNet.forward(X,normalized_loc)\n",
    "    \n",
    "    # RNN\n",
    "    lstm_output,state=lstm_cell(glimpses_out,state)\n",
    "    \n",
    "    # emit mean of location\n",
    "    loc_mean=emission_net(inputs=lstm_output)\n",
    "    \n",
    "    # sample next location by gaussian distribution centered at loc_mean\n",
    "    loc_sample=tf.random_normal(shape=(tot_size,2),mean=loc_mean,stddev=loc_std)\n",
    "    \n",
    "    # calculate the -loglikelihood of the sampled position\n",
    "    llh=loglikelihood(loc_sample,loc_mean)\n",
    "    loglikelihood_his.append(llh)\n",
    "    \n",
    "    # normalize the location for next input\n",
    "    normalized_loc=tf.tanh(loc_sample)\n",
    "    \n",
    "    # output time independent baseline\n",
    "    baseline=baseline_net(inputs=lstm_output)\n",
    "    baseline_his.append(tf.squeeze(baseline))\n",
    "\n",
    "# pack data for calculation\n",
    "baseline_his=tf.stack(baseline_his)\n",
    "loglikelihood_his=tf.stack(loglikelihood_his)\n",
    "\n",
    "# make prediction\n",
    "score=predict_net(inputs=lstm_output)\n",
    "prediction=tf.argmax(score,1)\n",
    "\n",
    "# calculate reward, do variance reduction and calculate reinforced loglikelihood\n",
    "reward=tf.cast(tf.equal(prediction,tf.argmax(y,1)),dtype=tf.float32)\n",
    "reduce_var_reward=reward-tf.stop_gradient(baseline_his)\n",
    "reinforce_llh=tf.reduce_mean(loglikelihood_his*reduce_var_reward)\n",
    "\n",
    "# regression baseline towards reward\n",
    "baseline_mse=tf.reduce_mean(tf.square(reward-baseline_his))\n",
    "\n",
    "# softmax to output\n",
    "softmax_loss=tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=y,logits=score))\n",
    "\n",
    "# summarize loss\n",
    "loss=reinforce_llh+baseline_mse+softmax_loss\n",
    "\n",
    "\n",
    "optimizier=tf.train.AdamOptimizer(learning_rate=1e-6)\n",
    "train_step = optimizier.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_epoch=100\n",
    "print_every=50\n",
    "num_iteration=num_train//batch_size\n",
    "loss_his=[]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     print(tf.global_variables())\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(max_epoch):\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'start epoch %d/%d' % (epoch+1,max_epoch))\n",
    "        tot_loss=0\n",
    "        for it in range(num_iteration):\n",
    "            images,labels=mnist.train.next_batch(batch_size)\n",
    "            # prepare data for monte carlo test\n",
    "            images=np.tile(images,(MC_test,1))\n",
    "            labels=np.tile(labels,(MC_test,1))\n",
    "            feed_dict={X:images.reshape(tot_size,28,28,1),y:labels}\n",
    "            loss_1,loss_2,loss_3,loss_out,_=sess.run([reinforce_llh,baseline_mse,softmax_loss,loss,train_step],\n",
    "                                                     feed_dict=feed_dict)\n",
    "            tot_loss+=loss_out\n",
    "            if it==0 or (it+1)%print_every==0 or it==num_iteration-1:\n",
    "                print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "                      'iter',it+1,': loss_1 =',loss_1,'loss_2 =',loss_2,'loss_3 =',loss_3,'total_loss =',loss_out)\n",
    "        loss_his.append(tot_loss/num_iteration)\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "              'end epoch, average loss =',(tot_loss/num_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "ptr,=plt.plot(range(max_epoch),loss_his)\n",
    "plt.xlabel('training epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mean=tf.zeros((100,2),dtype=tf.float32)\n",
    "# std=tf.constant([1,1],dtype=tf.float32)\n",
    "# gaussian=tf.distributions.Normal(loc=mean,scale=std)\n",
    "# rand=tf.random_normal(shape=(100,2),mean=0,stddev=1)\n",
    "# sampled=mean+rand\n",
    "# prob=-gaussian.log_prob(sampled)\n",
    "# prob=tf.reduce_mean(tf.reduce_sum(prob,1))\n",
    "# with tf.Session() as sess:\n",
    "#     out=sess.run([prob])\n",
    "#     print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
