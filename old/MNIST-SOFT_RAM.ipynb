{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"mnist/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train=mnist.train.num_examples\n",
    "num_val=mnist.validation.num_examples\n",
    "num_test=mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "img_size=28\n",
    "RNN_unit=img_size*img_size\n",
    "N_watch=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(dtype=tf.float32,shape=[None,img_size*img_size])\n",
    "y=tf.placeholder(dtype=tf.int64,shape=[None,10])\n",
    "\n",
    "predict_net=tf.layers.Dense(units=10)\n",
    "\n",
    "def get_next_input(output, i):\n",
    "    attention_weight=tf.nn.softmax(output)\n",
    "    weighted_graph=X*attention_weight\n",
    "    return weighted_graph\n",
    "\n",
    "lstm_cell = tf.nn.rnn_cell.LSTMCell(RNN_unit, state_is_tuple=True)\n",
    "init_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "inputs=[X]\n",
    "inputs.extend([0]*N_watch)\n",
    "outputs,_ = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, init_state, lstm_cell, loop_function=get_next_input)\n",
    "\n",
    "output=outputs[-1]\n",
    "score=predict_net(output)\n",
    "\n",
    "predictions = tf.argmax(score, 1)\n",
    "correct_predictions = tf.equal(predictions, tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "\n",
    "loss=tf.losses.softmax_cross_entropy(onehot_labels=y,logits=score)\n",
    "optimizier=tf.train.AdamOptimizer(learning_rate=1e-5)\n",
    "train_step = optimizier.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch=100\n",
    "print_every=200\n",
    "\n",
    "def train():\n",
    "    num_iteration=num_train//batch_size\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=mnist.train.next_batch(batch_size)\n",
    "        loss_num,_ = sess.run([loss,train_step],feed_dict={X:images,y:labels})\n",
    "        if it==0 or (it+1)%print_every==0 or it==num_iteration-1:\n",
    "            print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "                  'iteration %d/%d:' % (it+1,num_iteration),'current training loss = %f' % (loss_num))\n",
    "            \n",
    "def eval(dataset,num_iteration):\n",
    "    total_loss=0\n",
    "    total_accuracy=0\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=dataset.next_batch(batch_size)\n",
    "        loss_num,accuracy_num = sess.run([loss,accuracy],feed_dict={X:images,y:labels})\n",
    "        total_loss+=loss_num\n",
    "        total_accuracy+=accuracy_num\n",
    "    total_loss/=num_iteration\n",
    "    total_accuracy/=num_iteration\n",
    "    return total_loss,total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 12:49:45 start epoch 1/100:\n",
      "2018-04-17 12:49:45 iteration 1/859: current training loss = 2.302674\n",
      "2018-04-17 12:49:47 iteration 200/859: current training loss = 2.033743\n",
      "2018-04-17 12:49:49 iteration 400/859: current training loss = 0.936832\n",
      "2018-04-17 12:49:51 iteration 600/859: current training loss = 0.495727\n",
      "2018-04-17 12:49:53 iteration 800/859: current training loss = 0.456990\n",
      "2018-04-17 12:49:54 iteration 859/859: current training loss = 0.519808\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 12:50:10 end epoch 1/100: acc_train=85.602% acc_val=86.269% acc_test=85.817%\n",
      "2018-04-17 12:50:10 start epoch 2/100:\n",
      "2018-04-17 12:50:10 iteration 1/859: current training loss = 0.394443\n",
      "2018-04-17 12:50:12 iteration 200/859: current training loss = 0.515124\n",
      "2018-04-17 12:50:14 iteration 400/859: current training loss = 0.273336\n",
      "2018-04-17 12:50:17 iteration 600/859: current training loss = 0.277814\n",
      "2018-04-17 12:50:19 iteration 800/859: current training loss = 0.468870\n",
      "2018-04-17 12:50:20 iteration 859/859: current training loss = 0.322037\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 12:50:34 end epoch 2/100: acc_train=90.228% acc_val=90.606% acc_test=90.231%\n",
      "2018-04-17 12:50:34 start epoch 3/100:\n",
      "2018-04-17 12:50:34 iteration 1/859: current training loss = 0.358087\n",
      "2018-04-17 12:50:36 iteration 200/859: current training loss = 0.211550\n",
      "2018-04-17 12:50:38 iteration 400/859: current training loss = 0.215866\n",
      "2018-04-17 12:50:40 iteration 600/859: current training loss = 0.172148\n",
      "2018-04-17 12:50:42 iteration 800/859: current training loss = 0.213500\n",
      "2018-04-17 12:50:43 iteration 859/859: current training loss = 0.261345\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 12:50:57 end epoch 3/100: acc_train=91.413% acc_val=92.194% acc_test=91.695%\n",
      "2018-04-17 12:50:57 start epoch 4/100:\n",
      "2018-04-17 12:50:57 iteration 1/859: current training loss = 0.373598\n",
      "2018-04-17 12:51:00 iteration 200/859: current training loss = 0.235207\n",
      "2018-04-17 12:51:02 iteration 400/859: current training loss = 0.255537\n",
      "2018-04-17 12:51:04 iteration 600/859: current training loss = 0.365384\n",
      "2018-04-17 12:51:06 iteration 800/859: current training loss = 0.337621\n",
      "2018-04-17 12:51:07 iteration 859/859: current training loss = 0.125588\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 12:51:22 end epoch 4/100: acc_train=92.753% acc_val=93.075% acc_test=92.811%\n",
      "2018-04-17 12:51:22 start epoch 5/100:\n",
      "2018-04-17 12:51:22 iteration 1/859: current training loss = 0.122986\n",
      "2018-04-17 12:51:23 iteration 200/859: current training loss = 0.231391\n",
      "2018-04-17 12:51:25 iteration 400/859: current training loss = 0.078165\n",
      "2018-04-17 12:51:28 iteration 600/859: current training loss = 0.295625\n",
      "2018-04-17 12:51:30 iteration 800/859: current training loss = 0.152113\n",
      "2018-04-17 12:51:31 iteration 859/859: current training loss = 0.113874\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 12:51:46 end epoch 5/100: acc_train=93.288% acc_val=93.453% acc_test=93.278%\n",
      "2018-04-17 12:51:46 start epoch 6/100:\n",
      "2018-04-17 12:51:46 iteration 1/859: current training loss = 0.155869\n",
      "2018-04-17 12:51:48 iteration 200/859: current training loss = 0.231658\n",
      "2018-04-17 12:51:51 iteration 400/859: current training loss = 0.156776\n",
      "2018-04-17 12:51:53 iteration 600/859: current training loss = 0.350841\n",
      "2018-04-17 12:51:55 iteration 800/859: current training loss = 0.114175\n",
      "2018-04-17 12:51:55 iteration 859/859: current training loss = 0.329583\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 12:52:13 end epoch 6/100: acc_train=93.974% acc_val=94.169% acc_test=93.747%\n",
      "2018-04-17 12:52:13 start epoch 7/100:\n",
      "2018-04-17 12:52:13 iteration 1/859: current training loss = 0.346525\n",
      "2018-04-17 12:52:16 iteration 200/859: current training loss = 0.149740\n",
      "2018-04-17 12:52:18 iteration 400/859: current training loss = 0.326323\n",
      "2018-04-17 12:52:21 iteration 600/859: current training loss = 0.213073\n",
      "2018-04-17 12:52:23 iteration 800/859: current training loss = 0.234967\n",
      "2018-04-17 12:52:23 iteration 859/859: current training loss = 0.239597\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 12:52:41 end epoch 7/100: acc_train=94.501% acc_val=94.487% acc_test=94.055%\n",
      "2018-04-17 12:52:41 start epoch 8/100:\n",
      "2018-04-17 12:52:41 iteration 1/859: current training loss = 0.164576\n",
      "2018-04-17 12:52:43 iteration 200/859: current training loss = 0.316779\n",
      "2018-04-17 12:52:45 iteration 400/859: current training loss = 0.233126\n",
      "2018-04-17 12:52:47 iteration 600/859: current training loss = 0.237297\n",
      "2018-04-17 12:52:50 iteration 800/859: current training loss = 0.133699\n",
      "2018-04-17 12:52:50 iteration 859/859: current training loss = 0.346135\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 12:53:06 end epoch 8/100: acc_train=94.819% acc_val=94.675% acc_test=94.350%\n",
      "2018-04-17 12:53:06 start epoch 9/100:\n",
      "2018-04-17 12:53:06 iteration 1/859: current training loss = 0.125309\n",
      "2018-04-17 12:53:08 iteration 200/859: current training loss = 0.119263\n",
      "2018-04-17 12:53:11 iteration 400/859: current training loss = 0.106847\n",
      "2018-04-17 12:53:13 iteration 600/859: current training loss = 0.160058\n",
      "2018-04-17 12:53:15 iteration 800/859: current training loss = 0.105201\n",
      "2018-04-17 12:53:16 iteration 859/859: current training loss = 0.078177\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 12:53:32 end epoch 9/100: acc_train=95.312% acc_val=95.291% acc_test=94.764%\n",
      "2018-04-17 12:53:32 start epoch 10/100:\n",
      "2018-04-17 12:53:32 iteration 1/859: current training loss = 0.213405\n",
      "2018-04-17 12:53:36 iteration 200/859: current training loss = 0.370618\n",
      "2018-04-17 12:53:38 iteration 400/859: current training loss = 0.066517\n",
      "2018-04-17 12:53:40 iteration 600/859: current training loss = 0.191927\n",
      "2018-04-17 12:53:41 iteration 800/859: current training loss = 0.175690\n",
      "2018-04-17 12:53:42 iteration 859/859: current training loss = 0.101736\n",
      "2018-04-17 12:53:59 end epoch 10/100: acc_train=95.352% acc_val=94.975% acc_test=94.858%\n",
      "2018-04-17 12:53:59 start epoch 11/100:\n",
      "2018-04-17 12:53:59 iteration 1/859: current training loss = 0.131112\n",
      "2018-04-17 12:54:01 iteration 200/859: current training loss = 0.166742\n",
      "2018-04-17 12:54:03 iteration 400/859: current training loss = 0.118349\n",
      "2018-04-17 12:54:05 iteration 600/859: current training loss = 0.256726\n",
      "2018-04-17 12:54:09 iteration 800/859: current training loss = 0.106967\n",
      "2018-04-17 12:54:09 iteration 859/859: current training loss = 0.200985\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 12:54:27 end epoch 11/100: acc_train=95.748% acc_val=95.328% acc_test=95.128%\n",
      "2018-04-17 12:54:27 start epoch 12/100:\n",
      "2018-04-17 12:54:27 iteration 1/859: current training loss = 0.030937\n",
      "2018-04-17 12:54:29 iteration 200/859: current training loss = 0.108782\n",
      "2018-04-17 12:54:32 iteration 400/859: current training loss = 0.044475\n",
      "2018-04-17 12:54:34 iteration 600/859: current training loss = 0.186528\n",
      "2018-04-17 12:54:36 iteration 800/859: current training loss = 0.051083\n",
      "2018-04-17 12:54:36 iteration 859/859: current training loss = 0.141216\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 12:54:51 end epoch 12/100: acc_train=96.156% acc_val=95.519% acc_test=95.547%\n",
      "2018-04-17 12:54:51 start epoch 13/100:\n",
      "2018-04-17 12:54:52 iteration 1/859: current training loss = 0.054792\n",
      "2018-04-17 12:54:55 iteration 200/859: current training loss = 0.058905\n",
      "2018-04-17 12:54:57 iteration 400/859: current training loss = 0.083228\n",
      "2018-04-17 12:54:59 iteration 600/859: current training loss = 0.116693\n",
      "2018-04-17 12:55:01 iteration 800/859: current training loss = 0.084896\n",
      "2018-04-17 12:55:01 iteration 859/859: current training loss = 0.158070\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 12:55:19 end epoch 13/100: acc_train=96.349% acc_val=95.975% acc_test=95.736%\n",
      "2018-04-17 12:55:19 start epoch 14/100:\n",
      "2018-04-17 12:55:19 iteration 1/859: current training loss = 0.098162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 12:55:21 iteration 200/859: current training loss = 0.131777\n",
      "2018-04-17 12:55:23 iteration 400/859: current training loss = 0.125124\n",
      "2018-04-17 12:55:25 iteration 600/859: current training loss = 0.110708\n",
      "2018-04-17 12:55:29 iteration 800/859: current training loss = 0.094765\n",
      "2018-04-17 12:55:30 iteration 859/859: current training loss = 0.065174\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 12:55:47 end epoch 14/100: acc_train=96.537% acc_val=96.072% acc_test=95.875%\n",
      "2018-04-17 12:55:47 start epoch 15/100:\n",
      "2018-04-17 12:55:47 iteration 1/859: current training loss = 0.143843\n",
      "2018-04-17 12:55:49 iteration 200/859: current training loss = 0.131266\n",
      "2018-04-17 12:55:52 iteration 400/859: current training loss = 0.057898\n",
      "2018-04-17 12:55:54 iteration 600/859: current training loss = 0.288570\n",
      "2018-04-17 12:55:56 iteration 800/859: current training loss = 0.109277\n",
      "2018-04-17 12:55:57 iteration 859/859: current training loss = 0.148653\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 12:56:13 end epoch 15/100: acc_train=96.813% acc_val=96.147% acc_test=96.052%\n",
      "2018-04-17 12:56:13 start epoch 16/100:\n",
      "2018-04-17 12:56:13 iteration 1/859: current training loss = 0.073737\n",
      "2018-04-17 12:56:16 iteration 200/859: current training loss = 0.058778\n",
      "2018-04-17 12:56:18 iteration 400/859: current training loss = 0.119564\n",
      "2018-04-17 12:56:20 iteration 600/859: current training loss = 0.130757\n",
      "2018-04-17 12:56:22 iteration 800/859: current training loss = 0.031909\n",
      "2018-04-17 12:56:22 iteration 859/859: current training loss = 0.097091\n",
      "2018-04-17 12:56:38 end epoch 16/100: acc_train=96.797% acc_val=96.031% acc_test=95.978%\n",
      "2018-04-17 12:56:38 start epoch 17/100:\n",
      "2018-04-17 12:56:38 iteration 1/859: current training loss = 0.133096\n",
      "2018-04-17 12:56:39 iteration 200/859: current training loss = 0.047551\n",
      "2018-04-17 12:56:41 iteration 400/859: current training loss = 0.120275\n",
      "2018-04-17 12:56:43 iteration 600/859: current training loss = 0.128848\n",
      "2018-04-17 12:56:47 iteration 800/859: current training loss = 0.043744\n",
      "2018-04-17 12:56:47 iteration 859/859: current training loss = 0.049232\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 12:57:01 end epoch 17/100: acc_train=97.198% acc_val=96.381% acc_test=96.342%\n",
      "2018-04-17 12:57:01 start epoch 18/100:\n",
      "2018-04-17 12:57:01 iteration 1/859: current training loss = 0.056956\n",
      "2018-04-17 12:57:03 iteration 200/859: current training loss = 0.146529\n",
      "2018-04-17 12:57:06 iteration 400/859: current training loss = 0.196438\n",
      "2018-04-17 12:57:08 iteration 600/859: current training loss = 0.106736\n",
      "2018-04-17 12:57:10 iteration 800/859: current training loss = 0.102204\n",
      "2018-04-17 12:57:11 iteration 859/859: current training loss = 0.162405\n",
      "2018-04-17 12:57:25 end epoch 18/100: acc_train=97.249% acc_val=96.275% acc_test=96.288%\n",
      "2018-04-17 12:57:25 start epoch 19/100:\n",
      "2018-04-17 12:57:25 iteration 1/859: current training loss = 0.050867\n",
      "2018-04-17 12:57:29 iteration 200/859: current training loss = 0.207686\n",
      "2018-04-17 12:57:31 iteration 400/859: current training loss = 0.015963\n",
      "2018-04-17 12:57:33 iteration 600/859: current training loss = 0.211538\n",
      "2018-04-17 12:57:35 iteration 800/859: current training loss = 0.101926\n",
      "2018-04-17 12:57:36 iteration 859/859: current training loss = 0.128218\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 12:57:51 end epoch 19/100: acc_train=97.409% acc_val=96.678% acc_test=96.453%\n",
      "2018-04-17 12:57:51 start epoch 20/100:\n",
      "2018-04-17 12:57:51 iteration 1/859: current training loss = 0.129055\n",
      "2018-04-17 12:57:53 iteration 200/859: current training loss = 0.051293\n",
      "2018-04-17 12:57:54 iteration 400/859: current training loss = 0.024140\n",
      "2018-04-17 12:57:56 iteration 600/859: current training loss = 0.125220\n",
      "2018-04-17 12:57:59 iteration 800/859: current training loss = 0.101199\n",
      "2018-04-17 12:58:00 iteration 859/859: current training loss = 0.094399\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 12:58:14 end epoch 20/100: acc_train=97.504% acc_val=96.741% acc_test=96.562%\n",
      "2018-04-17 12:58:14 start epoch 21/100:\n",
      "2018-04-17 12:58:14 iteration 1/859: current training loss = 0.038095\n",
      "2018-04-17 12:58:16 iteration 200/859: current training loss = 0.053998\n",
      "2018-04-17 12:58:21 iteration 400/859: current training loss = 0.048577\n",
      "2018-04-17 12:58:23 iteration 600/859: current training loss = 0.196936\n",
      "2018-04-17 12:58:25 iteration 800/859: current training loss = 0.082729\n",
      "2018-04-17 12:58:25 iteration 859/859: current training loss = 0.049761\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 12:58:39 end epoch 21/100: acc_train=97.620% acc_val=96.962% acc_test=96.583%\n",
      "2018-04-17 12:58:39 start epoch 22/100:\n",
      "2018-04-17 12:58:39 iteration 1/859: current training loss = 0.043255\n",
      "2018-04-17 12:58:43 iteration 200/859: current training loss = 0.050274\n",
      "2018-04-17 12:58:45 iteration 400/859: current training loss = 0.068720\n",
      "2018-04-17 12:58:47 iteration 600/859: current training loss = 0.057660\n",
      "2018-04-17 12:58:49 iteration 800/859: current training loss = 0.017731\n",
      "2018-04-17 12:58:50 iteration 859/859: current training loss = 0.106106\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 12:59:05 end epoch 22/100: acc_train=97.706% acc_val=96.966% acc_test=96.506%\n",
      "2018-04-17 12:59:05 start epoch 23/100:\n",
      "2018-04-17 12:59:05 iteration 1/859: current training loss = 0.019919\n",
      "2018-04-17 12:59:07 iteration 200/859: current training loss = 0.134961\n",
      "2018-04-17 12:59:09 iteration 400/859: current training loss = 0.052906\n",
      "2018-04-17 12:59:11 iteration 600/859: current training loss = 0.134590\n",
      "2018-04-17 12:59:15 iteration 800/859: current training loss = 0.039438\n",
      "2018-04-17 12:59:15 iteration 859/859: current training loss = 0.081473\n",
      "2018-04-17 12:59:29 end epoch 23/100: acc_train=97.853% acc_val=96.872% acc_test=96.752%\n",
      "2018-04-17 12:59:29 start epoch 24/100:\n",
      "2018-04-17 12:59:29 iteration 1/859: current training loss = 0.017097\n",
      "2018-04-17 12:59:31 iteration 200/859: current training loss = 0.038180\n",
      "2018-04-17 12:59:33 iteration 400/859: current training loss = 0.058520\n",
      "2018-04-17 12:59:35 iteration 600/859: current training loss = 0.060128\n",
      "2018-04-17 12:59:37 iteration 800/859: current training loss = 0.034119\n",
      "2018-04-17 12:59:38 iteration 859/859: current training loss = 0.183366\n",
      "2018-04-17 12:59:52 end epoch 24/100: acc_train=97.916% acc_val=96.891% acc_test=96.756%\n",
      "2018-04-17 12:59:52 start epoch 25/100:\n",
      "2018-04-17 12:59:52 iteration 1/859: current training loss = 0.036908\n",
      "2018-04-17 12:59:54 iteration 200/859: current training loss = 0.037841\n",
      "2018-04-17 12:59:56 iteration 400/859: current training loss = 0.046197\n",
      "2018-04-17 12:59:58 iteration 600/859: current training loss = 0.172666\n",
      "2018-04-17 13:00:00 iteration 800/859: current training loss = 0.087342\n",
      "2018-04-17 13:00:01 iteration 859/859: current training loss = 0.035032\n",
      "2018-04-17 13:00:17 end epoch 25/100: acc_train=97.964% acc_val=96.603% acc_test=96.586%\n",
      "2018-04-17 13:00:17 start epoch 26/100:\n",
      "2018-04-17 13:00:17 iteration 1/859: current training loss = 0.067079\n",
      "2018-04-17 13:00:19 iteration 200/859: current training loss = 0.068415\n",
      "2018-04-17 13:00:21 iteration 400/859: current training loss = 0.018696\n",
      "2018-04-17 13:00:23 iteration 600/859: current training loss = 0.049615\n",
      "2018-04-17 13:00:26 iteration 800/859: current training loss = 0.012488\n",
      "2018-04-17 13:00:27 iteration 859/859: current training loss = 0.131802\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 13:00:44 end epoch 26/100: acc_train=98.066% acc_val=97.106% acc_test=96.705%\n",
      "2018-04-17 13:00:44 start epoch 27/100:\n",
      "2018-04-17 13:00:44 iteration 1/859: current training loss = 0.038498\n",
      "2018-04-17 13:00:46 iteration 200/859: current training loss = 0.043668\n",
      "2018-04-17 13:00:48 iteration 400/859: current training loss = 0.021122\n",
      "2018-04-17 13:00:50 iteration 600/859: current training loss = 0.003250\n",
      "2018-04-17 13:00:52 iteration 800/859: current training loss = 0.034920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 13:00:53 iteration 859/859: current training loss = 0.009471\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 13:01:08 end epoch 27/100: acc_train=98.263% acc_val=97.128% acc_test=96.933%\n",
      "2018-04-17 13:01:08 start epoch 28/100:\n",
      "2018-04-17 13:01:08 iteration 1/859: current training loss = 0.006701\n",
      "2018-04-17 13:01:11 iteration 200/859: current training loss = 0.013714\n",
      "2018-04-17 13:01:13 iteration 400/859: current training loss = 0.045426\n",
      "2018-04-17 13:01:15 iteration 600/859: current training loss = 0.074597\n",
      "2018-04-17 13:01:16 iteration 800/859: current training loss = 0.077855\n",
      "2018-04-17 13:01:17 iteration 859/859: current training loss = 0.063901\n",
      "2018-04-17 13:01:32 end epoch 28/100: acc_train=98.242% acc_val=96.906% acc_test=96.812%\n",
      "2018-04-17 13:01:32 start epoch 29/100:\n",
      "2018-04-17 13:01:32 iteration 1/859: current training loss = 0.034085\n",
      "2018-04-17 13:01:34 iteration 200/859: current training loss = 0.065635\n",
      "2018-04-17 13:01:36 iteration 400/859: current training loss = 0.062827\n",
      "2018-04-17 13:01:38 iteration 600/859: current training loss = 0.035394\n",
      "2018-04-17 13:01:41 iteration 800/859: current training loss = 0.053150\n",
      "2018-04-17 13:01:42 iteration 859/859: current training loss = 0.113371\n",
      "2018-04-17 13:01:57 end epoch 29/100: acc_train=98.194% acc_val=96.741% acc_test=96.883%\n",
      "2018-04-17 13:01:57 start epoch 30/100:\n",
      "2018-04-17 13:01:57 iteration 1/859: current training loss = 0.033238\n",
      "2018-04-17 13:01:58 iteration 200/859: current training loss = 0.039214\n",
      "2018-04-17 13:02:00 iteration 400/859: current training loss = 0.061530\n",
      "2018-04-17 13:02:03 iteration 600/859: current training loss = 0.010999\n",
      "2018-04-17 13:02:04 iteration 800/859: current training loss = 0.015755\n",
      "2018-04-17 13:02:05 iteration 859/859: current training loss = 0.010382\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 13:02:20 end epoch 30/100: acc_train=98.525% acc_val=97.188% acc_test=97.030%\n",
      "2018-04-17 13:02:20 start epoch 31/100:\n",
      "2018-04-17 13:02:20 iteration 1/859: current training loss = 0.117080\n",
      "2018-04-17 13:02:23 iteration 200/859: current training loss = 0.114345\n",
      "2018-04-17 13:02:25 iteration 400/859: current training loss = 0.016662\n",
      "2018-04-17 13:02:27 iteration 600/859: current training loss = 0.011364\n",
      "2018-04-17 13:02:29 iteration 800/859: current training loss = 0.039789\n",
      "2018-04-17 13:02:29 iteration 859/859: current training loss = 0.055015\n",
      "2018-04-17 13:02:44 end epoch 31/100: acc_train=98.628% acc_val=97.078% acc_test=97.053%\n",
      "2018-04-17 13:02:44 start epoch 32/100:\n",
      "2018-04-17 13:02:44 iteration 1/859: current training loss = 0.033065\n",
      "2018-04-17 13:02:46 iteration 200/859: current training loss = 0.024158\n",
      "2018-04-17 13:02:48 iteration 400/859: current training loss = 0.055276\n",
      "2018-04-17 13:02:50 iteration 600/859: current training loss = 0.037930\n",
      "2018-04-17 13:02:55 iteration 800/859: current training loss = 0.069306\n",
      "2018-04-17 13:02:55 iteration 859/859: current training loss = 0.045703\n",
      "2018-04-17 13:03:11 end epoch 32/100: acc_train=98.581% acc_val=97.166% acc_test=96.873%\n",
      "2018-04-17 13:03:11 start epoch 33/100:\n",
      "2018-04-17 13:03:11 iteration 1/859: current training loss = 0.019463\n",
      "2018-04-17 13:03:13 iteration 200/859: current training loss = 0.134082\n",
      "2018-04-17 13:03:14 iteration 400/859: current training loss = 0.069161\n",
      "2018-04-17 13:03:17 iteration 600/859: current training loss = 0.087689\n",
      "2018-04-17 13:03:18 iteration 800/859: current training loss = 0.066938\n",
      "2018-04-17 13:03:19 iteration 859/859: current training loss = 0.021927\n",
      "2018-04-17 13:03:33 end epoch 33/100: acc_train=98.623% acc_val=97.084% acc_test=97.067%\n",
      "2018-04-17 13:03:33 start epoch 34/100:\n",
      "2018-04-17 13:03:33 iteration 1/859: current training loss = 0.008234\n",
      "2018-04-17 13:03:36 iteration 200/859: current training loss = 0.028028\n",
      "2018-04-17 13:03:38 iteration 400/859: current training loss = 0.028091\n",
      "2018-04-17 13:03:40 iteration 600/859: current training loss = 0.030566\n",
      "2018-04-17 13:03:41 iteration 800/859: current training loss = 0.132944\n",
      "2018-04-17 13:03:42 iteration 859/859: current training loss = 0.025164\n",
      "2018-04-17 13:03:56 end epoch 34/100: acc_train=98.688% acc_val=97.169% acc_test=97.036%\n",
      "2018-04-17 13:03:56 start epoch 35/100:\n",
      "2018-04-17 13:03:56 iteration 1/859: current training loss = 0.033689\n",
      "2018-04-17 13:03:58 iteration 200/859: current training loss = 0.019967\n",
      "2018-04-17 13:03:59 iteration 400/859: current training loss = 0.045087\n",
      "2018-04-17 13:04:01 iteration 600/859: current training loss = 0.026723\n",
      "2018-04-17 13:04:04 iteration 800/859: current training loss = 0.002765\n",
      "2018-04-17 13:04:05 iteration 859/859: current training loss = 0.116209\n",
      "2018-04-17 13:04:18 end epoch 35/100: acc_train=98.817% acc_val=97.062% acc_test=97.089%\n",
      "2018-04-17 13:04:18 start epoch 36/100:\n",
      "2018-04-17 13:04:18 iteration 1/859: current training loss = 0.081469\n",
      "2018-04-17 13:04:20 iteration 200/859: current training loss = 0.041630\n",
      "2018-04-17 13:04:21 iteration 400/859: current training loss = 0.076574\n",
      "2018-04-17 13:04:24 iteration 600/859: current training loss = 0.069722\n",
      "2018-04-17 13:04:26 iteration 800/859: current training loss = 0.036546\n",
      "2018-04-17 13:04:27 iteration 859/859: current training loss = 0.017660\n",
      "2018-04-17 13:04:42 end epoch 36/100: acc_train=98.832% acc_val=97.097% acc_test=97.120%\n",
      "2018-04-17 13:04:42 start epoch 37/100:\n",
      "2018-04-17 13:04:42 iteration 1/859: current training loss = 0.011116\n",
      "2018-04-17 13:04:43 iteration 200/859: current training loss = 0.023982\n",
      "2018-04-17 13:04:48 iteration 400/859: current training loss = 0.010046\n",
      "2018-04-17 13:04:50 iteration 600/859: current training loss = 0.010602\n",
      "2018-04-17 13:04:52 iteration 800/859: current training loss = 0.022062\n",
      "2018-04-17 13:04:53 iteration 859/859: current training loss = 0.059672\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 13:05:11 end epoch 37/100: acc_train=98.957% acc_val=97.425% acc_test=97.327%\n",
      "2018-04-17 13:05:11 start epoch 38/100:\n",
      "2018-04-17 13:05:11 iteration 1/859: current training loss = 0.009834\n",
      "2018-04-17 13:05:13 iteration 200/859: current training loss = 0.045709\n",
      "2018-04-17 13:05:15 iteration 400/859: current training loss = 0.067410\n",
      "2018-04-17 13:05:17 iteration 600/859: current training loss = 0.031930\n",
      "2018-04-17 13:05:20 iteration 800/859: current training loss = 0.017175\n",
      "2018-04-17 13:05:20 iteration 859/859: current training loss = 0.034356\n",
      "2018-04-17 13:05:35 end epoch 38/100: acc_train=98.912% acc_val=97.209% acc_test=97.102%\n",
      "2018-04-17 13:05:36 start epoch 39/100:\n",
      "2018-04-17 13:05:36 iteration 1/859: current training loss = 0.016194\n",
      "2018-04-17 13:05:37 iteration 200/859: current training loss = 0.026411\n",
      "2018-04-17 13:05:39 iteration 400/859: current training loss = 0.047526\n",
      "2018-04-17 13:05:43 iteration 600/859: current training loss = 0.011228\n",
      "2018-04-17 13:05:45 iteration 800/859: current training loss = 0.027316\n",
      "2018-04-17 13:05:46 iteration 859/859: current training loss = 0.016675\n",
      "2018-04-17 13:06:01 end epoch 39/100: acc_train=98.884% acc_val=97.206% acc_test=97.027%\n",
      "2018-04-17 13:06:01 start epoch 40/100:\n",
      "2018-04-17 13:06:01 iteration 1/859: current training loss = 0.020811\n",
      "2018-04-17 13:06:03 iteration 200/859: current training loss = 0.043295\n",
      "2018-04-17 13:06:06 iteration 400/859: current training loss = 0.047972\n",
      "2018-04-17 13:06:07 iteration 600/859: current training loss = 0.143900\n",
      "2018-04-17 13:06:09 iteration 800/859: current training loss = 0.066057\n",
      "2018-04-17 13:06:10 iteration 859/859: current training loss = 0.017714\n",
      "2018-04-17 13:06:26 end epoch 40/100: acc_train=99.058% acc_val=97.041% acc_test=97.086%\n",
      "2018-04-17 13:06:26 start epoch 41/100:\n",
      "2018-04-17 13:06:26 iteration 1/859: current training loss = 0.037432\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 13:08:25 end epoch 45/100: acc_train=99.113% acc_val=97.491% acc_test=97.283%\n",
      "2018-04-17 13:08:25 start epoch 46/100:\n",
      "2018-04-17 13:08:25 iteration 1/859: current training loss = 0.090616\n",
      "2018-04-17 13:08:27 iteration 200/859: current training loss = 0.019181\n",
      "2018-04-17 13:08:31 iteration 400/859: current training loss = 0.050185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 13:08:33 iteration 600/859: current training loss = 0.070535\n",
      "2018-04-17 13:08:35 iteration 800/859: current training loss = 0.005099\n",
      "2018-04-17 13:08:35 iteration 859/859: current training loss = 0.031220\n",
      "2018-04-17 13:08:51 end epoch 46/100: acc_train=99.111% acc_val=96.972% acc_test=97.169%\n",
      "2018-04-17 13:08:51 start epoch 47/100:\n",
      "2018-04-17 13:08:51 iteration 1/859: current training loss = 0.055605\n",
      "2018-04-17 13:08:53 iteration 200/859: current training loss = 0.063109\n",
      "2018-04-17 13:08:55 iteration 400/859: current training loss = 0.050711\n",
      "2018-04-17 13:08:57 iteration 600/859: current training loss = 0.059239\n",
      "2018-04-17 13:08:59 iteration 800/859: current training loss = 0.007327\n",
      "2018-04-17 13:09:01 iteration 859/859: current training loss = 0.003897\n",
      "2018-04-17 13:09:15 end epoch 47/100: acc_train=99.362% acc_val=97.484% acc_test=97.425%\n",
      "2018-04-17 13:09:15 start epoch 48/100:\n",
      "2018-04-17 13:09:15 iteration 1/859: current training loss = 0.009237\n",
      "2018-04-17 13:09:17 iteration 200/859: current training loss = 0.014931\n",
      "2018-04-17 13:09:19 iteration 400/859: current training loss = 0.014500\n",
      "2018-04-17 13:09:22 iteration 600/859: current training loss = 0.006934\n",
      "2018-04-17 13:09:24 iteration 800/859: current training loss = 0.001103\n",
      "2018-04-17 13:09:24 iteration 859/859: current training loss = 0.013882\n",
      "2018-04-17 13:09:41 end epoch 48/100: acc_train=99.352% acc_val=97.441% acc_test=97.230%\n",
      "2018-04-17 13:09:41 start epoch 49/100:\n",
      "2018-04-17 13:09:41 iteration 1/859: current training loss = 0.011361\n",
      "2018-04-17 13:09:43 iteration 200/859: current training loss = 0.021727\n",
      "2018-04-17 13:09:47 iteration 400/859: current training loss = 0.005035\n",
      "2018-04-17 13:09:49 iteration 600/859: current training loss = 0.092383\n",
      "2018-04-17 13:09:51 iteration 800/859: current training loss = 0.001935\n",
      "2018-04-17 13:09:51 iteration 859/859: current training loss = 0.004602\n",
      "2018-04-17 13:10:08 end epoch 49/100: acc_train=99.474% acc_val=97.491% acc_test=97.395%\n",
      "2018-04-17 13:10:08 start epoch 50/100:\n",
      "2018-04-17 13:10:08 iteration 1/859: current training loss = 0.001026\n",
      "2018-04-17 13:10:10 iteration 200/859: current training loss = 0.002177\n",
      "2018-04-17 13:10:12 iteration 400/859: current training loss = 0.023649\n",
      "2018-04-17 13:10:13 iteration 600/859: current training loss = 0.011142\n",
      "2018-04-17 13:10:15 iteration 800/859: current training loss = 0.020017\n",
      "2018-04-17 13:10:18 iteration 859/859: current training loss = 0.025987\n",
      "2018-04-17 13:10:33 end epoch 50/100: acc_train=99.500% acc_val=97.441% acc_test=97.420%\n",
      "2018-04-17 13:10:33 start epoch 51/100:\n",
      "2018-04-17 13:10:33 iteration 1/859: current training loss = 0.013023\n",
      "2018-04-17 13:10:35 iteration 200/859: current training loss = 0.019367\n",
      "2018-04-17 13:10:37 iteration 400/859: current training loss = 0.010042\n",
      "2018-04-17 13:10:41 iteration 600/859: current training loss = 0.011087\n",
      "2018-04-17 13:10:43 iteration 800/859: current training loss = 0.013452\n",
      "2018-04-17 13:10:44 iteration 859/859: current training loss = 0.001659\n",
      "2018-04-17 13:11:00 end epoch 51/100: acc_train=99.432% acc_val=97.391% acc_test=97.352%\n",
      "2018-04-17 13:11:00 start epoch 52/100:\n",
      "2018-04-17 13:11:00 iteration 1/859: current training loss = 0.011697\n",
      "2018-04-17 13:11:02 iteration 200/859: current training loss = 0.029754\n",
      "2018-04-17 13:11:05 iteration 400/859: current training loss = 0.023668\n",
      "2018-04-17 13:11:07 iteration 600/859: current training loss = 0.005117\n",
      "2018-04-17 13:11:09 iteration 800/859: current training loss = 0.130029\n",
      "2018-04-17 13:11:10 iteration 859/859: current training loss = 0.010567\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 13:11:26 end epoch 52/100: acc_train=99.508% acc_val=97.506% acc_test=97.581%\n",
      "2018-04-17 13:11:26 start epoch 53/100:\n",
      "2018-04-17 13:11:26 iteration 1/859: current training loss = 0.010411\n",
      "2018-04-17 13:11:29 iteration 200/859: current training loss = 0.106119\n",
      "2018-04-17 13:11:31 iteration 400/859: current training loss = 0.005424\n",
      "2018-04-17 13:11:34 iteration 600/859: current training loss = 0.016863\n",
      "2018-04-17 13:11:36 iteration 800/859: current training loss = 0.023730\n",
      "2018-04-17 13:11:36 iteration 859/859: current training loss = 0.011003\n",
      "2018-04-17 13:11:52 end epoch 53/100: acc_train=99.573% acc_val=97.256% acc_test=97.314%\n",
      "2018-04-17 13:11:52 start epoch 54/100:\n",
      "2018-04-17 13:11:52 iteration 1/859: current training loss = 0.052833\n",
      "2018-04-17 13:11:54 iteration 200/859: current training loss = 0.006304\n",
      "2018-04-17 13:11:56 iteration 400/859: current training loss = 0.045809\n",
      "2018-04-17 13:11:59 iteration 600/859: current training loss = 0.006472\n",
      "2018-04-17 13:12:01 iteration 800/859: current training loss = 0.004323\n",
      "2018-04-17 13:12:01 iteration 859/859: current training loss = 0.005053\n",
      "2018-04-17 13:12:16 end epoch 54/100: acc_train=99.575% acc_val=97.425% acc_test=97.373%\n",
      "2018-04-17 13:12:16 start epoch 55/100:\n",
      "2018-04-17 13:12:16 iteration 1/859: current training loss = 0.001359\n",
      "2018-04-17 13:12:18 iteration 200/859: current training loss = 0.004622\n",
      "2018-04-17 13:12:21 iteration 400/859: current training loss = 0.046029\n",
      "2018-04-17 13:12:23 iteration 600/859: current training loss = 0.007019\n",
      "2018-04-17 13:12:25 iteration 800/859: current training loss = 0.027848\n",
      "2018-04-17 13:12:25 iteration 859/859: current training loss = 0.023702\n",
      "2018-04-17 13:12:40 end epoch 55/100: acc_train=99.587% acc_val=97.216% acc_test=97.484%\n",
      "2018-04-17 13:12:40 start epoch 56/100:\n",
      "2018-04-17 13:12:40 iteration 1/859: current training loss = 0.007461\n",
      "2018-04-17 13:12:43 iteration 200/859: current training loss = 0.007791\n",
      "2018-04-17 13:12:45 iteration 400/859: current training loss = 0.019259\n",
      "2018-04-17 13:12:47 iteration 600/859: current training loss = 0.003829\n",
      "2018-04-17 13:12:48 iteration 800/859: current training loss = 0.018391\n",
      "2018-04-17 13:12:49 iteration 859/859: current training loss = 0.010634\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 13:13:05 end epoch 56/100: acc_train=99.628% acc_val=97.525% acc_test=97.562%\n",
      "2018-04-17 13:13:05 start epoch 57/100:\n",
      "2018-04-17 13:13:05 iteration 1/859: current training loss = 0.003880\n",
      "2018-04-17 13:13:07 iteration 200/859: current training loss = 0.027398\n",
      "2018-04-17 13:13:09 iteration 400/859: current training loss = 0.012716\n",
      "2018-04-17 13:13:12 iteration 600/859: current training loss = 0.004057\n",
      "2018-04-17 13:13:14 iteration 800/859: current training loss = 0.005090\n",
      "2018-04-17 13:13:15 iteration 859/859: current training loss = 0.004701\n",
      "2018-04-17 13:13:32 end epoch 57/100: acc_train=99.602% acc_val=97.469% acc_test=97.339%\n",
      "2018-04-17 13:13:32 start epoch 58/100:\n",
      "2018-04-17 13:13:32 iteration 1/859: current training loss = 0.014385\n",
      "2018-04-17 13:13:33 iteration 200/859: current training loss = 0.007624\n",
      "2018-04-17 13:13:36 iteration 400/859: current training loss = 0.013412\n",
      "2018-04-17 13:13:37 iteration 600/859: current training loss = 0.014401\n",
      "2018-04-17 13:13:39 iteration 800/859: current training loss = 0.002077\n",
      "2018-04-17 13:13:39 iteration 859/859: current training loss = 0.008432\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 13:13:54 end epoch 58/100: acc_train=99.562% acc_val=97.559% acc_test=97.386%\n",
      "2018-04-17 13:13:54 start epoch 59/100:\n",
      "2018-04-17 13:13:54 iteration 1/859: current training loss = 0.011458\n",
      "2018-04-17 13:13:58 iteration 200/859: current training loss = 0.048089\n",
      "2018-04-17 13:13:59 iteration 400/859: current training loss = 0.011773\n",
      "2018-04-17 13:14:02 iteration 600/859: current training loss = 0.007481\n",
      "2018-04-17 13:14:04 iteration 800/859: current training loss = 0.015971\n",
      "2018-04-17 13:14:04 iteration 859/859: current training loss = 0.019348\n",
      "2018-04-17 13:14:19 end epoch 59/100: acc_train=99.257% acc_val=96.847% acc_test=97.214%\n",
      "2018-04-17 13:14:19 start epoch 60/100:\n",
      "2018-04-17 13:14:19 iteration 1/859: current training loss = 0.012917\n",
      "2018-04-17 13:14:21 iteration 200/859: current training loss = 0.002748\n",
      "2018-04-17 13:14:23 iteration 400/859: current training loss = 0.002235\n",
      "2018-04-17 13:14:25 iteration 600/859: current training loss = 0.039776\n",
      "2018-04-17 13:14:29 iteration 800/859: current training loss = 0.085321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 13:14:30 iteration 859/859: current training loss = 0.006695\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 13:14:44 end epoch 60/100: acc_train=99.759% acc_val=97.678% acc_test=97.541%\n",
      "2018-04-17 13:14:44 start epoch 61/100:\n",
      "2018-04-17 13:14:44 iteration 1/859: current training loss = 0.006601\n",
      "2018-04-17 13:14:46 iteration 200/859: current training loss = 0.028407\n",
      "2018-04-17 13:14:49 iteration 400/859: current training loss = 0.001811\n",
      "2018-04-17 13:14:51 iteration 600/859: current training loss = 0.011395\n",
      "2018-04-17 13:14:53 iteration 800/859: current training loss = 0.009340\n",
      "2018-04-17 13:14:53 iteration 859/859: current training loss = 0.002763\n",
      "2018-04-17 13:15:07 end epoch 61/100: acc_train=99.686% acc_val=97.494% acc_test=97.458%\n",
      "2018-04-17 13:15:07 start epoch 62/100:\n",
      "2018-04-17 13:15:07 iteration 1/859: current training loss = 0.004034\n",
      "2018-04-17 13:15:11 iteration 200/859: current training loss = 0.007319\n",
      "2018-04-17 13:15:13 iteration 400/859: current training loss = 0.011141\n",
      "2018-04-17 13:15:14 iteration 600/859: current training loss = 0.026892\n",
      "2018-04-17 13:15:16 iteration 800/859: current training loss = 0.003034\n",
      "2018-04-17 13:15:17 iteration 859/859: current training loss = 0.023995\n",
      "2018-04-17 13:15:32 end epoch 62/100: acc_train=99.755% acc_val=97.550% acc_test=97.530%\n",
      "2018-04-17 13:15:32 start epoch 63/100:\n",
      "2018-04-17 13:15:32 iteration 1/859: current training loss = 0.000985\n",
      "2018-04-17 13:15:33 iteration 200/859: current training loss = 0.001950\n",
      "2018-04-17 13:15:35 iteration 400/859: current training loss = 0.001019\n",
      "2018-04-17 13:15:37 iteration 600/859: current training loss = 0.000992\n",
      "2018-04-17 13:15:40 iteration 800/859: current training loss = 0.001175\n",
      "2018-04-17 13:15:40 iteration 859/859: current training loss = 0.000883\n",
      "2018-04-17 13:15:55 end epoch 63/100: acc_train=99.782% acc_val=97.513% acc_test=97.392%\n",
      "2018-04-17 13:15:55 start epoch 64/100:\n",
      "2018-04-17 13:15:55 iteration 1/859: current training loss = 0.006252\n",
      "2018-04-17 13:15:57 iteration 200/859: current training loss = 0.002195\n",
      "2018-04-17 13:16:00 iteration 400/859: current training loss = 0.002756\n",
      "2018-04-17 13:16:02 iteration 600/859: current training loss = 0.015218\n",
      "2018-04-17 13:16:04 iteration 800/859: current training loss = 0.026737\n",
      "2018-04-17 13:16:05 iteration 859/859: current training loss = 0.003454\n",
      "2018-04-17 13:16:19 end epoch 64/100: acc_train=99.595% acc_val=97.159% acc_test=97.361%\n",
      "2018-04-17 13:16:19 start epoch 65/100:\n",
      "2018-04-17 13:16:19 iteration 1/859: current training loss = 0.010093\n",
      "2018-04-17 13:16:22 iteration 200/859: current training loss = 0.009828\n",
      "2018-04-17 13:16:24 iteration 400/859: current training loss = 0.034307\n",
      "2018-04-17 13:16:26 iteration 600/859: current training loss = 0.002091\n",
      "2018-04-17 13:16:28 iteration 800/859: current training loss = 0.033469\n",
      "2018-04-17 13:16:29 iteration 859/859: current training loss = 0.018264\n",
      "2018-04-17 13:16:43 end epoch 65/100: acc_train=99.488% acc_val=97.144% acc_test=97.091%\n",
      "2018-04-17 13:16:43 start epoch 66/100:\n",
      "2018-04-17 13:16:43 iteration 1/859: current training loss = 0.031171\n",
      "2018-04-17 13:16:45 iteration 200/859: current training loss = 0.003046\n",
      "2018-04-17 13:16:47 iteration 400/859: current training loss = 0.012437\n",
      "2018-04-17 13:16:49 iteration 600/859: current training loss = 0.001405\n",
      "2018-04-17 13:16:52 iteration 800/859: current training loss = 0.000612\n",
      "2018-04-17 13:16:52 iteration 859/859: current training loss = 0.000883\n",
      "2018-04-17 13:17:07 end epoch 66/100: acc_train=99.853% acc_val=97.575% acc_test=97.484%\n",
      "2018-04-17 13:17:07 start epoch 67/100:\n",
      "2018-04-17 13:17:07 iteration 1/859: current training loss = 0.000874\n",
      "2018-04-17 13:17:08 iteration 200/859: current training loss = 0.011517\n",
      "2018-04-17 13:17:12 iteration 400/859: current training loss = 0.002438\n",
      "2018-04-17 13:17:14 iteration 600/859: current training loss = 0.017266\n",
      "2018-04-17 13:17:16 iteration 800/859: current training loss = 0.003983\n",
      "2018-04-17 13:17:16 iteration 859/859: current training loss = 0.008942\n",
      "2018-04-17 13:17:31 end epoch 67/100: acc_train=99.845% acc_val=97.547% acc_test=97.702%\n",
      "2018-04-17 13:17:31 start epoch 68/100:\n",
      "2018-04-17 13:17:31 iteration 1/859: current training loss = 0.003860\n",
      "2018-04-17 13:17:33 iteration 200/859: current training loss = 0.003354\n",
      "2018-04-17 13:17:35 iteration 400/859: current training loss = 0.006494\n",
      "2018-04-17 13:17:37 iteration 600/859: current training loss = 0.008345\n",
      "2018-04-17 13:17:39 iteration 800/859: current training loss = 0.008654\n",
      "2018-04-17 13:17:39 iteration 859/859: current training loss = 0.001965\n",
      "2018-04-17 13:17:55 end epoch 68/100: acc_train=99.838% acc_val=97.419% acc_test=97.483%\n",
      "2018-04-17 13:17:55 start epoch 69/100:\n",
      "2018-04-17 13:17:55 iteration 1/859: current training loss = 0.001483\n",
      "2018-04-17 13:17:57 iteration 200/859: current training loss = 0.002620\n",
      "2018-04-17 13:17:59 iteration 400/859: current training loss = 0.004772\n",
      "2018-04-17 13:18:01 iteration 600/859: current training loss = 0.019906\n",
      "2018-04-17 13:18:04 iteration 800/859: current training loss = 0.002787\n",
      "2018-04-17 13:18:04 iteration 859/859: current training loss = 0.002069\n",
      "2018-04-17 13:18:19 end epoch 69/100: acc_train=99.830% acc_val=97.428% acc_test=97.503%\n",
      "2018-04-17 13:18:19 start epoch 70/100:\n",
      "2018-04-17 13:18:19 iteration 1/859: current training loss = 0.005156\n",
      "2018-04-17 13:18:20 iteration 200/859: current training loss = 0.005708\n",
      "2018-04-17 13:18:23 iteration 400/859: current training loss = 0.002634\n",
      "2018-04-17 13:18:25 iteration 600/859: current training loss = 0.000622\n",
      "2018-04-17 13:18:26 iteration 800/859: current training loss = 0.002226\n",
      "2018-04-17 13:18:27 iteration 859/859: current training loss = 0.019497\n",
      "2018-04-17 13:18:42 end epoch 70/100: acc_train=99.853% acc_val=97.472% acc_test=97.452%\n",
      "2018-04-17 13:18:42 start epoch 71/100:\n",
      "2018-04-17 13:18:42 iteration 1/859: current training loss = 0.001764\n",
      "2018-04-17 13:18:44 iteration 200/859: current training loss = 0.002972\n",
      "2018-04-17 13:18:46 iteration 400/859: current training loss = 0.002452\n",
      "2018-04-17 13:18:48 iteration 600/859: current training loss = 0.003665\n",
      "2018-04-17 13:18:50 iteration 800/859: current training loss = 0.017234\n",
      "2018-04-17 13:18:51 iteration 859/859: current training loss = 0.001995\n",
      "2018-04-17 13:19:07 end epoch 71/100: acc_train=99.791% acc_val=97.553% acc_test=97.452%\n",
      "2018-04-17 13:19:07 start epoch 72/100:\n",
      "2018-04-17 13:19:07 iteration 1/859: current training loss = 0.004194\n",
      "2018-04-17 13:19:09 iteration 200/859: current training loss = 0.007279\n",
      "2018-04-17 13:19:11 iteration 400/859: current training loss = 0.000940\n",
      "2018-04-17 13:19:12 iteration 600/859: current training loss = 0.002428\n",
      "2018-04-17 13:19:15 iteration 800/859: current training loss = 0.008336\n",
      "2018-04-17 13:19:16 iteration 859/859: current training loss = 0.002464\n",
      "2018-04-17 13:19:30 end epoch 72/100: acc_train=99.681% acc_val=97.509% acc_test=97.300%\n",
      "2018-04-17 13:19:30 start epoch 73/100:\n",
      "2018-04-17 13:19:30 iteration 1/859: current training loss = 0.002434\n",
      "2018-04-17 13:19:32 iteration 200/859: current training loss = 0.001153\n",
      "2018-04-17 13:19:34 iteration 400/859: current training loss = 0.001495\n",
      "2018-04-17 13:19:36 iteration 600/859: current training loss = 0.002233\n",
      "2018-04-17 13:19:38 iteration 800/859: current training loss = 0.003461\n",
      "2018-04-17 13:19:39 iteration 859/859: current training loss = 0.002804\n",
      "2018-04-17 13:19:53 end epoch 73/100: acc_train=99.827% acc_val=97.478% acc_test=97.433%\n",
      "2018-04-17 13:19:53 start epoch 74/100:\n",
      "2018-04-17 13:19:53 iteration 1/859: current training loss = 0.003468\n",
      "2018-04-17 13:19:56 iteration 200/859: current training loss = 0.001526\n",
      "2018-04-17 13:19:58 iteration 400/859: current training loss = 0.003202\n",
      "2018-04-17 13:20:00 iteration 600/859: current training loss = 0.008293\n",
      "2018-04-17 13:20:02 iteration 800/859: current training loss = 0.009633\n",
      "2018-04-17 13:20:02 iteration 859/859: current training loss = 0.008422\n",
      "2018-04-17 13:20:17 end epoch 74/100: acc_train=99.771% acc_val=97.569% acc_test=97.466%\n",
      "2018-04-17 13:20:17 start epoch 75/100:\n",
      "2018-04-17 13:20:17 iteration 1/859: current training loss = 0.011755\n",
      "2018-04-17 13:20:19 iteration 200/859: current training loss = 0.002628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 13:20:21 iteration 400/859: current training loss = 0.000748\n",
      "2018-04-17 13:20:23 iteration 600/859: current training loss = 0.000500\n",
      "2018-04-17 13:20:26 iteration 800/859: current training loss = 0.005257\n",
      "2018-04-17 13:20:27 iteration 859/859: current training loss = 0.003906\n",
      "2018-04-17 13:20:41 end epoch 75/100: acc_train=99.820% acc_val=97.097% acc_test=97.211%\n",
      "2018-04-17 13:20:41 start epoch 76/100:\n",
      "2018-04-17 13:20:41 iteration 1/859: current training loss = 0.027441\n",
      "2018-04-17 13:20:43 iteration 200/859: current training loss = 0.002817\n",
      "2018-04-17 13:20:45 iteration 400/859: current training loss = 0.011387\n",
      "2018-04-17 13:20:48 iteration 600/859: current training loss = 0.001013\n",
      "2018-04-17 13:20:50 iteration 800/859: current training loss = 0.001913\n",
      "2018-04-17 13:20:50 iteration 859/859: current training loss = 0.014988\n",
      "2018-04-17 13:21:09 end epoch 76/100: acc_train=99.913% acc_val=97.525% acc_test=97.503%\n",
      "2018-04-17 13:21:09 start epoch 77/100:\n",
      "2018-04-17 13:21:09 iteration 1/859: current training loss = 0.000633\n",
      "2018-04-17 13:21:12 iteration 200/859: current training loss = 0.001737\n",
      "2018-04-17 13:21:14 iteration 400/859: current training loss = 0.001940\n",
      "2018-04-17 13:21:16 iteration 600/859: current training loss = 0.004620\n",
      "2018-04-17 13:21:18 iteration 800/859: current training loss = 0.004178\n",
      "2018-04-17 13:21:18 iteration 859/859: current training loss = 0.001075\n",
      "2018-04-17 13:21:34 end epoch 77/100: acc_train=99.912% acc_val=97.275% acc_test=97.659%\n",
      "2018-04-17 13:21:34 start epoch 78/100:\n",
      "2018-04-17 13:21:34 iteration 1/859: current training loss = 0.002931\n",
      "2018-04-17 13:21:36 iteration 200/859: current training loss = 0.000958\n",
      "2018-04-17 13:21:38 iteration 400/859: current training loss = 0.003091\n",
      "2018-04-17 13:21:40 iteration 600/859: current training loss = 0.002866\n",
      "2018-04-17 13:21:43 iteration 800/859: current training loss = 0.001401\n",
      "2018-04-17 13:21:44 iteration 859/859: current training loss = 0.006614\n",
      "2018-04-17 13:22:00 end epoch 78/100: acc_train=99.902% acc_val=97.650% acc_test=97.484%\n",
      "2018-04-17 13:22:00 start epoch 79/100:\n",
      "2018-04-17 13:22:00 iteration 1/859: current training loss = 0.003187\n",
      "2018-04-17 13:22:02 iteration 200/859: current training loss = 0.001718\n",
      "2018-04-17 13:22:03 iteration 400/859: current training loss = 0.003303\n",
      "2018-04-17 13:22:06 iteration 600/859: current training loss = 0.006152\n",
      "2018-04-17 13:22:08 iteration 800/859: current training loss = 0.006542\n",
      "2018-04-17 13:22:09 iteration 859/859: current training loss = 0.008666\n",
      "2018-04-17 13:22:25 end epoch 79/100: acc_train=99.936% acc_val=97.612% acc_test=97.494%\n",
      "2018-04-17 13:22:25 start epoch 80/100:\n",
      "2018-04-17 13:22:25 iteration 1/859: current training loss = 0.005470\n",
      "2018-04-17 13:22:28 iteration 200/859: current training loss = 0.002179\n",
      "2018-04-17 13:22:30 iteration 400/859: current training loss = 0.002352\n",
      "2018-04-17 13:22:32 iteration 600/859: current training loss = 0.000453\n",
      "2018-04-17 13:22:33 iteration 800/859: current training loss = 0.005246\n",
      "2018-04-17 13:22:34 iteration 859/859: current training loss = 0.001636\n",
      "2018-04-17 13:22:50 end epoch 80/100: acc_train=99.737% acc_val=97.375% acc_test=97.336%\n",
      "2018-04-17 13:22:50 start epoch 81/100:\n",
      "2018-04-17 13:22:50 iteration 1/859: current training loss = 0.030622\n",
      "2018-04-17 13:22:52 iteration 200/859: current training loss = 0.000230\n",
      "2018-04-17 13:22:54 iteration 400/859: current training loss = 0.000524\n",
      "2018-04-17 13:22:56 iteration 600/859: current training loss = 0.003354\n",
      "2018-04-17 13:22:59 iteration 800/859: current training loss = 0.007456\n",
      "2018-04-17 13:22:59 iteration 859/859: current training loss = 0.001044\n",
      "2018-04-17 13:23:14 end epoch 81/100: acc_train=99.927% acc_val=97.541% acc_test=97.519%\n",
      "2018-04-17 13:23:14 start epoch 82/100:\n",
      "2018-04-17 13:23:14 iteration 1/859: current training loss = 0.001388\n",
      "2018-04-17 13:23:16 iteration 200/859: current training loss = 0.001107\n",
      "2018-04-17 13:23:18 iteration 400/859: current training loss = 0.000905\n",
      "2018-04-17 13:23:20 iteration 600/859: current training loss = 0.005802\n",
      "2018-04-17 13:23:22 iteration 800/859: current training loss = 0.000878\n",
      "2018-04-17 13:23:23 iteration 859/859: current training loss = 0.001323\n",
      "2018-04-17 13:23:36 end epoch 82/100: acc_train=99.970% acc_val=97.581% acc_test=97.589%\n",
      "2018-04-17 13:23:36 start epoch 83/100:\n",
      "2018-04-17 13:23:36 iteration 1/859: current training loss = 0.001681\n",
      "2018-04-17 13:23:39 iteration 200/859: current training loss = 0.008480\n",
      "2018-04-17 13:23:41 iteration 400/859: current training loss = 0.001424\n",
      "2018-04-17 13:23:43 iteration 600/859: current training loss = 0.000775\n",
      "2018-04-17 13:23:45 iteration 800/859: current training loss = 0.008458\n",
      "2018-04-17 13:23:45 iteration 859/859: current training loss = 0.000925\n",
      "2018-04-17 13:24:01 end epoch 83/100: acc_train=99.882% acc_val=97.556% acc_test=97.664%\n",
      "2018-04-17 13:24:01 start epoch 84/100:\n",
      "2018-04-17 13:24:01 iteration 1/859: current training loss = 0.003608\n",
      "2018-04-17 13:24:03 iteration 200/859: current training loss = 0.001132\n",
      "2018-04-17 13:24:05 iteration 400/859: current training loss = 0.002911\n",
      "2018-04-17 13:24:07 iteration 600/859: current training loss = 0.001992\n",
      "2018-04-17 13:24:10 iteration 800/859: current training loss = 0.001314\n",
      "2018-04-17 13:24:10 iteration 859/859: current training loss = 0.004201\n",
      "2018-04-17 13:24:25 end epoch 84/100: acc_train=99.972% acc_val=97.662% acc_test=97.714%\n",
      "2018-04-17 13:24:25 start epoch 85/100:\n",
      "2018-04-17 13:24:25 iteration 1/859: current training loss = 0.000514\n",
      "2018-04-17 13:24:27 iteration 200/859: current training loss = 0.001676\n",
      "2018-04-17 13:24:29 iteration 400/859: current training loss = 0.022093\n",
      "2018-04-17 13:24:32 iteration 600/859: current training loss = 0.009396\n",
      "2018-04-17 13:24:34 iteration 800/859: current training loss = 0.000664\n",
      "2018-04-17 13:24:34 iteration 859/859: current training loss = 0.016955\n",
      "2018-04-17 13:24:49 end epoch 85/100: acc_train=99.941% acc_val=97.569% acc_test=97.736%\n",
      "2018-04-17 13:24:49 start epoch 86/100:\n",
      "2018-04-17 13:24:49 iteration 1/859: current training loss = 0.001792\n",
      "2018-04-17 13:24:52 iteration 200/859: current training loss = 0.001471\n",
      "2018-04-17 13:24:54 iteration 400/859: current training loss = 0.002624\n",
      "2018-04-17 13:24:56 iteration 600/859: current training loss = 0.001436\n",
      "2018-04-17 13:24:58 iteration 800/859: current training loss = 0.002258\n",
      "2018-04-17 13:24:58 iteration 859/859: current training loss = 0.011721\n",
      "2018-04-17 13:25:14 end epoch 86/100: acc_train=99.968% acc_val=97.616% acc_test=97.605%\n",
      "2018-04-17 13:25:14 start epoch 87/100:\n",
      "2018-04-17 13:25:14 iteration 1/859: current training loss = 0.000736\n",
      "2018-04-17 13:25:16 iteration 200/859: current training loss = 0.002058\n",
      "2018-04-17 13:25:17 iteration 400/859: current training loss = 0.006649\n",
      "2018-04-17 13:25:19 iteration 600/859: current training loss = 0.013231\n",
      "2018-04-17 13:25:22 iteration 800/859: current training loss = 0.003385\n",
      "2018-04-17 13:25:22 iteration 859/859: current training loss = 0.001567\n",
      "2018-04-17 13:25:37 end epoch 87/100: acc_train=99.982% acc_val=97.375% acc_test=97.612%\n",
      "2018-04-17 13:25:37 start epoch 88/100:\n",
      "2018-04-17 13:25:37 iteration 1/859: current training loss = 0.000957\n",
      "2018-04-17 13:25:38 iteration 200/859: current training loss = 0.000494\n",
      "2018-04-17 13:25:40 iteration 400/859: current training loss = 0.000963\n",
      "2018-04-17 13:25:44 iteration 600/859: current training loss = 0.002489\n",
      "2018-04-17 13:25:46 iteration 800/859: current training loss = 0.002523\n",
      "2018-04-17 13:25:46 iteration 859/859: current training loss = 0.009080\n",
      "2018-04-17 13:26:01 end epoch 88/100: acc_train=99.741% acc_val=97.284% acc_test=97.409%\n",
      "2018-04-17 13:26:01 start epoch 89/100:\n",
      "2018-04-17 13:26:01 iteration 1/859: current training loss = 0.006440\n",
      "2018-04-17 13:26:03 iteration 200/859: current training loss = 0.037705\n",
      "2018-04-17 13:26:06 iteration 400/859: current training loss = 0.002323\n",
      "2018-04-17 13:26:08 iteration 600/859: current training loss = 0.005459\n",
      "2018-04-17 13:26:10 iteration 800/859: current training loss = 0.005229\n",
      "2018-04-17 13:26:10 iteration 859/859: current training loss = 0.005558\n",
      "2018-04-17 13:26:26 end epoch 89/100: acc_train=99.981% acc_val=97.537% acc_test=97.722%\n",
      "2018-04-17 13:26:26 start epoch 90/100:\n",
      "2018-04-17 13:26:26 iteration 1/859: current training loss = 0.000802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 13:26:28 iteration 200/859: current training loss = 0.004679\n",
      "2018-04-17 13:26:29 iteration 400/859: current training loss = 0.035776\n",
      "2018-04-17 13:26:31 iteration 600/859: current training loss = 0.006273\n",
      "2018-04-17 13:26:34 iteration 800/859: current training loss = 0.001328\n",
      "2018-04-17 13:26:34 iteration 859/859: current training loss = 0.013200\n",
      "2018-04-17 13:26:48 end epoch 90/100: acc_train=99.888% acc_val=97.466% acc_test=97.320%\n",
      "2018-04-17 13:26:48 start epoch 91/100:\n",
      "2018-04-17 13:26:48 iteration 1/859: current training loss = 0.000748\n",
      "2018-04-17 13:26:50 iteration 200/859: current training loss = 0.004525\n",
      "2018-04-17 13:26:52 iteration 400/859: current training loss = 0.021099\n",
      "2018-04-17 13:26:55 iteration 600/859: current training loss = 0.001778\n",
      "2018-04-17 13:26:57 iteration 800/859: current training loss = 0.001522\n",
      "2018-04-17 13:26:58 iteration 859/859: current training loss = 0.048503\n",
      "2018-04-17 13:27:13 end epoch 91/100: acc_train=99.805% acc_val=97.394% acc_test=97.405%\n",
      "2018-04-17 13:27:13 start epoch 92/100:\n",
      "2018-04-17 13:27:13 iteration 1/859: current training loss = 0.002459\n",
      "2018-04-17 13:27:15 iteration 200/859: current training loss = 0.005919\n",
      "2018-04-17 13:27:19 iteration 400/859: current training loss = 0.000668\n",
      "2018-04-17 13:27:21 iteration 600/859: current training loss = 0.001019\n",
      "2018-04-17 13:27:23 iteration 800/859: current training loss = 0.001686\n",
      "2018-04-17 13:27:24 iteration 859/859: current training loss = 0.002835\n",
      "2018-04-17 13:27:41 end epoch 92/100: acc_train=99.987% acc_val=97.603% acc_test=97.713%\n",
      "2018-04-17 13:27:41 start epoch 93/100:\n",
      "2018-04-17 13:27:41 iteration 1/859: current training loss = 0.000479\n",
      "2018-04-17 13:27:43 iteration 200/859: current training loss = 0.000792\n",
      "2018-04-17 13:27:45 iteration 400/859: current training loss = 0.000912\n",
      "2018-04-17 13:27:47 iteration 600/859: current training loss = 0.027234\n",
      "2018-04-17 13:27:49 iteration 800/859: current training loss = 0.004597\n",
      "2018-04-17 13:27:50 iteration 859/859: current training loss = 0.000675\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 13:28:07 end epoch 93/100: acc_train=99.941% acc_val=97.744% acc_test=97.572%\n",
      "2018-04-17 13:28:07 start epoch 94/100:\n",
      "2018-04-17 13:28:07 iteration 1/859: current training loss = 0.000212\n",
      "2018-04-17 13:28:09 iteration 200/859: current training loss = 0.000923\n",
      "2018-04-17 13:28:11 iteration 400/859: current training loss = 0.006415\n",
      "2018-04-17 13:28:14 iteration 600/859: current training loss = 0.002649\n",
      "2018-04-17 13:28:15 iteration 800/859: current training loss = 0.001833\n",
      "2018-04-17 13:28:16 iteration 859/859: current training loss = 0.000392\n",
      "2018-04-17 13:28:30 end epoch 94/100: acc_train=99.969% acc_val=97.631% acc_test=97.700%\n",
      "2018-04-17 13:28:30 start epoch 95/100:\n",
      "2018-04-17 13:28:30 iteration 1/859: current training loss = 0.002948\n",
      "2018-04-17 13:28:32 iteration 200/859: current training loss = 0.000571\n",
      "2018-04-17 13:28:35 iteration 400/859: current training loss = 0.001892\n",
      "2018-04-17 13:28:37 iteration 600/859: current training loss = 0.002898\n",
      "2018-04-17 13:28:39 iteration 800/859: current training loss = 0.005946\n",
      "2018-04-17 13:28:39 iteration 859/859: current training loss = 0.064287\n",
      "2018-04-17 13:28:54 end epoch 95/100: acc_train=99.432% acc_val=97.428% acc_test=97.233%\n",
      "2018-04-17 13:28:54 start epoch 96/100:\n",
      "2018-04-17 13:28:54 iteration 1/859: current training loss = 0.000775\n",
      "2018-04-17 13:28:56 iteration 200/859: current training loss = 0.000948\n",
      "2018-04-17 13:28:58 iteration 400/859: current training loss = 0.004076\n",
      "2018-04-17 13:29:00 iteration 600/859: current training loss = 0.002659\n",
      "2018-04-17 13:29:01 iteration 800/859: current training loss = 0.000516\n",
      "2018-04-17 13:29:03 iteration 859/859: current training loss = 0.001432\n",
      "2018-04-17 13:29:17 end epoch 96/100: acc_train=99.987% acc_val=97.675% acc_test=97.588%\n",
      "2018-04-17 13:29:17 start epoch 97/100:\n",
      "2018-04-17 13:29:17 iteration 1/859: current training loss = 0.000430\n",
      "2018-04-17 13:29:19 iteration 200/859: current training loss = 0.009930\n",
      "2018-04-17 13:29:21 iteration 400/859: current training loss = 0.000821\n",
      "2018-04-17 13:29:24 iteration 600/859: current training loss = 0.001053\n",
      "2018-04-17 13:29:26 iteration 800/859: current training loss = 0.002844\n",
      "2018-04-17 13:29:27 iteration 859/859: current training loss = 0.000473\n",
      "2018-04-17 13:29:40 end epoch 97/100: acc_train=99.988% acc_val=97.547% acc_test=97.469%\n",
      "2018-04-17 13:29:40 start epoch 98/100:\n",
      "2018-04-17 13:29:40 iteration 1/859: current training loss = 0.000188\n",
      "2018-04-17 13:29:42 iteration 200/859: current training loss = 0.000768\n",
      "2018-04-17 13:29:45 iteration 400/859: current training loss = 0.000458\n",
      "2018-04-17 13:29:47 iteration 600/859: current training loss = 0.001829\n",
      "2018-04-17 13:29:49 iteration 800/859: current training loss = 0.001007\n",
      "2018-04-17 13:29:50 iteration 859/859: current training loss = 0.018269\n",
      "Currently maximum accuracy on validation set, model saved in path: model/SRAM/SRAM.ckpt\n",
      "2018-04-17 13:30:06 end epoch 98/100: acc_train=99.887% acc_val=97.809% acc_test=97.420%\n",
      "2018-04-17 13:30:06 start epoch 99/100:\n",
      "2018-04-17 13:30:06 iteration 1/859: current training loss = 0.000195\n",
      "2018-04-17 13:30:08 iteration 200/859: current training loss = 0.001681\n",
      "2018-04-17 13:30:10 iteration 400/859: current training loss = 0.000402\n",
      "2018-04-17 13:30:11 iteration 600/859: current training loss = 0.004751\n",
      "2018-04-17 13:30:13 iteration 800/859: current training loss = 0.001160\n",
      "2018-04-17 13:30:15 iteration 859/859: current training loss = 0.002927\n",
      "2018-04-17 13:30:29 end epoch 99/100: acc_train=99.958% acc_val=97.650% acc_test=97.619%\n",
      "2018-04-17 13:30:29 start epoch 100/100:\n",
      "2018-04-17 13:30:29 iteration 1/859: current training loss = 0.000918\n",
      "2018-04-17 13:30:31 iteration 200/859: current training loss = 0.001239\n",
      "2018-04-17 13:30:33 iteration 400/859: current training loss = 0.001889\n",
      "2018-04-17 13:30:36 iteration 600/859: current training loss = 0.000537\n",
      "2018-04-17 13:30:38 iteration 800/859: current training loss = 0.000766\n",
      "2018-04-17 13:30:38 iteration 859/859: current training loss = 0.001003\n",
      "2018-04-17 13:30:53 end epoch 100/100: acc_train=99.987% acc_val=97.719% acc_test=97.639%\n"
     ]
    }
   ],
   "source": [
    "acc_train_his=[]\n",
    "acc_val_his=[]\n",
    "acc_test_his=[]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    max_acc=None\n",
    "    for epoch in range(max_epoch):\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'start epoch %d/%d:' % (epoch+1,max_epoch))\n",
    "        train()\n",
    "        loss_train,acc_train=eval(mnist.train,500)\n",
    "        loss_val,acc_val=eval(mnist.validation,70)\n",
    "        loss_test,acc_test=eval(mnist.test,150)\n",
    "        acc_train_his.append(acc_train)\n",
    "        acc_val_his.append(acc_val)\n",
    "        acc_test_his.append(acc_test)\n",
    "        \n",
    "        if max_acc==None or acc_val>max_acc:\n",
    "            max_acc=acc_val\n",
    "            save_path = saver.save(sess, \"model/SRAM/SRAM.ckpt\")\n",
    "            print(\"Currently maximum accuracy on validation set, model saved in path: %s\" % save_path)\n",
    "        \n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'end epoch %d/%d:' % (epoch+1,max_epoch),\n",
    "             'acc_train=%.3f%% acc_val=%.3f%% acc_test=%.3f%%' % (acc_train*100.0,acc_val*100.0,acc_test*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(2)\n",
    "ptr,=plt.plot(range(max_epoch),acc_train_his,'r-')\n",
    "pva,=plt.plot(range(max_epoch),acc_val_his,'b-')\n",
    "pte,=plt.plot(range(max_epoch),acc_test_his,'y-')\n",
    "plt.xlabel('training epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('accuracy on three sets')\n",
    "plt.legend((ptr,pva,pte),('train','validation','test'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/SRAM/SRAM.ckpt\n",
      "Accuracy on training set is 99.878%\n",
      "Accuracy on validation set is 97.796%\n",
      "Accuracy on testing set is 97.426%\n"
     ]
    }
   ],
   "source": [
    "saver=tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, \"model/SRAM/SRAM.ckpt\")\n",
    "    _,acc_train=eval(mnist.train,num_train//batch_size)\n",
    "    _,acc_val=eval(mnist.validation,num_val//batch_size)\n",
    "    _,acc_test=eval(mnist.test,num_test//batch_size)\n",
    "    print('Accuracy on training set is %.3f%%' % (acc_train*100.0))\n",
    "    print('Accuracy on validation set is %.3f%%' % (acc_val*100.0))\n",
    "    print('Accuracy on testing set is %.3f%%' % (acc_test*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
