{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"mnist/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train=mnist.train.num_examples\n",
    "num_val=mnist.validation.images.shape\n",
    "num_test=mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(dtype=tf.float32,shape=[None,28,28,1],name='X')\n",
    "y=tf.placeholder(dtype=tf.int64,shape=[None,10],name='y')\n",
    "is_training=tf.placeholder(dtype=tf.bool,name='is_training')\n",
    "cnn_out1=tf.layers.conv2d(X,128,kernel_size=3,strides=(1, 1),padding='same')\n",
    "bn_out1=tf.layers.batch_normalization(cnn_out1,axis=3,training=is_training)\n",
    "relu_out1=tf.nn.relu(bn_out1)\n",
    "pool_out1=tf.layers.max_pooling2d(relu_out1,[2,2],[2,2])\n",
    "cnn_out2=tf.layers.conv2d(pool_out1,128,kernel_size=3,strides=(1, 1),padding='same')\n",
    "bn_out2=tf.layers.batch_normalization(cnn_out2,axis=3,training=is_training)\n",
    "relu_out1=tf.nn.relu(bn_out2)\n",
    "pool_out2=tf.layers.max_pooling2d(relu_out1,[2,2],[2,2])\n",
    "flt=tf.layers.flatten(pool_out2)\n",
    "out1=tf.layers.dense(flt,1024)\n",
    "# bn_out3=tf.layers.batch_normalization(out1,axis=1,training=is_training)\n",
    "out2=tf.nn.relu(out1)\n",
    "score=tf.layers.dense(out2,10)\n",
    "predictions = tf.argmax(score, 1)\n",
    "correct_predictions = tf.equal(predictions, tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "\n",
    "loss=tf.losses.softmax_cross_entropy(onehot_labels=y,logits=score)\n",
    "optimizier=tf.train.AdamOptimizer(learning_rate=1e-6)\n",
    "train_step = optimizier.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 17:41:56 start epoch 1/50:\n",
      "2018-04-13 17:41:57 iteration 1/6875: current training loss = 3.083660\n",
      "2018-04-13 17:42:00 iteration 625/6875: current training loss = 1.272324\n",
      "2018-04-13 17:42:02 iteration 1250/6875: current training loss = 0.537513\n",
      "2018-04-13 17:42:05 iteration 1875/6875: current training loss = 0.483856\n",
      "2018-04-13 17:42:07 iteration 2500/6875: current training loss = 0.385862\n",
      "2018-04-13 17:42:10 iteration 3125/6875: current training loss = 0.712461\n",
      "2018-04-13 17:42:13 iteration 3750/6875: current training loss = 0.320487\n",
      "2018-04-13 17:42:15 iteration 4375/6875: current training loss = 0.391533\n",
      "2018-04-13 17:42:18 iteration 5000/6875: current training loss = 0.166195\n",
      "2018-04-13 17:42:20 iteration 5625/6875: current training loss = 0.367317\n",
      "2018-04-13 17:42:23 iteration 6250/6875: current training loss = 0.047774\n",
      "2018-04-13 17:42:26 iteration 6875/6875: current training loss = 0.671304\n",
      "2018-04-13 17:42:32 end epoch 1/50: acc_train=77.569% acc_val=79.200% acc_test=78.212%\n",
      "2018-04-13 17:42:32 start epoch 2/50:\n",
      "2018-04-13 17:42:32 iteration 1/6875: current training loss = 0.060834\n",
      "2018-04-13 17:42:35 iteration 625/6875: current training loss = 0.740017\n",
      "2018-04-13 17:42:37 iteration 1250/6875: current training loss = 0.067261\n",
      "2018-04-13 17:42:40 iteration 1875/6875: current training loss = 0.040450\n",
      "2018-04-13 17:42:42 iteration 2500/6875: current training loss = 0.065369\n",
      "2018-04-13 17:42:45 iteration 3125/6875: current training loss = 0.376545\n",
      "2018-04-13 17:42:47 iteration 3750/6875: current training loss = 0.047722\n",
      "2018-04-13 17:42:50 iteration 4375/6875: current training loss = 0.085048\n",
      "2018-04-13 17:42:53 iteration 5000/6875: current training loss = 0.284878\n",
      "2018-04-13 17:42:55 iteration 5625/6875: current training loss = 0.190393\n",
      "2018-04-13 17:42:58 iteration 6250/6875: current training loss = 0.004967\n",
      "2018-04-13 17:43:00 iteration 6875/6875: current training loss = 0.483334\n",
      "2018-04-13 17:43:07 end epoch 2/50: acc_train=87.162% acc_val=88.250% acc_test=88.100%\n",
      "2018-04-13 17:43:07 start epoch 3/50:\n",
      "2018-04-13 17:43:07 iteration 1/6875: current training loss = 0.141819\n",
      "2018-04-13 17:43:10 iteration 625/6875: current training loss = 0.034900\n",
      "2018-04-13 17:43:12 iteration 1250/6875: current training loss = 0.062313\n",
      "2018-04-13 17:43:15 iteration 1875/6875: current training loss = 0.147071\n",
      "2018-04-13 17:43:17 iteration 2500/6875: current training loss = 0.753991\n",
      "2018-04-13 17:43:20 iteration 3125/6875: current training loss = 0.042944\n",
      "2018-04-13 17:43:23 iteration 3750/6875: current training loss = 0.037166\n",
      "2018-04-13 17:43:25 iteration 4375/6875: current training loss = 0.020613\n",
      "2018-04-13 17:43:28 iteration 5000/6875: current training loss = 0.009404\n",
      "2018-04-13 17:43:30 iteration 5625/6875: current training loss = 0.026310\n",
      "2018-04-13 17:43:33 iteration 6250/6875: current training loss = 0.006309\n",
      "2018-04-13 17:43:35 iteration 6875/6875: current training loss = 0.069819\n",
      "2018-04-13 17:43:42 end epoch 3/50: acc_train=92.138% acc_val=91.825% acc_test=91.987%\n",
      "2018-04-13 17:43:42 start epoch 4/50:\n",
      "2018-04-13 17:43:42 iteration 1/6875: current training loss = 0.052548\n",
      "2018-04-13 17:43:44 iteration 625/6875: current training loss = 0.067381\n",
      "2018-04-13 17:43:47 iteration 1250/6875: current training loss = 0.056833\n",
      "2018-04-13 17:43:50 iteration 1875/6875: current training loss = 0.048471\n",
      "2018-04-13 17:43:52 iteration 2500/6875: current training loss = 0.173637\n",
      "2018-04-13 17:43:55 iteration 3125/6875: current training loss = 0.227145\n",
      "2018-04-13 17:43:57 iteration 3750/6875: current training loss = 0.078627\n",
      "2018-04-13 17:44:00 iteration 4375/6875: current training loss = 0.144703\n",
      "2018-04-13 17:44:03 iteration 5000/6875: current training loss = 0.031186\n",
      "2018-04-13 17:44:05 iteration 5625/6875: current training loss = 0.025989\n",
      "2018-04-13 17:44:08 iteration 6250/6875: current training loss = 0.221109\n",
      "2018-04-13 17:44:10 iteration 6875/6875: current training loss = 0.003312\n",
      "2018-04-13 17:44:17 end epoch 4/50: acc_train=94.056% acc_val=93.950% acc_test=94.325%\n",
      "2018-04-13 17:44:17 start epoch 5/50:\n",
      "2018-04-13 17:44:17 iteration 1/6875: current training loss = 0.062403\n",
      "2018-04-13 17:44:20 iteration 625/6875: current training loss = 0.041040\n",
      "2018-04-13 17:44:22 iteration 1250/6875: current training loss = 0.012191\n",
      "2018-04-13 17:44:25 iteration 1875/6875: current training loss = 0.027622\n",
      "2018-04-13 17:44:27 iteration 2500/6875: current training loss = 0.068237\n",
      "2018-04-13 17:44:30 iteration 3125/6875: current training loss = 0.035351\n",
      "2018-04-13 17:44:32 iteration 3750/6875: current training loss = 0.120814\n",
      "2018-04-13 17:44:35 iteration 4375/6875: current training loss = 0.007417\n",
      "2018-04-13 17:44:37 iteration 5000/6875: current training loss = 0.023821\n",
      "2018-04-13 17:44:40 iteration 5625/6875: current training loss = 0.617333\n",
      "2018-04-13 17:44:43 iteration 6250/6875: current training loss = 0.042099\n",
      "2018-04-13 17:44:45 iteration 6875/6875: current training loss = 0.012264\n",
      "2018-04-13 17:44:52 end epoch 5/50: acc_train=94.869% acc_val=94.700% acc_test=95.100%\n",
      "2018-04-13 17:44:52 start epoch 6/50:\n",
      "2018-04-13 17:44:52 iteration 1/6875: current training loss = 0.003611\n",
      "2018-04-13 17:44:55 iteration 625/6875: current training loss = 0.006907\n",
      "2018-04-13 17:44:57 iteration 1250/6875: current training loss = 0.010363\n",
      "2018-04-13 17:45:00 iteration 1875/6875: current training loss = 0.005808\n",
      "2018-04-13 17:45:02 iteration 2500/6875: current training loss = 0.028168\n",
      "2018-04-13 17:45:05 iteration 3125/6875: current training loss = 0.004832\n",
      "2018-04-13 17:45:07 iteration 3750/6875: current training loss = 0.046905\n",
      "2018-04-13 17:45:10 iteration 4375/6875: current training loss = 0.090469\n",
      "2018-04-13 17:45:13 iteration 5000/6875: current training loss = 0.004549\n",
      "2018-04-13 17:45:15 iteration 5625/6875: current training loss = 0.006439\n",
      "2018-04-13 17:45:18 iteration 6250/6875: current training loss = 0.042609\n",
      "2018-04-13 17:45:20 iteration 6875/6875: current training loss = 0.014287\n",
      "2018-04-13 17:45:27 end epoch 6/50: acc_train=96.006% acc_val=95.650% acc_test=95.762%\n",
      "2018-04-13 17:45:27 start epoch 7/50:\n",
      "2018-04-13 17:45:27 iteration 1/6875: current training loss = 0.006370\n",
      "2018-04-13 17:45:29 iteration 625/6875: current training loss = 0.025293\n",
      "2018-04-13 17:45:32 iteration 1250/6875: current training loss = 0.020573\n",
      "2018-04-13 17:45:35 iteration 1875/6875: current training loss = 0.048860\n",
      "2018-04-13 17:45:37 iteration 2500/6875: current training loss = 0.006061\n",
      "2018-04-13 17:45:40 iteration 3125/6875: current training loss = 0.166520\n",
      "2018-04-13 17:45:42 iteration 3750/6875: current training loss = 0.000765\n",
      "2018-04-13 17:45:45 iteration 4375/6875: current training loss = 0.101156\n",
      "2018-04-13 17:45:47 iteration 5000/6875: current training loss = 0.002267\n",
      "2018-04-13 17:45:50 iteration 5625/6875: current training loss = 0.021343\n",
      "2018-04-13 17:45:53 iteration 6250/6875: current training loss = 0.011346\n",
      "2018-04-13 17:45:55 iteration 6875/6875: current training loss = 0.197380\n",
      "2018-04-13 17:46:02 end epoch 7/50: acc_train=96.769% acc_val=96.675% acc_test=96.662%\n",
      "2018-04-13 17:46:02 start epoch 8/50:\n",
      "2018-04-13 17:46:02 iteration 1/6875: current training loss = 0.061760\n",
      "2018-04-13 17:46:05 iteration 625/6875: current training loss = 0.003287\n",
      "2018-04-13 17:46:07 iteration 1250/6875: current training loss = 0.014673\n",
      "2018-04-13 17:46:10 iteration 1875/6875: current training loss = 0.001381\n",
      "2018-04-13 17:46:13 iteration 2500/6875: current training loss = 0.004234\n",
      "2018-04-13 17:46:15 iteration 3125/6875: current training loss = 0.002138\n",
      "2018-04-13 17:46:18 iteration 3750/6875: current training loss = 0.017210\n",
      "2018-04-13 17:46:20 iteration 4375/6875: current training loss = 0.103385\n",
      "2018-04-13 17:46:23 iteration 5000/6875: current training loss = 0.001834\n",
      "2018-04-13 17:46:25 iteration 5625/6875: current training loss = 0.113700\n",
      "2018-04-13 17:46:28 iteration 6250/6875: current training loss = 0.002268\n",
      "2018-04-13 17:46:30 iteration 6875/6875: current training loss = 0.009677\n",
      "2018-04-13 17:46:37 end epoch 8/50: acc_train=96.625% acc_val=96.675% acc_test=96.800%\n",
      "2018-04-13 17:46:37 start epoch 9/50:\n",
      "2018-04-13 17:46:37 iteration 1/6875: current training loss = 0.006080\n",
      "2018-04-13 17:46:40 iteration 625/6875: current training loss = 0.006556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 17:46:42 iteration 1250/6875: current training loss = 0.007404\n",
      "2018-04-13 17:46:45 iteration 1875/6875: current training loss = 0.038815\n",
      "2018-04-13 17:46:47 iteration 2500/6875: current training loss = 0.023837\n",
      "2018-04-13 17:46:50 iteration 3125/6875: current training loss = 0.006166\n",
      "2018-04-13 17:46:53 iteration 3750/6875: current training loss = 0.001890\n",
      "2018-04-13 17:46:55 iteration 4375/6875: current training loss = 0.013509\n",
      "2018-04-13 17:46:58 iteration 5000/6875: current training loss = 0.581377\n",
      "2018-04-13 17:47:00 iteration 5625/6875: current training loss = 0.020825\n",
      "2018-04-13 17:47:03 iteration 6250/6875: current training loss = 0.012956\n",
      "2018-04-13 17:47:05 iteration 6875/6875: current training loss = 0.001586\n",
      "2018-04-13 17:47:11 end epoch 9/50: acc_train=97.319% acc_val=96.950% acc_test=97.225%\n",
      "2018-04-13 17:47:11 start epoch 10/50:\n",
      "2018-04-13 17:47:11 iteration 1/6875: current training loss = 0.005783\n",
      "2018-04-13 17:47:14 iteration 625/6875: current training loss = 0.080624\n",
      "2018-04-13 17:47:16 iteration 1250/6875: current training loss = 0.028686\n",
      "2018-04-13 17:47:19 iteration 1875/6875: current training loss = 0.002622\n",
      "2018-04-13 17:47:21 iteration 2500/6875: current training loss = 0.023822\n",
      "2018-04-13 17:47:24 iteration 3125/6875: current training loss = 0.112592\n",
      "2018-04-13 17:47:26 iteration 3750/6875: current training loss = 0.052892\n",
      "2018-04-13 17:47:29 iteration 4375/6875: current training loss = 0.009233\n",
      "2018-04-13 17:47:31 iteration 5000/6875: current training loss = 0.013949\n",
      "2018-04-13 17:47:34 iteration 5625/6875: current training loss = 0.312470\n",
      "2018-04-13 17:47:36 iteration 6250/6875: current training loss = 0.037870\n",
      "2018-04-13 17:47:39 iteration 6875/6875: current training loss = 0.016215\n",
      "2018-04-13 17:47:45 end epoch 10/50: acc_train=97.500% acc_val=97.275% acc_test=97.125%\n",
      "2018-04-13 17:47:45 start epoch 11/50:\n",
      "2018-04-13 17:47:45 iteration 1/6875: current training loss = 0.108578\n",
      "2018-04-13 17:47:48 iteration 625/6875: current training loss = 0.023815\n",
      "2018-04-13 17:47:51 iteration 1250/6875: current training loss = 0.023073\n",
      "2018-04-13 17:47:53 iteration 1875/6875: current training loss = 0.002703\n",
      "2018-04-13 17:47:56 iteration 2500/6875: current training loss = 0.004196\n",
      "2018-04-13 17:47:58 iteration 3125/6875: current training loss = 0.001802\n",
      "2018-04-13 17:48:01 iteration 3750/6875: current training loss = 0.001635\n",
      "2018-04-13 17:48:03 iteration 4375/6875: current training loss = 0.013139\n",
      "2018-04-13 17:48:06 iteration 5000/6875: current training loss = 0.011587\n",
      "2018-04-13 17:48:08 iteration 5625/6875: current training loss = 0.051403\n",
      "2018-04-13 17:48:11 iteration 6250/6875: current training loss = 0.101313\n",
      "2018-04-13 17:48:14 iteration 6875/6875: current training loss = 0.011229\n",
      "2018-04-13 17:48:19 end epoch 11/50: acc_train=97.556% acc_val=97.200% acc_test=96.975%\n",
      "2018-04-13 17:48:19 start epoch 12/50:\n",
      "2018-04-13 17:48:19 iteration 1/6875: current training loss = 0.006269\n",
      "2018-04-13 17:48:22 iteration 625/6875: current training loss = 0.005023\n",
      "2018-04-13 17:48:24 iteration 1250/6875: current training loss = 0.013870\n",
      "2018-04-13 17:48:27 iteration 1875/6875: current training loss = 0.000832\n",
      "2018-04-13 17:48:30 iteration 2500/6875: current training loss = 0.003195\n",
      "2018-04-13 17:48:32 iteration 3125/6875: current training loss = 0.000868\n",
      "2018-04-13 17:48:35 iteration 3750/6875: current training loss = 0.000971\n",
      "2018-04-13 17:48:37 iteration 4375/6875: current training loss = 0.014456\n",
      "2018-04-13 17:48:40 iteration 5000/6875: current training loss = 0.005153\n",
      "2018-04-13 17:48:42 iteration 5625/6875: current training loss = 0.013490\n",
      "2018-04-13 17:48:45 iteration 6250/6875: current training loss = 0.001661\n",
      "2018-04-13 17:48:48 iteration 6875/6875: current training loss = 0.027850\n",
      "2018-04-13 17:48:54 end epoch 12/50: acc_train=97.556% acc_val=97.325% acc_test=97.362%\n",
      "2018-04-13 17:48:54 start epoch 13/50:\n",
      "2018-04-13 17:48:54 iteration 1/6875: current training loss = 0.033412\n",
      "2018-04-13 17:48:57 iteration 625/6875: current training loss = 0.000520\n",
      "2018-04-13 17:48:59 iteration 1250/6875: current training loss = 0.041831\n",
      "2018-04-13 17:49:02 iteration 1875/6875: current training loss = 0.025852\n",
      "2018-04-13 17:49:05 iteration 2500/6875: current training loss = 0.004447\n",
      "2018-04-13 17:49:07 iteration 3125/6875: current training loss = 0.003740\n",
      "2018-04-13 17:49:10 iteration 3750/6875: current training loss = 0.018278\n",
      "2018-04-13 17:49:12 iteration 4375/6875: current training loss = 0.001491\n",
      "2018-04-13 17:49:15 iteration 5000/6875: current training loss = 0.011404\n",
      "2018-04-13 17:49:17 iteration 5625/6875: current training loss = 0.005645\n",
      "2018-04-13 17:49:20 iteration 6250/6875: current training loss = 0.172350\n",
      "2018-04-13 17:49:22 iteration 6875/6875: current training loss = 0.000114\n",
      "2018-04-13 17:49:28 end epoch 13/50: acc_train=97.938% acc_val=97.825% acc_test=97.400%\n",
      "2018-04-13 17:49:28 start epoch 14/50:\n",
      "2018-04-13 17:49:28 iteration 1/6875: current training loss = 0.002603\n",
      "2018-04-13 17:49:30 iteration 625/6875: current training loss = 0.008381\n",
      "2018-04-13 17:49:33 iteration 1250/6875: current training loss = 0.017011\n",
      "2018-04-13 17:49:35 iteration 1875/6875: current training loss = 0.043928\n",
      "2018-04-13 17:49:38 iteration 2500/6875: current training loss = 0.022234\n",
      "2018-04-13 17:49:40 iteration 3125/6875: current training loss = 0.113248\n",
      "2018-04-13 17:49:43 iteration 3750/6875: current training loss = 0.014241\n",
      "2018-04-13 17:49:46 iteration 4375/6875: current training loss = 0.007259\n",
      "2018-04-13 17:49:48 iteration 5000/6875: current training loss = 0.170623\n",
      "2018-04-13 17:49:51 iteration 5625/6875: current training loss = 0.001776\n",
      "2018-04-13 17:49:54 iteration 6250/6875: current training loss = 0.004277\n",
      "2018-04-13 17:49:56 iteration 6875/6875: current training loss = 0.005493\n",
      "2018-04-13 17:50:02 end epoch 14/50: acc_train=97.688% acc_val=97.250% acc_test=97.362%\n",
      "2018-04-13 17:50:02 start epoch 15/50:\n",
      "2018-04-13 17:50:02 iteration 1/6875: current training loss = 0.012621\n",
      "2018-04-13 17:50:04 iteration 625/6875: current training loss = 0.041154\n",
      "2018-04-13 17:50:07 iteration 1250/6875: current training loss = 0.022695\n",
      "2018-04-13 17:50:10 iteration 1875/6875: current training loss = 0.000879\n",
      "2018-04-13 17:50:12 iteration 2500/6875: current training loss = 0.001176\n",
      "2018-04-13 17:50:15 iteration 3125/6875: current training loss = 0.002569\n",
      "2018-04-13 17:50:17 iteration 3750/6875: current training loss = 0.003208\n",
      "2018-04-13 17:50:20 iteration 4375/6875: current training loss = 0.000237\n",
      "2018-04-13 17:50:22 iteration 5000/6875: current training loss = 0.000577\n",
      "2018-04-13 17:50:25 iteration 5625/6875: current training loss = 0.001903\n",
      "2018-04-13 17:50:27 iteration 6250/6875: current training loss = 0.004013\n",
      "2018-04-13 17:50:30 iteration 6875/6875: current training loss = 0.010320\n",
      "2018-04-13 17:50:36 end epoch 15/50: acc_train=97.537% acc_val=97.425% acc_test=97.312%\n",
      "2018-04-13 17:50:36 start epoch 16/50:\n",
      "2018-04-13 17:50:36 iteration 1/6875: current training loss = 0.001877\n",
      "2018-04-13 17:50:39 iteration 625/6875: current training loss = 0.001726\n",
      "2018-04-13 17:50:41 iteration 1250/6875: current training loss = 0.094942\n",
      "2018-04-13 17:50:44 iteration 1875/6875: current training loss = 0.001889\n",
      "2018-04-13 17:50:47 iteration 2500/6875: current training loss = 0.000941\n",
      "2018-04-13 17:50:49 iteration 3125/6875: current training loss = 0.045065\n",
      "2018-04-13 17:50:52 iteration 3750/6875: current training loss = 0.000318\n",
      "2018-04-13 17:50:54 iteration 4375/6875: current training loss = 0.082359\n",
      "2018-04-13 17:50:57 iteration 5000/6875: current training loss = 0.026522\n",
      "2018-04-13 17:51:00 iteration 5625/6875: current training loss = 0.001478\n",
      "2018-04-13 17:51:02 iteration 6250/6875: current training loss = 0.001074\n",
      "2018-04-13 17:51:05 iteration 6875/6875: current training loss = 0.006887\n",
      "2018-04-13 17:51:11 end epoch 16/50: acc_train=98.294% acc_val=97.650% acc_test=97.825%\n",
      "2018-04-13 17:51:11 start epoch 17/50:\n",
      "2018-04-13 17:51:11 iteration 1/6875: current training loss = 0.144282\n",
      "2018-04-13 17:51:14 iteration 625/6875: current training loss = 0.004522\n",
      "2018-04-13 17:51:16 iteration 1250/6875: current training loss = 0.001144\n",
      "2018-04-13 17:51:19 iteration 1875/6875: current training loss = 0.185436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 17:51:21 iteration 2500/6875: current training loss = 0.008049\n",
      "2018-04-13 17:51:24 iteration 3125/6875: current training loss = 0.003444\n",
      "2018-04-13 17:51:27 iteration 3750/6875: current training loss = 0.015647\n",
      "2018-04-13 17:51:29 iteration 4375/6875: current training loss = 0.041330\n",
      "2018-04-13 17:51:32 iteration 5000/6875: current training loss = 0.002378\n",
      "2018-04-13 17:51:34 iteration 5625/6875: current training loss = 0.007615\n",
      "2018-04-13 17:51:37 iteration 6250/6875: current training loss = 0.010574\n",
      "2018-04-13 17:51:39 iteration 6875/6875: current training loss = 0.032308\n",
      "2018-04-13 17:51:45 end epoch 17/50: acc_train=97.513% acc_val=97.400% acc_test=97.175%\n",
      "2018-04-13 17:51:45 start epoch 18/50:\n",
      "2018-04-13 17:51:45 iteration 1/6875: current training loss = 0.001264\n",
      "2018-04-13 17:51:47 iteration 625/6875: current training loss = 0.024492\n",
      "2018-04-13 17:51:50 iteration 1250/6875: current training loss = 0.005093\n",
      "2018-04-13 17:51:52 iteration 1875/6875: current training loss = 0.011213\n",
      "2018-04-13 17:51:55 iteration 2500/6875: current training loss = 0.002310\n",
      "2018-04-13 17:51:58 iteration 3125/6875: current training loss = 0.000196\n",
      "2018-04-13 17:52:00 iteration 3750/6875: current training loss = 0.017583\n",
      "2018-04-13 17:52:03 iteration 4375/6875: current training loss = 0.000457\n",
      "2018-04-13 17:52:05 iteration 5000/6875: current training loss = 0.009767\n",
      "2018-04-13 17:52:08 iteration 5625/6875: current training loss = 0.000246\n",
      "2018-04-13 17:52:10 iteration 6250/6875: current training loss = 0.003085\n",
      "2018-04-13 17:52:13 iteration 6875/6875: current training loss = 0.034752\n",
      "2018-04-13 17:52:18 end epoch 18/50: acc_train=97.731% acc_val=97.050% acc_test=97.413%\n",
      "2018-04-13 17:52:18 start epoch 19/50:\n",
      "2018-04-13 17:52:18 iteration 1/6875: current training loss = 0.007908\n",
      "2018-04-13 17:52:21 iteration 625/6875: current training loss = 0.000828\n",
      "2018-04-13 17:52:23 iteration 1250/6875: current training loss = 0.002470\n",
      "2018-04-13 17:52:26 iteration 1875/6875: current training loss = 0.043100\n",
      "2018-04-13 17:52:28 iteration 2500/6875: current training loss = 0.000386\n",
      "2018-04-13 17:52:31 iteration 3125/6875: current training loss = 0.000120\n",
      "2018-04-13 17:52:33 iteration 3750/6875: current training loss = 0.009061\n",
      "2018-04-13 17:52:36 iteration 4375/6875: current training loss = 0.011671\n",
      "2018-04-13 17:52:39 iteration 5000/6875: current training loss = 0.000824\n",
      "2018-04-13 17:52:41 iteration 5625/6875: current training loss = 0.052705\n",
      "2018-04-13 17:52:44 iteration 6250/6875: current training loss = 0.003193\n",
      "2018-04-13 17:52:46 iteration 6875/6875: current training loss = 0.009146\n",
      "2018-04-13 17:52:53 end epoch 19/50: acc_train=97.300% acc_val=97.275% acc_test=96.888%\n",
      "2018-04-13 17:52:53 start epoch 20/50:\n",
      "2018-04-13 17:52:53 iteration 1/6875: current training loss = 0.000969\n",
      "2018-04-13 17:52:55 iteration 625/6875: current training loss = 0.000970\n",
      "2018-04-13 17:52:58 iteration 1250/6875: current training loss = 0.020285\n",
      "2018-04-13 17:53:01 iteration 1875/6875: current training loss = 0.035257\n",
      "2018-04-13 17:53:03 iteration 2500/6875: current training loss = 0.083335\n",
      "2018-04-13 17:53:06 iteration 3125/6875: current training loss = 0.007233\n",
      "2018-04-13 17:53:09 iteration 3750/6875: current training loss = 0.000974\n",
      "2018-04-13 17:53:11 iteration 4375/6875: current training loss = 0.043052\n",
      "2018-04-13 17:53:14 iteration 5000/6875: current training loss = 0.045469\n",
      "2018-04-13 17:53:16 iteration 5625/6875: current training loss = 0.119299\n",
      "2018-04-13 17:53:19 iteration 6250/6875: current training loss = 0.005484\n",
      "2018-04-13 17:53:22 iteration 6875/6875: current training loss = 0.008075\n",
      "2018-04-13 17:53:28 end epoch 20/50: acc_train=97.694% acc_val=96.975% acc_test=97.225%\n",
      "2018-04-13 17:53:28 start epoch 21/50:\n",
      "2018-04-13 17:53:28 iteration 1/6875: current training loss = 0.000474\n",
      "2018-04-13 17:53:31 iteration 625/6875: current training loss = 0.005474\n",
      "2018-04-13 17:53:33 iteration 1250/6875: current training loss = 0.001831\n",
      "2018-04-13 17:53:36 iteration 1875/6875: current training loss = 0.009743\n",
      "2018-04-13 17:53:38 iteration 2500/6875: current training loss = 0.004207\n",
      "2018-04-13 17:53:41 iteration 3125/6875: current training loss = 0.000053\n",
      "2018-04-13 17:53:43 iteration 3750/6875: current training loss = 0.000297\n",
      "2018-04-13 17:53:46 iteration 4375/6875: current training loss = 0.001915\n",
      "2018-04-13 17:53:49 iteration 5000/6875: current training loss = 0.008041\n",
      "2018-04-13 17:53:51 iteration 5625/6875: current training loss = 0.002920\n",
      "2018-04-13 17:53:54 iteration 6250/6875: current training loss = 0.014114\n",
      "2018-04-13 17:53:56 iteration 6875/6875: current training loss = 0.048014\n",
      "2018-04-13 17:54:03 end epoch 21/50: acc_train=97.438% acc_val=97.225% acc_test=97.025%\n",
      "2018-04-13 17:54:03 start epoch 22/50:\n",
      "2018-04-13 17:54:03 iteration 1/6875: current training loss = 0.031127\n",
      "2018-04-13 17:54:05 iteration 625/6875: current training loss = 0.000253\n",
      "2018-04-13 17:54:08 iteration 1250/6875: current training loss = 0.000260\n",
      "2018-04-13 17:54:10 iteration 1875/6875: current training loss = 0.043392\n",
      "2018-04-13 17:54:13 iteration 2500/6875: current training loss = 0.000968\n",
      "2018-04-13 17:54:15 iteration 3125/6875: current training loss = 0.005639\n",
      "2018-04-13 17:54:18 iteration 3750/6875: current training loss = 0.000491\n",
      "2018-04-13 17:54:21 iteration 4375/6875: current training loss = 0.003734\n",
      "2018-04-13 17:54:23 iteration 5000/6875: current training loss = 0.001100\n",
      "2018-04-13 17:54:26 iteration 5625/6875: current training loss = 0.005069\n",
      "2018-04-13 17:54:28 iteration 6250/6875: current training loss = 0.006360\n",
      "2018-04-13 17:54:31 iteration 6875/6875: current training loss = 0.002138\n",
      "2018-04-13 17:54:37 end epoch 22/50: acc_train=97.394% acc_val=96.800% acc_test=96.938%\n",
      "2018-04-13 17:54:37 start epoch 23/50:\n",
      "2018-04-13 17:54:37 iteration 1/6875: current training loss = 0.000332\n",
      "2018-04-13 17:54:40 iteration 625/6875: current training loss = 0.003318\n",
      "2018-04-13 17:54:42 iteration 1250/6875: current training loss = 0.012386\n",
      "2018-04-13 17:54:45 iteration 1875/6875: current training loss = 0.000335\n",
      "2018-04-13 17:54:47 iteration 2500/6875: current training loss = 0.000394\n",
      "2018-04-13 17:54:50 iteration 3125/6875: current training loss = 0.000568\n",
      "2018-04-13 17:54:52 iteration 3750/6875: current training loss = 0.008852\n",
      "2018-04-13 17:54:55 iteration 4375/6875: current training loss = 0.049465\n",
      "2018-04-13 17:54:58 iteration 5000/6875: current training loss = 0.001372\n",
      "2018-04-13 17:55:00 iteration 5625/6875: current training loss = 0.001513\n",
      "2018-04-13 17:55:03 iteration 6250/6875: current training loss = 0.001585\n",
      "2018-04-13 17:55:05 iteration 6875/6875: current training loss = 0.002464\n",
      "2018-04-13 17:55:11 end epoch 23/50: acc_train=97.600% acc_val=97.250% acc_test=97.013%\n",
      "2018-04-13 17:55:11 start epoch 24/50:\n",
      "2018-04-13 17:55:11 iteration 1/6875: current training loss = 0.000599\n",
      "2018-04-13 17:55:14 iteration 625/6875: current training loss = 0.023406\n",
      "2018-04-13 17:55:16 iteration 1250/6875: current training loss = 0.011233\n",
      "2018-04-13 17:55:19 iteration 1875/6875: current training loss = 0.000968\n",
      "2018-04-13 17:55:22 iteration 2500/6875: current training loss = 0.003464\n",
      "2018-04-13 17:55:24 iteration 3125/6875: current training loss = 0.000492\n",
      "2018-04-13 17:55:27 iteration 3750/6875: current training loss = 0.000402\n",
      "2018-04-13 17:55:29 iteration 4375/6875: current training loss = 0.025702\n",
      "2018-04-13 17:55:32 iteration 5000/6875: current training loss = 0.000101\n",
      "2018-04-13 17:55:34 iteration 5625/6875: current training loss = 0.010252\n",
      "2018-04-13 17:55:37 iteration 6250/6875: current training loss = 0.003196\n",
      "2018-04-13 17:55:40 iteration 6875/6875: current training loss = 0.001839\n",
      "2018-04-13 17:55:46 end epoch 24/50: acc_train=97.394% acc_val=96.975% acc_test=97.362%\n",
      "2018-04-13 17:55:46 start epoch 25/50:\n",
      "2018-04-13 17:55:46 iteration 1/6875: current training loss = 0.000358\n",
      "2018-04-13 17:55:49 iteration 625/6875: current training loss = 0.024328\n",
      "2018-04-13 17:55:51 iteration 1250/6875: current training loss = 0.000147\n",
      "2018-04-13 17:55:54 iteration 1875/6875: current training loss = 0.032593\n",
      "2018-04-13 17:55:56 iteration 2500/6875: current training loss = 0.000164\n",
      "2018-04-13 17:55:59 iteration 3125/6875: current training loss = 0.001951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 17:56:02 iteration 3750/6875: current training loss = 0.002358\n",
      "2018-04-13 17:56:04 iteration 4375/6875: current training loss = 0.001308\n",
      "2018-04-13 17:56:07 iteration 5000/6875: current training loss = 0.018116\n",
      "2018-04-13 17:56:09 iteration 5625/6875: current training loss = 0.000212\n",
      "2018-04-13 17:56:12 iteration 6250/6875: current training loss = 0.001722\n",
      "2018-04-13 17:56:14 iteration 6875/6875: current training loss = 0.004283\n",
      "2018-04-13 17:56:20 end epoch 25/50: acc_train=96.944% acc_val=96.500% acc_test=96.213%\n",
      "2018-04-13 17:56:20 start epoch 26/50:\n",
      "2018-04-13 17:56:20 iteration 1/6875: current training loss = 0.001250\n",
      "2018-04-13 17:56:23 iteration 625/6875: current training loss = 0.006684\n",
      "2018-04-13 17:56:25 iteration 1250/6875: current training loss = 0.009406\n",
      "2018-04-13 17:56:27 iteration 1875/6875: current training loss = 0.004368\n",
      "2018-04-13 17:56:30 iteration 2500/6875: current training loss = 0.031838\n",
      "2018-04-13 17:56:32 iteration 3125/6875: current training loss = 0.002353\n",
      "2018-04-13 17:56:35 iteration 3750/6875: current training loss = 0.000171\n",
      "2018-04-13 17:56:37 iteration 4375/6875: current training loss = 0.087560\n",
      "2018-04-13 17:56:40 iteration 5000/6875: current training loss = 0.002081\n",
      "2018-04-13 17:56:43 iteration 5625/6875: current training loss = 0.001272\n",
      "2018-04-13 17:56:45 iteration 6250/6875: current training loss = 0.001831\n",
      "2018-04-13 17:56:48 iteration 6875/6875: current training loss = 0.004054\n",
      "2018-04-13 17:56:54 end epoch 26/50: acc_train=97.219% acc_val=96.750% acc_test=96.575%\n",
      "2018-04-13 17:56:54 start epoch 27/50:\n",
      "2018-04-13 17:56:54 iteration 1/6875: current training loss = 0.001521\n",
      "2018-04-13 17:56:57 iteration 625/6875: current training loss = 0.001798\n",
      "2018-04-13 17:56:59 iteration 1250/6875: current training loss = 0.000454\n",
      "2018-04-13 17:57:02 iteration 1875/6875: current training loss = 0.005303\n",
      "2018-04-13 17:57:04 iteration 2500/6875: current training loss = 0.000576\n",
      "2018-04-13 17:57:07 iteration 3125/6875: current training loss = 0.001019\n",
      "2018-04-13 17:57:10 iteration 3750/6875: current training loss = 0.003452\n",
      "2018-04-13 17:57:13 iteration 4375/6875: current training loss = 0.002631\n",
      "2018-04-13 17:57:15 iteration 5000/6875: current training loss = 0.000351\n",
      "2018-04-13 17:57:18 iteration 5625/6875: current training loss = 0.092523\n",
      "2018-04-13 17:57:20 iteration 6250/6875: current training loss = 0.001359\n",
      "2018-04-13 17:57:23 iteration 6875/6875: current training loss = 0.001051\n",
      "2018-04-13 17:57:29 end epoch 27/50: acc_train=95.181% acc_val=95.500% acc_test=94.487%\n",
      "2018-04-13 17:57:29 start epoch 28/50:\n",
      "2018-04-13 17:57:29 iteration 1/6875: current training loss = 0.000530\n",
      "2018-04-13 17:57:32 iteration 625/6875: current training loss = 0.000433\n",
      "2018-04-13 17:57:35 iteration 1250/6875: current training loss = 0.003243\n",
      "2018-04-13 17:57:37 iteration 1875/6875: current training loss = 0.020376\n",
      "2018-04-13 17:57:40 iteration 2500/6875: current training loss = 0.000877\n",
      "2018-04-13 17:57:42 iteration 3125/6875: current training loss = 0.000796\n",
      "2018-04-13 17:57:45 iteration 3750/6875: current training loss = 0.000671\n",
      "2018-04-13 17:57:47 iteration 4375/6875: current training loss = 0.001357\n",
      "2018-04-13 17:57:50 iteration 5000/6875: current training loss = 0.007715\n",
      "2018-04-13 17:57:53 iteration 5625/6875: current training loss = 0.000639\n",
      "2018-04-13 17:57:55 iteration 6250/6875: current training loss = 0.000430\n",
      "2018-04-13 17:57:58 iteration 6875/6875: current training loss = 0.000923\n",
      "2018-04-13 17:58:04 end epoch 28/50: acc_train=96.931% acc_val=96.325% acc_test=96.500%\n",
      "2018-04-13 17:58:04 start epoch 29/50:\n",
      "2018-04-13 17:58:04 iteration 1/6875: current training loss = 0.000986\n",
      "2018-04-13 17:58:07 iteration 625/6875: current training loss = 0.002563\n",
      "2018-04-13 17:58:09 iteration 1250/6875: current training loss = 0.000247\n",
      "2018-04-13 17:58:12 iteration 1875/6875: current training loss = 0.001233\n",
      "2018-04-13 17:58:14 iteration 2500/6875: current training loss = 0.000360\n",
      "2018-04-13 17:58:17 iteration 3125/6875: current training loss = 0.001435\n",
      "2018-04-13 17:58:19 iteration 3750/6875: current training loss = 0.008362\n",
      "2018-04-13 17:58:22 iteration 4375/6875: current training loss = 0.021758\n",
      "2018-04-13 17:58:25 iteration 5000/6875: current training loss = 0.001805\n",
      "2018-04-13 17:58:27 iteration 5625/6875: current training loss = 0.009583\n",
      "2018-04-13 17:58:30 iteration 6250/6875: current training loss = 0.000967\n",
      "2018-04-13 17:58:32 iteration 6875/6875: current training loss = 0.008290\n",
      "2018-04-13 17:58:39 end epoch 29/50: acc_train=95.737% acc_val=95.275% acc_test=95.188%\n",
      "2018-04-13 17:58:39 start epoch 30/50:\n",
      "2018-04-13 17:58:39 iteration 1/6875: current training loss = 0.002934\n",
      "2018-04-13 17:58:41 iteration 625/6875: current training loss = 0.011366\n",
      "2018-04-13 17:58:44 iteration 1250/6875: current training loss = 0.001042\n",
      "2018-04-13 17:58:47 iteration 1875/6875: current training loss = 0.004795\n",
      "2018-04-13 17:58:49 iteration 2500/6875: current training loss = 0.041807\n",
      "2018-04-13 17:58:52 iteration 3125/6875: current training loss = 0.003300\n",
      "2018-04-13 17:58:54 iteration 3750/6875: current training loss = 0.000392\n",
      "2018-04-13 17:58:57 iteration 4375/6875: current training loss = 0.000475\n",
      "2018-04-13 17:59:00 iteration 5000/6875: current training loss = 0.001831\n",
      "2018-04-13 17:59:02 iteration 5625/6875: current training loss = 0.000375\n",
      "2018-04-13 17:59:05 iteration 6250/6875: current training loss = 0.006230\n",
      "2018-04-13 17:59:07 iteration 6875/6875: current training loss = 0.003964\n",
      "2018-04-13 17:59:14 end epoch 30/50: acc_train=96.319% acc_val=96.000% acc_test=95.462%\n",
      "2018-04-13 17:59:14 start epoch 31/50:\n",
      "2018-04-13 17:59:14 iteration 1/6875: current training loss = 0.000342\n",
      "2018-04-13 17:59:17 iteration 625/6875: current training loss = 0.000626\n",
      "2018-04-13 17:59:19 iteration 1250/6875: current training loss = 0.005007\n",
      "2018-04-13 17:59:22 iteration 1875/6875: current training loss = 0.024653\n",
      "2018-04-13 17:59:24 iteration 2500/6875: current training loss = 0.001893\n",
      "2018-04-13 17:59:27 iteration 3125/6875: current training loss = 0.000423\n",
      "2018-04-13 17:59:30 iteration 3750/6875: current training loss = 0.000677\n",
      "2018-04-13 17:59:32 iteration 4375/6875: current training loss = 0.001099\n",
      "2018-04-13 17:59:35 iteration 5000/6875: current training loss = 0.002488\n",
      "2018-04-13 17:59:38 iteration 5625/6875: current training loss = 0.024694\n",
      "2018-04-13 17:59:40 iteration 6250/6875: current training loss = 0.009055\n",
      "2018-04-13 17:59:43 iteration 6875/6875: current training loss = 0.000134\n",
      "2018-04-13 17:59:49 end epoch 31/50: acc_train=95.800% acc_val=95.425% acc_test=95.138%\n",
      "2018-04-13 17:59:49 start epoch 32/50:\n",
      "2018-04-13 17:59:49 iteration 1/6875: current training loss = 0.000296\n",
      "2018-04-13 17:59:52 iteration 625/6875: current training loss = 0.022477\n",
      "2018-04-13 17:59:54 iteration 1250/6875: current training loss = 0.000145\n",
      "2018-04-13 17:59:57 iteration 1875/6875: current training loss = 0.000185\n",
      "2018-04-13 17:59:59 iteration 2500/6875: current training loss = 0.042568\n",
      "2018-04-13 18:00:02 iteration 3125/6875: current training loss = 0.000052\n",
      "2018-04-13 18:00:04 iteration 3750/6875: current training loss = 0.000410\n",
      "2018-04-13 18:00:07 iteration 4375/6875: current training loss = 0.000287\n",
      "2018-04-13 18:00:09 iteration 5000/6875: current training loss = 0.000227\n",
      "2018-04-13 18:00:12 iteration 5625/6875: current training loss = 0.000812\n",
      "2018-04-13 18:00:14 iteration 6250/6875: current training loss = 0.000082\n",
      "2018-04-13 18:00:17 iteration 6875/6875: current training loss = 0.000104\n",
      "2018-04-13 18:00:23 end epoch 32/50: acc_train=96.375% acc_val=96.325% acc_test=95.950%\n",
      "2018-04-13 18:00:23 start epoch 33/50:\n",
      "2018-04-13 18:00:23 iteration 1/6875: current training loss = 0.002804\n",
      "2018-04-13 18:00:26 iteration 625/6875: current training loss = 0.000297\n",
      "2018-04-13 18:00:28 iteration 1250/6875: current training loss = 0.025139\n",
      "2018-04-13 18:00:31 iteration 1875/6875: current training loss = 0.000328\n",
      "2018-04-13 18:00:33 iteration 2500/6875: current training loss = 0.001096\n",
      "2018-04-13 18:00:36 iteration 3125/6875: current training loss = 0.000906\n",
      "2018-04-13 18:00:38 iteration 3750/6875: current training loss = 0.002879\n",
      "2018-04-13 18:00:41 iteration 4375/6875: current training loss = 0.022065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 18:00:44 iteration 5000/6875: current training loss = 0.008785\n",
      "2018-04-13 18:00:46 iteration 5625/6875: current training loss = 0.000127\n",
      "2018-04-13 18:00:49 iteration 6250/6875: current training loss = 0.006951\n",
      "2018-04-13 18:00:51 iteration 6875/6875: current training loss = 0.000772\n",
      "2018-04-13 18:00:58 end epoch 33/50: acc_train=96.438% acc_val=95.675% acc_test=95.425%\n",
      "2018-04-13 18:00:58 start epoch 34/50:\n",
      "2018-04-13 18:00:58 iteration 1/6875: current training loss = 0.000730\n",
      "2018-04-13 18:01:00 iteration 625/6875: current training loss = 0.005548\n",
      "2018-04-13 18:01:03 iteration 1250/6875: current training loss = 0.005305\n",
      "2018-04-13 18:01:05 iteration 1875/6875: current training loss = 0.000722\n",
      "2018-04-13 18:01:08 iteration 2500/6875: current training loss = 0.000363\n",
      "2018-04-13 18:01:10 iteration 3125/6875: current training loss = 0.000097\n",
      "2018-04-13 18:01:13 iteration 3750/6875: current training loss = 0.005292\n",
      "2018-04-13 18:01:15 iteration 4375/6875: current training loss = 0.001585\n",
      "2018-04-13 18:01:18 iteration 5000/6875: current training loss = 0.004407\n",
      "2018-04-13 18:01:20 iteration 5625/6875: current training loss = 0.012289\n",
      "2018-04-13 18:01:23 iteration 6250/6875: current training loss = 0.005902\n",
      "2018-04-13 18:01:26 iteration 6875/6875: current training loss = 0.093182\n",
      "2018-04-13 18:01:32 end epoch 34/50: acc_train=95.806% acc_val=95.725% acc_test=95.213%\n",
      "2018-04-13 18:01:32 start epoch 35/50:\n",
      "2018-04-13 18:01:32 iteration 1/6875: current training loss = 0.000258\n",
      "2018-04-13 18:01:34 iteration 625/6875: current training loss = 0.018614\n",
      "2018-04-13 18:01:37 iteration 1250/6875: current training loss = 0.002708\n",
      "2018-04-13 18:01:39 iteration 1875/6875: current training loss = 0.007914\n",
      "2018-04-13 18:01:42 iteration 2500/6875: current training loss = 0.000103\n",
      "2018-04-13 18:01:45 iteration 3125/6875: current training loss = 0.000733\n",
      "2018-04-13 18:01:47 iteration 3750/6875: current training loss = 0.006734\n",
      "2018-04-13 18:01:50 iteration 4375/6875: current training loss = 0.000098\n",
      "2018-04-13 18:01:52 iteration 5000/6875: current training loss = 0.000744\n",
      "2018-04-13 18:01:55 iteration 5625/6875: current training loss = 0.000300\n",
      "2018-04-13 18:01:58 iteration 6250/6875: current training loss = 0.003891\n",
      "2018-04-13 18:02:00 iteration 6875/6875: current training loss = 0.000072\n",
      "2018-04-13 18:02:07 end epoch 35/50: acc_train=95.206% acc_val=94.625% acc_test=94.750%\n",
      "2018-04-13 18:02:07 start epoch 36/50:\n",
      "2018-04-13 18:02:07 iteration 1/6875: current training loss = 0.000130\n",
      "2018-04-13 18:02:09 iteration 625/6875: current training loss = 0.000488\n",
      "2018-04-13 18:02:12 iteration 1250/6875: current training loss = 0.000941\n",
      "2018-04-13 18:02:14 iteration 1875/6875: current training loss = 0.001211\n",
      "2018-04-13 18:02:17 iteration 2500/6875: current training loss = 0.003897\n",
      "2018-04-13 18:02:19 iteration 3125/6875: current training loss = 0.000058\n",
      "2018-04-13 18:02:22 iteration 3750/6875: current training loss = 0.014161\n",
      "2018-04-13 18:02:24 iteration 4375/6875: current training loss = 0.002743\n",
      "2018-04-13 18:02:27 iteration 5000/6875: current training loss = 0.001345\n",
      "2018-04-13 18:02:29 iteration 5625/6875: current training loss = 0.000298\n",
      "2018-04-13 18:02:32 iteration 6250/6875: current training loss = 0.002494\n",
      "2018-04-13 18:02:35 iteration 6875/6875: current training loss = 0.004642\n",
      "2018-04-13 18:02:41 end epoch 36/50: acc_train=93.844% acc_val=93.625% acc_test=93.062%\n",
      "2018-04-13 18:02:41 start epoch 37/50:\n",
      "2018-04-13 18:02:41 iteration 1/6875: current training loss = 0.015960\n",
      "2018-04-13 18:02:43 iteration 625/6875: current training loss = 0.000886\n",
      "2018-04-13 18:02:46 iteration 1250/6875: current training loss = 0.001022\n",
      "2018-04-13 18:02:48 iteration 1875/6875: current training loss = 0.003402\n",
      "2018-04-13 18:02:51 iteration 2500/6875: current training loss = 0.001137\n",
      "2018-04-13 18:02:53 iteration 3125/6875: current training loss = 0.007408\n",
      "2018-04-13 18:02:56 iteration 3750/6875: current training loss = 0.000935\n",
      "2018-04-13 18:02:59 iteration 4375/6875: current training loss = 0.000152\n",
      "2018-04-13 18:03:01 iteration 5000/6875: current training loss = 0.001342\n",
      "2018-04-13 18:03:04 iteration 5625/6875: current training loss = 0.002146\n",
      "2018-04-13 18:03:07 iteration 6250/6875: current training loss = 0.000045\n",
      "2018-04-13 18:03:09 iteration 6875/6875: current training loss = 0.002957\n",
      "2018-04-13 18:03:16 end epoch 37/50: acc_train=95.556% acc_val=95.025% acc_test=94.912%\n",
      "2018-04-13 18:03:16 start epoch 38/50:\n",
      "2018-04-13 18:03:16 iteration 1/6875: current training loss = 0.008532\n",
      "2018-04-13 18:03:18 iteration 625/6875: current training loss = 0.001224\n",
      "2018-04-13 18:03:21 iteration 1250/6875: current training loss = 0.000136\n",
      "2018-04-13 18:03:23 iteration 1875/6875: current training loss = 0.000088\n",
      "2018-04-13 18:03:26 iteration 2500/6875: current training loss = 0.005298\n",
      "2018-04-13 18:03:29 iteration 3125/6875: current training loss = 0.014792\n",
      "2018-04-13 18:03:31 iteration 3750/6875: current training loss = 0.000407\n",
      "2018-04-13 18:03:34 iteration 4375/6875: current training loss = 0.000533\n",
      "2018-04-13 18:03:36 iteration 5000/6875: current training loss = 0.000379\n",
      "2018-04-13 18:03:39 iteration 5625/6875: current training loss = 0.000495\n",
      "2018-04-13 18:03:41 iteration 6250/6875: current training loss = 0.000077\n",
      "2018-04-13 18:03:44 iteration 6875/6875: current training loss = 0.000315\n",
      "2018-04-13 18:03:50 end epoch 38/50: acc_train=94.775% acc_val=94.275% acc_test=93.975%\n",
      "2018-04-13 18:03:50 start epoch 39/50:\n",
      "2018-04-13 18:03:50 iteration 1/6875: current training loss = 0.000066\n",
      "2018-04-13 18:03:52 iteration 625/6875: current training loss = 0.000734\n",
      "2018-04-13 18:03:55 iteration 1250/6875: current training loss = 0.000819\n",
      "2018-04-13 18:03:57 iteration 1875/6875: current training loss = 0.000183\n",
      "2018-04-13 18:04:00 iteration 2500/6875: current training loss = 0.000046\n",
      "2018-04-13 18:04:02 iteration 3125/6875: current training loss = 0.000938\n",
      "2018-04-13 18:04:05 iteration 3750/6875: current training loss = 0.001011\n",
      "2018-04-13 18:04:07 iteration 4375/6875: current training loss = 0.000004\n",
      "2018-04-13 18:04:10 iteration 5000/6875: current training loss = 0.000371\n",
      "2018-04-13 18:04:12 iteration 5625/6875: current training loss = 0.001759\n",
      "2018-04-13 18:04:15 iteration 6250/6875: current training loss = 0.001699\n",
      "2018-04-13 18:04:18 iteration 6875/6875: current training loss = 0.002182\n",
      "2018-04-13 18:04:24 end epoch 39/50: acc_train=95.388% acc_val=94.950% acc_test=94.575%\n",
      "2018-04-13 18:04:24 start epoch 40/50:\n",
      "2018-04-13 18:04:24 iteration 1/6875: current training loss = 0.000341\n",
      "2018-04-13 18:04:26 iteration 625/6875: current training loss = 0.002435\n",
      "2018-04-13 18:04:29 iteration 1250/6875: current training loss = 0.000013\n",
      "2018-04-13 18:04:31 iteration 1875/6875: current training loss = 0.001171\n",
      "2018-04-13 18:04:34 iteration 2500/6875: current training loss = 0.010791\n",
      "2018-04-13 18:04:36 iteration 3125/6875: current training loss = 0.001968\n",
      "2018-04-13 18:04:39 iteration 3750/6875: current training loss = 0.000156\n",
      "2018-04-13 18:04:41 iteration 4375/6875: current training loss = 0.003757\n",
      "2018-04-13 18:04:44 iteration 5000/6875: current training loss = 0.000280\n",
      "2018-04-13 18:04:47 iteration 5625/6875: current training loss = 0.002948\n",
      "2018-04-13 18:04:49 iteration 6250/6875: current training loss = 0.001857\n",
      "2018-04-13 18:04:52 iteration 6875/6875: current training loss = 0.001574\n",
      "2018-04-13 18:04:58 end epoch 40/50: acc_train=95.713% acc_val=94.975% acc_test=94.550%\n",
      "2018-04-13 18:04:58 start epoch 41/50:\n",
      "2018-04-13 18:04:58 iteration 1/6875: current training loss = 0.000590\n",
      "2018-04-13 18:05:01 iteration 625/6875: current training loss = 0.005180\n",
      "2018-04-13 18:05:04 iteration 1250/6875: current training loss = 0.001766\n",
      "2018-04-13 18:05:06 iteration 1875/6875: current training loss = 0.001464\n",
      "2018-04-13 18:05:09 iteration 2500/6875: current training loss = 0.000176\n",
      "2018-04-13 18:05:11 iteration 3125/6875: current training loss = 0.000254\n",
      "2018-04-13 18:05:14 iteration 3750/6875: current training loss = 0.001397\n",
      "2018-04-13 18:05:16 iteration 4375/6875: current training loss = 0.000742\n",
      "2018-04-13 18:05:19 iteration 5000/6875: current training loss = 0.000370\n",
      "2018-04-13 18:05:21 iteration 5625/6875: current training loss = 0.001630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 18:05:24 iteration 6250/6875: current training loss = 0.002048\n",
      "2018-04-13 18:05:27 iteration 6875/6875: current training loss = 0.000028\n",
      "2018-04-13 18:05:33 end epoch 41/50: acc_train=94.506% acc_val=94.200% acc_test=93.950%\n",
      "2018-04-13 18:05:33 start epoch 42/50:\n",
      "2018-04-13 18:05:33 iteration 1/6875: current training loss = 0.000561\n",
      "2018-04-13 18:05:36 iteration 625/6875: current training loss = 0.001908\n",
      "2018-04-13 18:05:38 iteration 1250/6875: current training loss = 0.000213\n",
      "2018-04-13 18:05:41 iteration 1875/6875: current training loss = 0.000894\n",
      "2018-04-13 18:05:43 iteration 2500/6875: current training loss = 0.000159\n",
      "2018-04-13 18:05:46 iteration 3125/6875: current training loss = 0.000070\n",
      "2018-04-13 18:05:48 iteration 3750/6875: current training loss = 0.001606\n",
      "2018-04-13 18:05:51 iteration 4375/6875: current training loss = 0.000170\n",
      "2018-04-13 18:05:54 iteration 5000/6875: current training loss = 0.001890\n",
      "2018-04-13 18:05:56 iteration 5625/6875: current training loss = 0.001980\n",
      "2018-04-13 18:05:59 iteration 6250/6875: current training loss = 0.003466\n",
      "2018-04-13 18:06:01 iteration 6875/6875: current training loss = 0.001799\n",
      "2018-04-13 18:06:08 end epoch 42/50: acc_train=96.325% acc_val=95.850% acc_test=95.325%\n",
      "2018-04-13 18:06:08 start epoch 43/50:\n",
      "2018-04-13 18:06:08 iteration 1/6875: current training loss = 0.000580\n",
      "2018-04-13 18:06:10 iteration 625/6875: current training loss = 0.001695\n",
      "2018-04-13 18:06:12 iteration 1250/6875: current training loss = 0.020348\n",
      "2018-04-13 18:06:15 iteration 1875/6875: current training loss = 0.000074\n",
      "2018-04-13 18:06:18 iteration 2500/6875: current training loss = 0.000176\n",
      "2018-04-13 18:06:20 iteration 3125/6875: current training loss = 0.001775\n",
      "2018-04-13 18:06:23 iteration 3750/6875: current training loss = 0.001595\n",
      "2018-04-13 18:06:25 iteration 4375/6875: current training loss = 0.000204\n",
      "2018-04-13 18:06:28 iteration 5000/6875: current training loss = 0.000051\n",
      "2018-04-13 18:06:30 iteration 5625/6875: current training loss = 0.000728\n",
      "2018-04-13 18:06:33 iteration 6250/6875: current training loss = 0.000322\n",
      "2018-04-13 18:06:35 iteration 6875/6875: current training loss = 0.000505\n",
      "2018-04-13 18:06:41 end epoch 43/50: acc_train=94.363% acc_val=93.725% acc_test=93.375%\n",
      "2018-04-13 18:06:41 start epoch 44/50:\n",
      "2018-04-13 18:06:41 iteration 1/6875: current training loss = 0.000035\n",
      "2018-04-13 18:06:44 iteration 625/6875: current training loss = 0.007780\n",
      "2018-04-13 18:06:46 iteration 1250/6875: current training loss = 0.000219\n",
      "2018-04-13 18:06:49 iteration 1875/6875: current training loss = 0.000241\n",
      "2018-04-13 18:06:51 iteration 2500/6875: current training loss = 0.000772\n",
      "2018-04-13 18:06:54 iteration 3125/6875: current training loss = 0.002565\n",
      "2018-04-13 18:06:57 iteration 3750/6875: current training loss = 0.000420\n",
      "2018-04-13 18:06:59 iteration 4375/6875: current training loss = 0.007788\n",
      "2018-04-13 18:07:02 iteration 5000/6875: current training loss = 0.000222\n",
      "2018-04-13 18:07:05 iteration 5625/6875: current training loss = 0.003958\n",
      "2018-04-13 18:07:07 iteration 6250/6875: current training loss = 0.000066\n",
      "2018-04-13 18:07:10 iteration 6875/6875: current training loss = 0.019382\n",
      "2018-04-13 18:07:16 end epoch 44/50: acc_train=93.537% acc_val=93.225% acc_test=93.137%\n",
      "2018-04-13 18:07:16 start epoch 45/50:\n",
      "2018-04-13 18:07:16 iteration 1/6875: current training loss = 0.021683\n",
      "2018-04-13 18:07:18 iteration 625/6875: current training loss = 0.000305\n",
      "2018-04-13 18:07:20 iteration 1250/6875: current training loss = 0.000016\n",
      "2018-04-13 18:07:23 iteration 1875/6875: current training loss = 0.000047\n",
      "2018-04-13 18:07:26 iteration 2500/6875: current training loss = 0.000219\n",
      "2018-04-13 18:07:28 iteration 3125/6875: current training loss = 0.003335\n",
      "2018-04-13 18:07:31 iteration 3750/6875: current training loss = 0.000009\n",
      "2018-04-13 18:07:34 iteration 4375/6875: current training loss = 0.000036\n",
      "2018-04-13 18:07:36 iteration 5000/6875: current training loss = 0.000568\n",
      "2018-04-13 18:07:39 iteration 5625/6875: current training loss = 0.000669\n",
      "2018-04-13 18:07:41 iteration 6250/6875: current training loss = 0.000418\n",
      "2018-04-13 18:07:44 iteration 6875/6875: current training loss = 0.000111\n",
      "2018-04-13 18:07:50 end epoch 45/50: acc_train=93.775% acc_val=93.600% acc_test=93.112%\n",
      "2018-04-13 18:07:50 start epoch 46/50:\n",
      "2018-04-13 18:07:50 iteration 1/6875: current training loss = 0.000075\n",
      "2018-04-13 18:07:53 iteration 625/6875: current training loss = 0.000012\n",
      "2018-04-13 18:07:55 iteration 1250/6875: current training loss = 0.000106\n",
      "2018-04-13 18:07:58 iteration 1875/6875: current training loss = 0.000217\n",
      "2018-04-13 18:08:01 iteration 2500/6875: current training loss = 0.001014\n",
      "2018-04-13 18:08:03 iteration 3125/6875: current training loss = 0.000036\n",
      "2018-04-13 18:08:06 iteration 3750/6875: current training loss = 0.000095\n",
      "2018-04-13 18:08:08 iteration 4375/6875: current training loss = 0.010279\n",
      "2018-04-13 18:08:11 iteration 5000/6875: current training loss = 0.000019\n",
      "2018-04-13 18:08:13 iteration 5625/6875: current training loss = 0.000173\n",
      "2018-04-13 18:08:16 iteration 6250/6875: current training loss = 0.000131\n",
      "2018-04-13 18:08:19 iteration 6875/6875: current training loss = 0.006737\n",
      "2018-04-13 18:08:25 end epoch 46/50: acc_train=91.731% acc_val=91.850% acc_test=91.212%\n",
      "2018-04-13 18:08:25 start epoch 47/50:\n",
      "2018-04-13 18:08:25 iteration 1/6875: current training loss = 0.000565\n",
      "2018-04-13 18:08:28 iteration 625/6875: current training loss = 0.001283\n",
      "2018-04-13 18:08:30 iteration 1250/6875: current training loss = 0.004593\n",
      "2018-04-13 18:08:33 iteration 1875/6875: current training loss = 0.000582\n",
      "2018-04-13 18:08:35 iteration 2500/6875: current training loss = 0.000052\n",
      "2018-04-13 18:08:38 iteration 3125/6875: current training loss = 0.003019\n",
      "2018-04-13 18:08:40 iteration 3750/6875: current training loss = 0.008986\n",
      "2018-04-13 18:08:43 iteration 4375/6875: current training loss = 0.000108\n",
      "2018-04-13 18:08:46 iteration 5000/6875: current training loss = 0.000009\n",
      "2018-04-13 18:08:48 iteration 5625/6875: current training loss = 0.000051\n",
      "2018-04-13 18:08:51 iteration 6250/6875: current training loss = 0.000848\n",
      "2018-04-13 18:08:53 iteration 6875/6875: current training loss = 0.000812\n",
      "2018-04-13 18:09:00 end epoch 47/50: acc_train=93.050% acc_val=92.625% acc_test=92.375%\n",
      "2018-04-13 18:09:00 start epoch 48/50:\n",
      "2018-04-13 18:09:00 iteration 1/6875: current training loss = 0.000145\n",
      "2018-04-13 18:09:02 iteration 625/6875: current training loss = 0.000022\n",
      "2018-04-13 18:09:05 iteration 1250/6875: current training loss = 0.000063\n",
      "2018-04-13 18:09:07 iteration 1875/6875: current training loss = 0.002697\n",
      "2018-04-13 18:09:10 iteration 2500/6875: current training loss = 0.004023\n",
      "2018-04-13 18:09:13 iteration 3125/6875: current training loss = 0.000032\n",
      "2018-04-13 18:09:15 iteration 3750/6875: current training loss = 0.000010\n",
      "2018-04-13 18:09:18 iteration 4375/6875: current training loss = 0.001173\n",
      "2018-04-13 18:09:20 iteration 5000/6875: current training loss = 0.000107\n",
      "2018-04-13 18:09:23 iteration 5625/6875: current training loss = 0.000564\n",
      "2018-04-13 18:09:26 iteration 6250/6875: current training loss = 0.000002\n",
      "2018-04-13 18:09:28 iteration 6875/6875: current training loss = 0.000005\n",
      "2018-04-13 18:09:35 end epoch 48/50: acc_train=93.181% acc_val=93.675% acc_test=92.900%\n",
      "2018-04-13 18:09:35 start epoch 49/50:\n",
      "2018-04-13 18:09:35 iteration 1/6875: current training loss = 0.017196\n",
      "2018-04-13 18:09:37 iteration 625/6875: current training loss = 0.000216\n",
      "2018-04-13 18:09:40 iteration 1250/6875: current training loss = 0.003120\n",
      "2018-04-13 18:09:43 iteration 1875/6875: current training loss = 0.001395\n",
      "2018-04-13 18:09:45 iteration 2500/6875: current training loss = 0.000249\n",
      "2018-04-13 18:09:48 iteration 3125/6875: current training loss = 0.005081\n",
      "2018-04-13 18:09:50 iteration 3750/6875: current training loss = 0.001075\n",
      "2018-04-13 18:09:53 iteration 4375/6875: current training loss = 0.000176\n",
      "2018-04-13 18:09:56 iteration 5000/6875: current training loss = 0.000634\n",
      "2018-04-13 18:09:58 iteration 5625/6875: current training loss = 0.002375\n",
      "2018-04-13 18:10:01 iteration 6250/6875: current training loss = 0.000099\n",
      "2018-04-13 18:10:03 iteration 6875/6875: current training loss = 0.000184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13 18:10:10 end epoch 49/50: acc_train=94.388% acc_val=94.450% acc_test=93.362%\n",
      "2018-04-13 18:10:10 start epoch 50/50:\n",
      "2018-04-13 18:10:10 iteration 1/6875: current training loss = 0.000009\n",
      "2018-04-13 18:10:12 iteration 625/6875: current training loss = 0.001656\n",
      "2018-04-13 18:10:15 iteration 1250/6875: current training loss = 0.000122\n",
      "2018-04-13 18:10:18 iteration 1875/6875: current training loss = 0.000226\n",
      "2018-04-13 18:10:20 iteration 2500/6875: current training loss = 0.000012\n",
      "2018-04-13 18:10:23 iteration 3125/6875: current training loss = 0.001295\n",
      "2018-04-13 18:10:25 iteration 3750/6875: current training loss = 0.000003\n",
      "2018-04-13 18:10:28 iteration 4375/6875: current training loss = 0.000023\n",
      "2018-04-13 18:10:30 iteration 5000/6875: current training loss = 0.000154\n",
      "2018-04-13 18:10:33 iteration 5625/6875: current training loss = 0.002574\n",
      "2018-04-13 18:10:36 iteration 6250/6875: current training loss = 0.025145\n",
      "2018-04-13 18:10:38 iteration 6875/6875: current training loss = 0.004937\n",
      "2018-04-13 18:10:44 end epoch 50/50: acc_train=93.169% acc_val=93.000% acc_test=92.787%\n"
     ]
    }
   ],
   "source": [
    "max_epoch=50\n",
    "batch_size=8\n",
    "print_every=625\n",
    "\n",
    "def train():\n",
    "    num_iteration=num_train//batch_size\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=mnist.train.next_batch(batch_size)\n",
    "        loss_num,_ = sess.run([loss,train_step],feed_dict={X:images.reshape(-1,28,28,1),y:labels,is_training:True})\n",
    "        if it==0 or (it+1)%print_every==0 or it==num_iteration-1:\n",
    "            print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "                  'iteration %d/%d:' % (it+1,num_iteration),'current training loss = %f' % (loss_num))\n",
    "            \n",
    "def eval(dataset,num_iteration):\n",
    "    total_loss=0\n",
    "    total_accuracy=0\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=dataset.next_batch(batch_size)\n",
    "        loss_num,accuracy_num = sess.run([loss,accuracy],feed_dict={X:images.reshape(-1,28,28,1),y:labels,is_training:False})\n",
    "        total_loss+=loss_num\n",
    "        total_accuracy+=accuracy_num\n",
    "    total_loss/=num_iteration\n",
    "    total_accuracy/=num_iteration\n",
    "    return total_loss,total_accuracy\n",
    "    \n",
    "acc_train_his=[]\n",
    "acc_val_his=[]\n",
    "acc_test_his=[]\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(max_epoch):\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'start epoch %d/%d:' % (epoch+1,max_epoch))\n",
    "        train()\n",
    "        loss_train,acc_train=eval(mnist.train,2000)\n",
    "        loss_val,acc_val=eval(mnist.validation,500)\n",
    "        loss_test,acc_test=eval(mnist.test,1000)\n",
    "        acc_train_his.append(acc_train)\n",
    "        acc_val_his.append(acc_val)\n",
    "        acc_test_his.append(acc_test)\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'end epoch %d/%d:' % (epoch+1,max_epoch),\n",
    "             'acc_train=%.3f%% acc_val=%.3f%% acc_test=%.3f%%' % (acc_train*100.0,acc_val*100.0,acc_test*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "ptr,=plt.plot(range(max_epoch),acc_train_his,'r-')\n",
    "pva,=plt.plot(range(max_epoch),acc_val_his,'b-')\n",
    "pte,=plt.plot(range(max_epoch),acc_test_his,'y-')\n",
    "plt.xlabel('training epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('accuracy on three sets')\n",
    "plt.legend((ptr,pva,pte),('train','validation','test'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
