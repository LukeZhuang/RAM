{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"mnist/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train=mnist.train.num_examples\n",
    "num_val=mnist.validation.num_examples\n",
    "num_test=mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size=64\n",
    "img_size=28\n",
    "sensor_unit=256\n",
    "lstm_size=256\n",
    "N_glimpse=6\n",
    "MC_test=128\n",
    "loc_std=0.5\n",
    "tot_size=batch_size*MC_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glimpse_Network():\n",
    "    def __init__(self):\n",
    "        self.glimspe_size=[5,10,15]\n",
    "        self.concat_size=5\n",
    "        self.img_net=tf.layers.Dense(units=sensor_unit,name='glimpse_net/img_net')\n",
    "        self.loc_net=tf.layers.Dense(units=sensor_unit,name='glimpse_net/loc_net')\n",
    "        \n",
    "    def glimpse_sensor(self,image,loc):\n",
    "        glimpses_list=[tf.image.extract_glimpse(input=image,size=[gs,gs],offsets=loc) for gs in self.glimspe_size]\n",
    "        glimpses_norm=[tf.image.resize_bilinear(g,[self.concat_size,self.concat_size]) for g in glimpses_list]\n",
    "        glimpses=tf.concat(values=glimpses_norm,axis=3)  # batch_size*concat_size*concat_size*3\n",
    "        return glimpses\n",
    "    \n",
    "    def forward(self,image,loc):\n",
    "        glimpses=self.glimpse_sensor(image,loc) # tot_size*concat_size*concat_size*3\n",
    "        glimpses=tf.stop_gradient(glimpses)  # gradient has no need to flow through glimpses\n",
    "        g_image=tf.nn.relu(self.img_net(inputs=tf.layers.flatten(glimpses)))\n",
    "        g_loc=tf.nn.relu(self.loc_net(inputs=loc))\n",
    "        g_out=tf.nn.relu(g_image+g_loc)\n",
    "        return g_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(dtype=tf.float32,shape=[None,28,28,1])\n",
    "y=tf.placeholder(dtype=tf.int64,shape=[None,10])\n",
    "this_size=tf.shape(X)[0]\n",
    "start_location=tf.random_uniform(shape=[this_size,2],minval=-1.0,maxval=1.0)\n",
    "gNet=Glimpse_Network()\n",
    "\n",
    "lstm_cell = tf.contrib.rnn.LSTMCell(lstm_size)\n",
    "state = lstm_cell.zero_state(this_size, tf.float32)\n",
    "\n",
    "# emission_net_low=tf.layers.Dense(units=128,name='emission_net_low')\n",
    "emission_net_high=tf.layers.Dense(units=2,name='emission_net_high')\n",
    "baseline_net_low=tf.layers.Dense(units=128,name='baseline_net_low')\n",
    "baseline_net_mid=tf.layers.Dense(units=128,name='baseline_net_mid')\n",
    "baseline_net_high=tf.layers.Dense(units=1,name='baseline_net_high')\n",
    "predict_net=tf.layers.Dense(units=10,name='predict_net')\n",
    "\n",
    "def loglikelihood(sample,mean):\n",
    "    gaussian=tf.distributions.Normal(loc=mean,scale=tf.constant([loc_std,loc_std]))\n",
    "    llh=-gaussian.log_prob(sample)\n",
    "    return tf.reduce_sum(llh,axis=1)\n",
    "    \n",
    "loc_his=[]\n",
    "loglikelihood_his=[]\n",
    "baseline_his=[]\n",
    "normalized_loc=start_location\n",
    "for ng in range(N_glimpse):\n",
    "    loc_his.append(normalized_loc)\n",
    "    \n",
    "    # extract glimpse\n",
    "    glimpses_out=gNet.forward(X,normalized_loc)\n",
    "    \n",
    "    # RNN\n",
    "    lstm_output,state=lstm_cell(glimpses_out,state)\n",
    "    \n",
    "    # emit mean of location\n",
    "#     loc_mean=emission_net_high(tf.nn.relu(emission_net_low(lstm_output)))\n",
    "    loc_mean=emission_net_high(lstm_output)\n",
    "    \n",
    "    # sample next location by gaussian distribution centered at loc_mean\n",
    "    loc_sample=tf.random_normal(shape=(this_size,2),mean=loc_mean,stddev=loc_std)\n",
    "    loc_sample=tf.stop_gradient(loc_sample)  # very important ***\n",
    "    \n",
    "    # calculate the -loglikelihood of the sampled position\n",
    "    llh=loglikelihood(loc_sample,loc_mean)\n",
    "    loglikelihood_his.append(llh)\n",
    "    \n",
    "    # normalize the location for next input\n",
    "    normalized_loc=tf.tanh(loc_sample)\n",
    "#     normalized_loc=tf.clip_by_value(normalized_loc,-1.0,1.0)\n",
    "    normalized_loc=tf.stop_gradient(normalized_loc)\n",
    "    \n",
    "    # output deep accurate baseline(value) network\n",
    "    baseline=baseline_net_high(tf.nn.relu(baseline_net_mid(tf.nn.relu(baseline_net_low(lstm_output)))))\n",
    "#     baseline=baseline_net_high(tf.nn.relu(baseline_net_low(lstm_output)))\n",
    "#     baseline=baseline_net_high(lstm_output)\n",
    "    baseline_his.append(tf.squeeze(baseline))\n",
    "\n",
    "# pack data for calculation\n",
    "baseline_his=tf.stack(baseline_his)\n",
    "loglikelihood_his=tf.stack(loglikelihood_his)\n",
    "reduce_llh=tf.reduce_mean(loglikelihood_his)\n",
    "\n",
    "# make prediction\n",
    "score=predict_net(inputs=lstm_output)\n",
    "prediction=tf.argmax(score,1)\n",
    "\n",
    "# calculate reward, do variance reduction and calculate reinforced loglikelihood\n",
    "reward=tf.cast(tf.equal(prediction,tf.argmax(y,1)),dtype=tf.float32)\n",
    "# stop gradient on reward(redundant because tf.equal does not have gradient)\n",
    "reward=tf.stop_gradient(reward)\n",
    "accuracy=tf.reduce_sum(reward)/tf.cast(this_size,dtype=tf.float32)\n",
    "reduce_var_reward=reward-tf.stop_gradient(baseline_his)\n",
    "reinforce_llh=tf.reduce_mean(loglikelihood_his*reduce_var_reward)\n",
    "\n",
    "# regression baseline towards reward\n",
    "baseline_mse=tf.reduce_mean(tf.square(reward-baseline_his))\n",
    "\n",
    "# softmax to output\n",
    "softmax_loss=tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=y,logits=score))\n",
    "\n",
    "# summarize loss\n",
    "loss=reinforce_llh+baseline_mse+softmax_loss\n",
    "\n",
    "# # for testing gradient flow\n",
    "# dweight1=tf.gradients(loss,[prediction])\n",
    "# dweight2=tf.gradients(reinforce_llh,[prediction])\n",
    "# dweight3=tf.gradients(baseline_mse,[prediction])\n",
    "# dweight4=tf.gradients(softmax_loss,[prediction])\n",
    "# print(dweight1,dweight2,dweight3,dweight4)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 1e-3\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,num_train//batch_size, 0.9, staircase=True)\n",
    "optimizier=tf.train.RMSPropOptimizer(learning_rate=1e-3)\n",
    "train_step = optimizier.minimize(loss,global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # for testing gradient flow\n",
    "# with tf.Session() as sess:\n",
    "#     tf.global_variables_initializer().run()\n",
    "#     images,labels=mnist.train.next_batch(1)\n",
    "#     feed_dict={X:images.reshape(1,28,28,1),y:labels}\n",
    "#     dw1,dw2,dw3,dw4=sess.run([dweight1,dweight2,dweight3,dweight4],feed_dict=feed_dict)\n",
    "#     print(dw1[0])\n",
    "#     print(dw2[0])\n",
    "#     print(dw3[0])\n",
    "#     print(dw4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch=50\n",
    "print_every=50\n",
    "\n",
    "def train():\n",
    "    num_iteration=num_train//batch_size\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=mnist.train.next_batch(batch_size)\n",
    "        # prepare data for monte carlo test\n",
    "        images=np.tile(images,(MC_test,1))\n",
    "        labels=np.tile(labels,(MC_test,1))\n",
    "        feed_dict={X:images.reshape(tot_size,28,28,1),y:labels}\n",
    "        loss_num,acc_num,_ = sess.run([loss,accuracy,train_step],feed_dict=feed_dict)\n",
    "        if it==0 or (it+1)%print_every==0 or it==num_iteration-1:\n",
    "            print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "                  'iteration %d/%d:' % (it+1,num_iteration),'loss=%8f, accuracy=%.3f%%' % (loss_num,acc_num*100.0))\n",
    "\n",
    "def eval(dataset,num_iteration):\n",
    "    total_loss=0\n",
    "    total_accuracy=0\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=dataset.next_batch(batch_size)\n",
    "        # no Monte Carlo test during evaludation step\n",
    "        loss_num,accuracy_num = sess.run([loss,accuracy],feed_dict={X:images.reshape(batch_size,28,28,1),y:labels})\n",
    "        total_loss+=loss_num\n",
    "        total_accuracy+=accuracy_num\n",
    "    total_loss/=num_iteration\n",
    "    total_accuracy/=num_iteration\n",
    "    return total_loss,total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-18 15:36:04 start epoch 1/50, with learning rate = 0.001000\n",
      "2018-04-18 15:36:06 iteration 1/859: loss=2.559655, accuracy=9.851%\n",
      "2018-04-18 15:36:21 iteration 50/859: loss=2.415035, accuracy=9.094%\n",
      "2018-04-18 15:36:36 iteration 100/859: loss=2.498299, accuracy=16.541%\n",
      "2018-04-18 15:36:53 iteration 150/859: loss=2.212351, accuracy=10.901%\n",
      "2018-04-18 15:37:09 iteration 200/859: loss=2.320610, accuracy=26.428%\n",
      "2018-04-18 15:37:25 iteration 250/859: loss=2.168611, accuracy=40.173%\n",
      "2018-04-18 15:37:41 iteration 300/859: loss=1.875765, accuracy=39.429%\n",
      "2018-04-18 15:37:57 iteration 350/859: loss=1.875242, accuracy=36.133%\n",
      "2018-04-18 15:38:13 iteration 400/859: loss=1.711219, accuracy=53.210%\n",
      "2018-04-18 15:38:29 iteration 450/859: loss=1.568711, accuracy=54.858%\n",
      "2018-04-18 15:38:46 iteration 500/859: loss=1.416793, accuracy=56.519%\n",
      "2018-04-18 15:39:02 iteration 550/859: loss=1.240467, accuracy=67.603%\n",
      "2018-04-18 15:39:19 iteration 600/859: loss=1.340807, accuracy=68.494%\n",
      "2018-04-18 15:39:35 iteration 650/859: loss=1.257616, accuracy=64.941%\n",
      "2018-04-18 15:39:51 iteration 700/859: loss=1.282426, accuracy=64.734%\n",
      "2018-04-18 15:40:08 iteration 750/859: loss=1.146998, accuracy=61.133%\n",
      "2018-04-18 15:40:24 iteration 800/859: loss=1.167312, accuracy=67.773%\n",
      "2018-04-18 15:40:40 iteration 850/859: loss=0.974474, accuracy=73.462%\n",
      "2018-04-18 15:40:43 iteration 859/859: loss=1.135542, accuracy=65.479%\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-18 15:40:54 end epoch 1/50: acc_train=70.391% acc_val=71.715% acc_test=71.344%\n",
      "2018-04-18 15:40:54 start epoch 2/50, with learning rate = 0.000900\n",
      "2018-04-18 15:40:55 iteration 1/859: loss=1.017210, accuracy=73.108%\n",
      "2018-04-18 15:41:11 iteration 50/859: loss=0.896028, accuracy=78.223%\n",
      "2018-04-18 15:41:27 iteration 100/859: loss=0.780238, accuracy=78.955%\n",
      "2018-04-18 15:41:43 iteration 150/859: loss=0.753646, accuracy=80.518%\n",
      "2018-04-18 15:41:59 iteration 200/859: loss=0.828052, accuracy=74.988%\n",
      "2018-04-18 15:42:15 iteration 250/859: loss=0.753888, accuracy=76.587%\n",
      "2018-04-18 15:42:32 iteration 300/859: loss=0.791877, accuracy=78.101%\n",
      "2018-04-18 15:42:48 iteration 350/859: loss=0.678966, accuracy=81.519%\n",
      "2018-04-18 15:43:04 iteration 400/859: loss=0.589861, accuracy=84.900%\n",
      "2018-04-18 15:43:21 iteration 450/859: loss=0.607117, accuracy=84.094%\n",
      "2018-04-18 15:43:37 iteration 500/859: loss=0.516262, accuracy=84.924%\n",
      "2018-04-18 15:43:53 iteration 550/859: loss=0.736630, accuracy=81.311%\n",
      "2018-04-18 15:44:09 iteration 600/859: loss=0.557305, accuracy=89.514%\n",
      "2018-04-18 15:44:26 iteration 650/859: loss=0.585631, accuracy=81.921%\n",
      "2018-04-18 15:44:42 iteration 700/859: loss=0.629542, accuracy=87.634%\n",
      "2018-04-18 15:44:59 iteration 750/859: loss=0.680363, accuracy=83.118%\n",
      "2018-04-18 15:45:15 iteration 800/859: loss=0.386930, accuracy=89.062%\n",
      "2018-04-18 15:45:31 iteration 850/859: loss=0.355812, accuracy=89.758%\n",
      "2018-04-18 15:45:34 iteration 859/859: loss=0.699538, accuracy=79.224%\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-18 15:45:45 end epoch 2/50: acc_train=85.719% acc_val=87.240% acc_test=86.268%\n",
      "2018-04-18 15:45:45 start epoch 3/50, with learning rate = 0.000810\n",
      "2018-04-18 15:45:45 iteration 1/859: loss=0.576441, accuracy=85.596%\n",
      "2018-04-18 15:46:02 iteration 50/859: loss=0.528529, accuracy=86.829%\n",
      "2018-04-18 15:46:18 iteration 100/859: loss=0.418181, accuracy=88.611%\n",
      "2018-04-18 15:46:34 iteration 150/859: loss=0.559531, accuracy=84.900%\n",
      "2018-04-18 15:46:50 iteration 200/859: loss=0.613770, accuracy=85.657%\n",
      "2018-04-18 15:47:07 iteration 250/859: loss=0.463056, accuracy=87.415%\n",
      "2018-04-18 15:47:23 iteration 300/859: loss=0.436134, accuracy=87.683%\n",
      "2018-04-18 15:47:40 iteration 350/859: loss=0.478036, accuracy=86.426%\n",
      "2018-04-18 15:47:56 iteration 400/859: loss=0.491529, accuracy=87.976%\n",
      "2018-04-18 15:48:12 iteration 450/859: loss=0.259510, accuracy=95.056%\n",
      "2018-04-18 15:48:29 iteration 500/859: loss=0.438568, accuracy=90.393%\n",
      "2018-04-18 15:48:45 iteration 550/859: loss=0.364651, accuracy=91.479%\n",
      "2018-04-18 15:49:01 iteration 600/859: loss=0.337218, accuracy=89.685%\n",
      "2018-04-18 15:49:17 iteration 650/859: loss=0.302474, accuracy=91.223%\n",
      "2018-04-18 15:49:34 iteration 700/859: loss=0.263231, accuracy=93.579%\n",
      "2018-04-18 15:49:50 iteration 750/859: loss=0.373002, accuracy=89.539%\n",
      "2018-04-18 15:50:06 iteration 800/859: loss=0.242947, accuracy=91.650%\n",
      "2018-04-18 15:50:22 iteration 850/859: loss=0.353356, accuracy=90.344%\n",
      "2018-04-18 15:50:25 iteration 859/859: loss=0.275771, accuracy=92.944%\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-18 15:50:36 end epoch 3/50: acc_train=92.802% acc_val=93.069% acc_test=93.289%\n",
      "2018-04-18 15:50:36 start epoch 4/50, with learning rate = 0.000729\n",
      "2018-04-18 15:50:36 iteration 1/859: loss=0.354852, accuracy=90.356%\n",
      "2018-04-18 15:50:53 iteration 50/859: loss=0.330912, accuracy=90.515%\n",
      "2018-04-18 15:51:09 iteration 100/859: loss=0.237167, accuracy=93.530%\n",
      "2018-04-18 15:51:25 iteration 150/859: loss=0.310696, accuracy=91.272%\n",
      "2018-04-18 15:51:41 iteration 200/859: loss=0.423081, accuracy=89.722%\n",
      "2018-04-18 15:51:58 iteration 250/859: loss=0.250936, accuracy=93.604%\n",
      "2018-04-18 15:52:14 iteration 300/859: loss=0.354313, accuracy=90.491%\n",
      "2018-04-18 15:52:31 iteration 350/859: loss=0.432109, accuracy=85.461%\n",
      "2018-04-18 15:52:47 iteration 400/859: loss=0.192399, accuracy=93.262%\n",
      "2018-04-18 15:53:03 iteration 450/859: loss=0.270771, accuracy=93.896%\n",
      "2018-04-18 15:53:20 iteration 500/859: loss=0.239782, accuracy=94.031%\n",
      "2018-04-18 15:53:36 iteration 550/859: loss=0.199285, accuracy=95.361%\n",
      "2018-04-18 15:53:52 iteration 600/859: loss=0.141674, accuracy=94.934%\n",
      "2018-04-18 15:54:09 iteration 650/859: loss=0.262545, accuracy=91.895%\n",
      "2018-04-18 15:54:25 iteration 700/859: loss=0.238349, accuracy=95.837%\n",
      "2018-04-18 15:54:42 iteration 750/859: loss=0.306282, accuracy=92.236%\n",
      "2018-04-18 15:54:58 iteration 800/859: loss=0.271052, accuracy=91.272%\n",
      "2018-04-18 15:55:14 iteration 850/859: loss=0.248105, accuracy=93.774%\n",
      "2018-04-18 15:55:17 iteration 859/859: loss=0.174896, accuracy=95.166%\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-18 15:55:28 end epoch 4/50: acc_train=93.885% acc_val=94.171% acc_test=94.181%\n",
      "2018-04-18 15:55:28 start epoch 5/50, with learning rate = 0.000656\n",
      "2018-04-18 15:55:28 iteration 1/859: loss=0.250707, accuracy=93.738%\n",
      "2018-04-18 15:55:44 iteration 50/859: loss=0.220680, accuracy=94.019%\n",
      "2018-04-18 15:56:00 iteration 100/859: loss=0.223577, accuracy=93.567%\n",
      "2018-04-18 15:56:17 iteration 150/859: loss=0.271166, accuracy=93.567%\n",
      "2018-04-18 15:56:33 iteration 200/859: loss=0.188080, accuracy=95.178%\n",
      "2018-04-18 15:56:49 iteration 250/859: loss=0.173432, accuracy=95.898%\n",
      "2018-04-18 15:57:06 iteration 300/859: loss=0.228157, accuracy=96.252%\n",
      "2018-04-18 15:57:22 iteration 350/859: loss=0.228437, accuracy=95.386%\n",
      "2018-04-18 15:57:38 iteration 400/859: loss=0.319286, accuracy=91.956%\n",
      "2018-04-18 15:57:55 iteration 450/859: loss=0.343965, accuracy=91.321%\n",
      "2018-04-18 15:58:11 iteration 500/859: loss=0.148226, accuracy=95.190%\n",
      "2018-04-18 15:58:27 iteration 550/859: loss=0.221897, accuracy=92.761%\n",
      "2018-04-18 15:58:44 iteration 600/859: loss=0.193319, accuracy=96.216%\n",
      "2018-04-18 15:59:00 iteration 650/859: loss=0.172842, accuracy=96.057%\n",
      "2018-04-18 15:59:16 iteration 700/859: loss=0.266217, accuracy=95.496%\n",
      "2018-04-18 15:59:33 iteration 750/859: loss=0.199956, accuracy=95.154%\n",
      "2018-04-18 15:59:49 iteration 800/859: loss=0.264790, accuracy=95.593%\n",
      "2018-04-18 16:00:06 iteration 850/859: loss=0.201046, accuracy=95.068%\n",
      "2018-04-18 16:00:08 iteration 859/859: loss=0.145683, accuracy=96.704%\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-18 16:00:19 end epoch 5/50: acc_train=95.474% acc_val=95.272% acc_test=95.673%\n",
      "2018-04-18 16:00:19 start epoch 6/50, with learning rate = 0.000590\n",
      "2018-04-18 16:00:19 iteration 1/859: loss=0.142556, accuracy=96.082%\n",
      "2018-04-18 16:00:35 iteration 50/859: loss=0.158432, accuracy=95.911%\n",
      "2018-04-18 16:00:52 iteration 100/859: loss=0.229536, accuracy=95.508%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-18 16:01:08 iteration 150/859: loss=0.222550, accuracy=94.897%\n",
      "2018-04-18 16:01:25 iteration 200/859: loss=0.196834, accuracy=95.886%\n",
      "2018-04-18 16:01:41 iteration 250/859: loss=0.090290, accuracy=98.083%\n",
      "2018-04-18 16:01:57 iteration 300/859: loss=0.218756, accuracy=93.762%\n",
      "2018-04-18 16:02:13 iteration 350/859: loss=0.110574, accuracy=96.375%\n",
      "2018-04-18 16:02:29 iteration 400/859: loss=0.210858, accuracy=96.533%\n",
      "2018-04-18 16:02:46 iteration 450/859: loss=0.229993, accuracy=93.347%\n",
      "2018-04-18 16:03:02 iteration 500/859: loss=0.119576, accuracy=95.654%\n",
      "2018-04-18 16:03:19 iteration 550/859: loss=0.219952, accuracy=94.836%\n",
      "2018-04-18 16:03:35 iteration 600/859: loss=0.142250, accuracy=95.618%\n",
      "2018-04-18 16:03:51 iteration 650/859: loss=0.201611, accuracy=95.691%\n",
      "2018-04-18 16:04:07 iteration 700/859: loss=0.106272, accuracy=96.802%\n",
      "2018-04-18 16:04:24 iteration 750/859: loss=0.102310, accuracy=97.717%\n",
      "2018-04-18 16:04:40 iteration 800/859: loss=0.104705, accuracy=96.948%\n",
      "2018-04-18 16:04:57 iteration 850/859: loss=0.234637, accuracy=92.432%\n",
      "2018-04-18 16:05:00 iteration 859/859: loss=0.199820, accuracy=95.056%\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-18 16:05:11 end epoch 6/50: acc_train=95.864% acc_val=96.174% acc_test=96.094%\n",
      "2018-04-18 16:05:11 start epoch 7/50, with learning rate = 0.000531\n",
      "2018-04-18 16:05:11 iteration 1/859: loss=0.227449, accuracy=95.569%\n",
      "2018-04-18 16:05:27 iteration 50/859: loss=0.127666, accuracy=96.887%\n",
      "2018-04-18 16:05:44 iteration 100/859: loss=0.126445, accuracy=96.667%\n",
      "2018-04-18 16:06:00 iteration 150/859: loss=0.200282, accuracy=95.154%\n",
      "2018-04-18 16:06:16 iteration 200/859: loss=0.189645, accuracy=96.692%\n",
      "2018-04-18 16:06:32 iteration 250/859: loss=0.124520, accuracy=95.447%\n",
      "2018-04-18 16:06:49 iteration 300/859: loss=0.158406, accuracy=95.752%\n",
      "2018-04-18 16:07:05 iteration 350/859: loss=0.195410, accuracy=95.398%\n",
      "2018-04-18 16:07:21 iteration 400/859: loss=0.137141, accuracy=96.045%\n",
      "2018-04-18 16:07:38 iteration 450/859: loss=0.130281, accuracy=97.534%\n",
      "2018-04-18 16:07:54 iteration 500/859: loss=0.108437, accuracy=97.839%\n",
      "2018-04-18 16:08:10 iteration 550/859: loss=0.182581, accuracy=94.177%\n",
      "2018-04-18 16:08:27 iteration 600/859: loss=0.101056, accuracy=97.107%\n",
      "2018-04-18 16:08:43 iteration 650/859: loss=0.188985, accuracy=95.581%\n",
      "2018-04-18 16:08:59 iteration 700/859: loss=0.090455, accuracy=96.863%\n",
      "2018-04-18 16:09:16 iteration 750/859: loss=0.096942, accuracy=97.620%\n",
      "2018-04-18 16:09:32 iteration 800/859: loss=0.169015, accuracy=95.471%\n",
      "2018-04-18 16:09:49 iteration 850/859: loss=0.089614, accuracy=97.864%\n",
      "2018-04-18 16:09:52 iteration 859/859: loss=0.070137, accuracy=96.790%\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-18 16:10:03 end epoch 7/50: acc_train=96.458% acc_val=96.394% acc_test=96.284%\n",
      "2018-04-18 16:10:03 start epoch 8/50, with learning rate = 0.000478\n",
      "2018-04-18 16:10:03 iteration 1/859: loss=0.205326, accuracy=96.082%\n",
      "2018-04-18 16:10:19 iteration 50/859: loss=0.099653, accuracy=97.839%\n",
      "2018-04-18 16:10:36 iteration 100/859: loss=0.074445, accuracy=97.620%\n",
      "2018-04-18 16:10:52 iteration 150/859: loss=0.106799, accuracy=98.218%\n",
      "2018-04-18 16:11:08 iteration 200/859: loss=0.163139, accuracy=94.202%\n",
      "2018-04-18 16:11:24 iteration 250/859: loss=0.182148, accuracy=95.850%\n",
      "2018-04-18 16:11:40 iteration 300/859: loss=0.117724, accuracy=96.021%\n",
      "2018-04-18 16:11:57 iteration 350/859: loss=0.071018, accuracy=97.388%\n",
      "2018-04-18 16:12:13 iteration 400/859: loss=0.106332, accuracy=98.206%\n",
      "2018-04-18 16:12:29 iteration 450/859: loss=0.060133, accuracy=96.863%\n",
      "2018-04-18 16:12:45 iteration 500/859: loss=0.111317, accuracy=96.240%\n",
      "2018-04-18 16:13:02 iteration 550/859: loss=0.144725, accuracy=96.289%\n",
      "2018-04-18 16:13:18 iteration 600/859: loss=0.096972, accuracy=98.328%\n",
      "2018-04-18 16:13:34 iteration 650/859: loss=0.167944, accuracy=97.168%\n",
      "2018-04-18 16:13:51 iteration 700/859: loss=0.103408, accuracy=97.559%\n",
      "2018-04-18 16:14:07 iteration 750/859: loss=0.139907, accuracy=96.753%\n",
      "2018-04-18 16:14:23 iteration 800/859: loss=0.095945, accuracy=97.021%\n",
      "2018-04-18 16:14:39 iteration 850/859: loss=0.074502, accuracy=96.667%\n",
      "2018-04-18 16:14:42 iteration 859/859: loss=0.162520, accuracy=95.227%\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-18 16:14:53 end epoch 8/50: acc_train=96.740% acc_val=96.795% acc_test=96.905%\n",
      "2018-04-18 16:14:53 start epoch 9/50, with learning rate = 0.000430\n",
      "2018-04-18 16:14:53 iteration 1/859: loss=0.134380, accuracy=96.863%\n",
      "2018-04-18 16:15:09 iteration 50/859: loss=0.153898, accuracy=96.106%\n",
      "2018-04-18 16:15:25 iteration 100/859: loss=0.156410, accuracy=96.277%\n",
      "2018-04-18 16:15:41 iteration 150/859: loss=0.115249, accuracy=96.863%\n",
      "2018-04-18 16:15:57 iteration 200/859: loss=0.100193, accuracy=97.571%\n",
      "2018-04-18 16:16:14 iteration 250/859: loss=0.116524, accuracy=97.192%\n",
      "2018-04-18 16:16:30 iteration 300/859: loss=0.207976, accuracy=93.103%\n",
      "2018-04-18 16:16:46 iteration 350/859: loss=0.126196, accuracy=98.010%\n",
      "2018-04-18 16:17:03 iteration 400/859: loss=0.154799, accuracy=95.129%\n",
      "2018-04-18 16:17:19 iteration 450/859: loss=0.175168, accuracy=95.166%\n",
      "2018-04-18 16:17:36 iteration 500/859: loss=0.044927, accuracy=97.681%\n",
      "2018-04-18 16:17:52 iteration 550/859: loss=0.091255, accuracy=98.083%\n",
      "2018-04-18 16:18:09 iteration 600/859: loss=0.093478, accuracy=98.450%\n",
      "2018-04-18 16:18:25 iteration 650/859: loss=0.202192, accuracy=94.238%\n",
      "2018-04-18 16:18:42 iteration 700/859: loss=0.071384, accuracy=97.815%\n",
      "2018-04-18 16:18:58 iteration 750/859: loss=0.176131, accuracy=94.421%\n",
      "2018-04-18 16:19:14 iteration 800/859: loss=0.187747, accuracy=97.388%\n",
      "2018-04-18 16:19:31 iteration 850/859: loss=0.109627, accuracy=96.973%\n",
      "2018-04-18 16:19:34 iteration 859/859: loss=0.107993, accuracy=96.790%\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-18 16:19:44 end epoch 9/50: acc_train=97.337% acc_val=97.556% acc_test=97.536%\n",
      "2018-04-18 16:19:44 start epoch 10/50, with learning rate = 0.000387\n",
      "2018-04-18 16:19:45 iteration 1/859: loss=0.075546, accuracy=98.682%\n",
      "2018-04-18 16:20:01 iteration 50/859: loss=0.126138, accuracy=97.644%\n",
      "2018-04-18 16:20:17 iteration 100/859: loss=0.065114, accuracy=98.340%\n",
      "2018-04-18 16:20:34 iteration 150/859: loss=0.106979, accuracy=98.279%\n",
      "2018-04-18 16:20:50 iteration 200/859: loss=0.067557, accuracy=97.974%\n",
      "2018-04-18 16:21:07 iteration 250/859: loss=0.138284, accuracy=96.423%\n",
      "2018-04-18 16:21:23 iteration 300/859: loss=0.129730, accuracy=96.179%\n",
      "2018-04-18 16:21:40 iteration 350/859: loss=0.148363, accuracy=95.337%\n",
      "2018-04-18 16:21:56 iteration 400/859: loss=0.075428, accuracy=97.949%\n",
      "2018-04-18 16:22:13 iteration 450/859: loss=0.092201, accuracy=96.667%\n",
      "2018-04-18 16:22:29 iteration 500/859: loss=0.204208, accuracy=95.068%\n",
      "2018-04-18 16:22:46 iteration 550/859: loss=0.107523, accuracy=96.729%\n",
      "2018-04-18 16:23:02 iteration 600/859: loss=0.184632, accuracy=96.057%\n",
      "2018-04-18 16:23:18 iteration 650/859: loss=0.151282, accuracy=96.375%\n",
      "2018-04-18 16:23:35 iteration 700/859: loss=0.037640, accuracy=98.450%\n",
      "2018-04-18 16:23:51 iteration 750/859: loss=0.171855, accuracy=96.216%\n",
      "2018-04-18 16:24:07 iteration 800/859: loss=0.098771, accuracy=98.535%\n",
      "2018-04-18 16:24:24 iteration 850/859: loss=0.105572, accuracy=95.642%\n",
      "2018-04-18 16:24:27 iteration 859/859: loss=0.055610, accuracy=97.192%\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-18 16:24:38 end epoch 10/50: acc_train=97.541% acc_val=97.576% acc_test=97.696%\n",
      "2018-04-18 16:24:38 start epoch 11/50, with learning rate = 0.000349\n",
      "2018-04-18 16:24:38 iteration 1/859: loss=0.121485, accuracy=97.583%\n",
      "2018-04-18 16:24:54 iteration 50/859: loss=0.067530, accuracy=98.364%\n",
      "2018-04-18 16:25:11 iteration 100/859: loss=0.105325, accuracy=96.362%\n",
      "2018-04-18 16:25:27 iteration 150/859: loss=0.066618, accuracy=98.853%\n",
      "2018-04-18 16:25:44 iteration 200/859: loss=0.132019, accuracy=95.813%\n",
      "2018-04-18 16:26:01 iteration 250/859: loss=0.034022, accuracy=98.547%\n",
      "2018-04-18 16:26:17 iteration 300/859: loss=0.088392, accuracy=97.461%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-18 16:26:33 iteration 350/859: loss=0.118070, accuracy=96.924%\n",
      "2018-04-18 16:26:49 iteration 400/859: loss=0.151057, accuracy=96.204%\n",
      "2018-04-18 16:27:06 iteration 450/859: loss=0.165315, accuracy=95.105%\n",
      "2018-04-18 16:27:22 iteration 500/859: loss=0.098182, accuracy=99.133%\n",
      "2018-04-18 16:27:38 iteration 550/859: loss=0.075533, accuracy=97.375%\n",
      "2018-04-18 16:27:55 iteration 600/859: loss=0.022963, accuracy=98.462%\n",
      "2018-04-18 16:28:11 iteration 650/859: loss=0.064707, accuracy=98.877%\n",
      "2018-04-18 16:28:27 iteration 700/859: loss=0.114286, accuracy=98.499%\n",
      "2018-04-18 16:28:44 iteration 750/859: loss=0.128626, accuracy=96.631%\n",
      "2018-04-18 16:29:00 iteration 800/859: loss=0.124238, accuracy=96.460%\n",
      "2018-04-18 16:29:17 iteration 850/859: loss=0.024803, accuracy=99.231%\n",
      "2018-04-18 16:29:20 iteration 859/859: loss=0.117498, accuracy=97.913%\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-18 16:29:31 end epoch 11/50: acc_train=97.324% acc_val=97.596% acc_test=97.326%\n",
      "2018-04-18 16:29:31 start epoch 12/50, with learning rate = 0.000314\n",
      "2018-04-18 16:29:31 iteration 1/859: loss=0.067053, accuracy=97.925%\n",
      "2018-04-18 16:29:47 iteration 50/859: loss=0.053885, accuracy=98.669%\n",
      "2018-04-18 16:30:04 iteration 100/859: loss=0.107084, accuracy=97.510%\n",
      "2018-04-18 16:30:20 iteration 150/859: loss=0.087431, accuracy=98.816%\n",
      "2018-04-18 16:30:37 iteration 200/859: loss=0.081885, accuracy=98.621%\n",
      "2018-04-18 16:30:53 iteration 250/859: loss=0.064744, accuracy=98.145%\n",
      "2018-04-18 16:31:10 iteration 300/859: loss=0.100186, accuracy=98.669%\n",
      "2018-04-18 16:31:26 iteration 350/859: loss=0.102662, accuracy=98.511%\n",
      "2018-04-18 16:31:43 iteration 400/859: loss=0.123131, accuracy=97.949%\n",
      "2018-04-18 16:31:59 iteration 450/859: loss=0.067084, accuracy=98.987%\n",
      "2018-04-18 16:32:15 iteration 500/859: loss=0.102810, accuracy=95.923%\n",
      "2018-04-18 16:32:32 iteration 550/859: loss=0.021061, accuracy=99.463%\n",
      "2018-04-18 16:32:48 iteration 600/859: loss=0.063448, accuracy=98.169%\n",
      "2018-04-18 16:33:05 iteration 650/859: loss=0.075179, accuracy=98.621%\n",
      "2018-04-18 16:33:21 iteration 700/859: loss=0.047451, accuracy=97.668%\n",
      "2018-04-18 16:33:37 iteration 750/859: loss=0.090217, accuracy=97.485%\n",
      "2018-04-18 16:33:54 iteration 800/859: loss=0.125220, accuracy=96.741%\n",
      "2018-04-18 16:34:10 iteration 850/859: loss=0.064261, accuracy=98.474%\n",
      "2018-04-18 16:34:13 iteration 859/859: loss=0.040566, accuracy=98.828%\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-18 16:34:24 end epoch 12/50: acc_train=97.830% acc_val=97.877% acc_test=98.007%\n",
      "2018-04-18 16:34:24 start epoch 13/50, with learning rate = 0.000282\n",
      "2018-04-18 16:34:24 iteration 1/859: loss=0.053039, accuracy=98.730%\n",
      "2018-04-18 16:34:40 iteration 50/859: loss=0.074120, accuracy=98.425%\n",
      "2018-04-18 16:34:57 iteration 100/859: loss=0.071555, accuracy=97.363%\n",
      "2018-04-18 16:35:13 iteration 150/859: loss=0.076771, accuracy=98.254%\n",
      "2018-04-18 16:35:29 iteration 200/859: loss=0.040327, accuracy=98.596%\n",
      "2018-04-18 16:35:45 iteration 250/859: loss=0.123749, accuracy=96.741%\n",
      "2018-04-18 16:36:02 iteration 300/859: loss=0.198735, accuracy=95.667%\n",
      "2018-04-18 16:36:18 iteration 350/859: loss=0.158836, accuracy=95.837%\n",
      "2018-04-18 16:36:35 iteration 400/859: loss=0.084464, accuracy=99.243%\n",
      "2018-04-18 16:36:51 iteration 450/859: loss=0.100095, accuracy=97.400%\n",
      "2018-04-18 16:37:08 iteration 500/859: loss=0.091414, accuracy=98.535%\n",
      "2018-04-18 16:37:24 iteration 550/859: loss=0.090296, accuracy=97.400%\n",
      "2018-04-18 16:37:41 iteration 600/859: loss=0.029290, accuracy=99.463%\n",
      "2018-04-18 16:37:57 iteration 650/859: loss=0.072371, accuracy=99.377%\n",
      "2018-04-18 16:38:13 iteration 700/859: loss=0.102460, accuracy=96.887%\n",
      "2018-04-18 16:38:30 iteration 750/859: loss=0.097711, accuracy=97.131%\n",
      "2018-04-18 16:38:46 iteration 800/859: loss=0.072748, accuracy=98.291%\n",
      "2018-04-18 16:39:02 iteration 850/859: loss=0.133618, accuracy=96.289%\n",
      "2018-04-18 16:39:05 iteration 859/859: loss=0.080313, accuracy=98.730%\n",
      "2018-04-18 16:39:15 end epoch 13/50: acc_train=97.704% acc_val=97.376% acc_test=97.786%\n",
      "2018-04-18 16:39:15 start epoch 14/50, with learning rate = 0.000254\n",
      "2018-04-18 16:39:16 iteration 1/859: loss=0.042753, accuracy=98.938%\n",
      "2018-04-18 16:39:32 iteration 50/859: loss=0.062357, accuracy=96.875%\n",
      "2018-04-18 16:39:48 iteration 100/859: loss=0.125140, accuracy=96.826%\n",
      "2018-04-18 16:40:05 iteration 150/859: loss=0.117881, accuracy=96.167%\n",
      "2018-04-18 16:40:21 iteration 200/859: loss=0.147381, accuracy=96.460%\n",
      "2018-04-18 16:40:37 iteration 250/859: loss=0.072681, accuracy=96.851%\n",
      "2018-04-18 16:40:54 iteration 300/859: loss=0.087656, accuracy=98.169%\n",
      "2018-04-18 16:41:10 iteration 350/859: loss=0.080464, accuracy=98.303%\n",
      "2018-04-18 16:41:27 iteration 400/859: loss=0.129395, accuracy=97.546%\n",
      "2018-04-18 16:41:43 iteration 450/859: loss=0.082567, accuracy=98.572%\n",
      "2018-04-18 16:41:59 iteration 500/859: loss=0.092030, accuracy=96.765%\n",
      "2018-04-18 16:42:15 iteration 550/859: loss=0.083848, accuracy=97.656%\n",
      "2018-04-18 16:42:32 iteration 600/859: loss=0.145749, accuracy=97.107%\n",
      "2018-04-18 16:42:48 iteration 650/859: loss=0.091128, accuracy=98.096%\n",
      "2018-04-18 16:43:04 iteration 700/859: loss=0.060570, accuracy=98.523%\n",
      "2018-04-18 16:43:21 iteration 750/859: loss=0.080613, accuracy=97.363%\n",
      "2018-04-18 16:43:37 iteration 800/859: loss=0.131245, accuracy=97.461%\n",
      "2018-04-18 16:43:54 iteration 850/859: loss=0.050332, accuracy=98.657%\n",
      "2018-04-18 16:43:56 iteration 859/859: loss=0.087023, accuracy=98.315%\n",
      "2018-04-18 16:44:07 end epoch 14/50: acc_train=98.108% acc_val=97.877% acc_test=97.696%\n",
      "2018-04-18 16:44:07 start epoch 15/50, with learning rate = 0.000229\n",
      "2018-04-18 16:44:07 iteration 1/859: loss=0.045396, accuracy=97.913%\n",
      "2018-04-18 16:44:23 iteration 50/859: loss=0.153221, accuracy=97.388%\n",
      "2018-04-18 16:44:40 iteration 100/859: loss=0.189625, accuracy=94.446%\n",
      "2018-04-18 16:44:56 iteration 150/859: loss=0.054577, accuracy=98.096%\n",
      "2018-04-18 16:45:12 iteration 200/859: loss=0.055170, accuracy=96.997%\n",
      "2018-04-18 16:45:28 iteration 250/859: loss=0.051066, accuracy=98.425%\n",
      "2018-04-18 16:45:45 iteration 300/859: loss=0.006069, accuracy=99.231%\n",
      "2018-04-18 16:46:01 iteration 350/859: loss=0.152074, accuracy=96.057%\n",
      "2018-04-18 16:46:17 iteration 400/859: loss=0.082590, accuracy=98.853%\n",
      "2018-04-18 16:46:34 iteration 450/859: loss=0.115954, accuracy=97.107%\n",
      "2018-04-18 16:46:50 iteration 500/859: loss=0.039994, accuracy=98.853%\n",
      "2018-04-18 16:47:06 iteration 550/859: loss=0.089435, accuracy=96.948%\n",
      "2018-04-18 16:47:23 iteration 600/859: loss=0.081016, accuracy=97.961%\n",
      "2018-04-18 16:47:39 iteration 650/859: loss=0.117018, accuracy=96.912%\n",
      "2018-04-18 16:47:55 iteration 700/859: loss=0.019635, accuracy=98.816%\n",
      "2018-04-18 16:48:12 iteration 750/859: loss=0.080707, accuracy=98.645%\n",
      "2018-04-18 16:48:29 iteration 800/859: loss=0.059700, accuracy=97.180%\n",
      "2018-04-18 16:48:45 iteration 850/859: loss=0.078061, accuracy=96.582%\n",
      "2018-04-18 16:48:48 iteration 859/859: loss=0.167427, accuracy=95.264%\n",
      "2018-04-18 16:48:58 end epoch 15/50: acc_train=97.937% acc_val=97.776% acc_test=97.796%\n",
      "2018-04-18 16:48:58 start epoch 16/50, with learning rate = 0.000206\n",
      "2018-04-18 16:48:59 iteration 1/859: loss=0.065029, accuracy=99.304%\n",
      "2018-04-18 16:49:15 iteration 50/859: loss=0.033279, accuracy=97.974%\n",
      "2018-04-18 16:49:32 iteration 100/859: loss=0.076590, accuracy=99.365%\n",
      "2018-04-18 16:49:48 iteration 150/859: loss=0.088374, accuracy=97.949%\n",
      "2018-04-18 16:50:05 iteration 200/859: loss=0.078545, accuracy=98.242%\n",
      "2018-04-18 16:50:22 iteration 250/859: loss=0.077982, accuracy=99.121%\n",
      "2018-04-18 16:50:38 iteration 300/859: loss=0.118818, accuracy=96.387%\n",
      "2018-04-18 16:50:55 iteration 350/859: loss=0.098366, accuracy=97.058%\n",
      "2018-04-18 16:51:11 iteration 400/859: loss=0.087432, accuracy=97.424%\n",
      "2018-04-18 16:51:27 iteration 450/859: loss=0.030689, accuracy=99.170%\n",
      "2018-04-18 16:51:44 iteration 500/859: loss=0.038420, accuracy=99.133%\n",
      "2018-04-18 16:52:00 iteration 550/859: loss=0.103916, accuracy=98.596%\n",
      "2018-04-18 16:52:16 iteration 600/859: loss=0.102759, accuracy=96.826%\n",
      "2018-04-18 16:52:33 iteration 650/859: loss=0.147397, accuracy=96.716%\n",
      "2018-04-18 16:52:49 iteration 700/859: loss=0.046527, accuracy=97.253%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-18 16:53:06 iteration 750/859: loss=0.125593, accuracy=96.155%\n",
      "2018-04-18 16:53:22 iteration 800/859: loss=0.008188, accuracy=98.596%\n",
      "2018-04-18 16:53:39 iteration 850/859: loss=0.087618, accuracy=98.389%\n",
      "2018-04-18 16:53:42 iteration 859/859: loss=0.119067, accuracy=97.058%\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-18 16:53:52 end epoch 16/50: acc_train=97.994% acc_val=97.957% acc_test=97.666%\n",
      "2018-04-18 16:53:52 start epoch 17/50, with learning rate = 0.000185\n",
      "2018-04-18 16:53:53 iteration 1/859: loss=0.060029, accuracy=98.633%\n",
      "2018-04-18 16:54:08 iteration 50/859: loss=0.116861, accuracy=96.252%\n",
      "2018-04-18 16:54:25 iteration 100/859: loss=0.179894, accuracy=95.520%\n",
      "2018-04-18 16:54:41 iteration 150/859: loss=0.093011, accuracy=97.021%\n",
      "2018-04-18 16:54:58 iteration 200/859: loss=0.020262, accuracy=98.682%\n",
      "2018-04-18 16:55:14 iteration 250/859: loss=0.070502, accuracy=98.865%\n",
      "2018-04-18 16:55:31 iteration 300/859: loss=0.046732, accuracy=97.595%\n",
      "2018-04-18 16:55:47 iteration 350/859: loss=0.099580, accuracy=97.070%\n",
      "2018-04-18 16:56:03 iteration 400/859: loss=0.048262, accuracy=98.120%\n",
      "2018-04-18 16:56:20 iteration 450/859: loss=0.105667, accuracy=96.606%\n",
      "2018-04-18 16:56:37 iteration 500/859: loss=0.090540, accuracy=97.961%\n",
      "2018-04-18 16:56:53 iteration 550/859: loss=0.057929, accuracy=98.438%\n",
      "2018-04-18 16:57:09 iteration 600/859: loss=0.095366, accuracy=96.802%\n",
      "2018-04-18 16:57:26 iteration 650/859: loss=0.064054, accuracy=97.766%\n",
      "2018-04-18 16:57:42 iteration 700/859: loss=0.073757, accuracy=97.717%\n",
      "2018-04-18 16:57:59 iteration 750/859: loss=0.026303, accuracy=98.853%\n",
      "2018-04-18 16:58:15 iteration 800/859: loss=0.067381, accuracy=97.803%\n",
      "2018-04-18 16:58:31 iteration 850/859: loss=0.049966, accuracy=98.413%\n",
      "2018-04-18 16:58:34 iteration 859/859: loss=0.139823, accuracy=96.033%\n",
      "2018-04-18 16:58:45 end epoch 17/50: acc_train=97.901% acc_val=97.796% acc_test=97.867%\n",
      "2018-04-18 16:58:45 start epoch 18/50, with learning rate = 0.000167\n",
      "2018-04-18 16:58:45 iteration 1/859: loss=0.084159, accuracy=98.450%\n",
      "2018-04-18 16:59:01 iteration 50/859: loss=0.036756, accuracy=98.889%\n",
      "2018-04-18 16:59:18 iteration 100/859: loss=0.102524, accuracy=97.815%\n",
      "2018-04-18 16:59:34 iteration 150/859: loss=0.084806, accuracy=98.877%\n",
      "2018-04-18 16:59:51 iteration 200/859: loss=0.010971, accuracy=98.596%\n",
      "2018-04-18 17:00:07 iteration 250/859: loss=0.086725, accuracy=97.180%\n",
      "2018-04-18 17:00:23 iteration 300/859: loss=0.093660, accuracy=97.168%\n",
      "2018-04-18 17:00:40 iteration 350/859: loss=0.052955, accuracy=99.109%\n",
      "2018-04-18 17:00:56 iteration 400/859: loss=0.080192, accuracy=98.633%\n",
      "2018-04-18 17:01:12 iteration 450/859: loss=0.078889, accuracy=98.071%\n",
      "2018-04-18 17:01:29 iteration 500/859: loss=0.109441, accuracy=98.145%\n",
      "2018-04-18 17:01:45 iteration 550/859: loss=0.077662, accuracy=97.852%\n",
      "2018-04-18 17:02:01 iteration 600/859: loss=0.040895, accuracy=98.450%\n",
      "2018-04-18 17:02:18 iteration 650/859: loss=0.151433, accuracy=96.606%\n",
      "2018-04-18 17:02:34 iteration 700/859: loss=0.015612, accuracy=99.243%\n",
      "2018-04-18 17:02:51 iteration 750/859: loss=0.093295, accuracy=96.899%\n",
      "2018-04-18 17:03:07 iteration 800/859: loss=0.069196, accuracy=99.023%\n",
      "2018-04-18 17:03:23 iteration 850/859: loss=0.051426, accuracy=98.560%\n",
      "2018-04-18 17:03:27 iteration 859/859: loss=0.137802, accuracy=95.374%\n",
      "2018-04-18 17:03:37 end epoch 18/50: acc_train=97.288% acc_val=97.296% acc_test=96.985%\n",
      "2018-04-18 17:03:37 start epoch 19/50, with learning rate = 0.000150\n",
      "2018-04-18 17:03:37 iteration 1/859: loss=0.116150, accuracy=98.254%\n",
      "2018-04-18 17:03:53 iteration 50/859: loss=0.050437, accuracy=98.279%\n",
      "2018-04-18 17:04:10 iteration 100/859: loss=0.136171, accuracy=97.571%\n",
      "2018-04-18 17:04:26 iteration 150/859: loss=0.019714, accuracy=99.536%\n",
      "2018-04-18 17:04:43 iteration 200/859: loss=0.018425, accuracy=99.634%\n",
      "2018-04-18 17:04:59 iteration 250/859: loss=0.075138, accuracy=98.462%\n",
      "2018-04-18 17:05:15 iteration 300/859: loss=0.046539, accuracy=99.084%\n",
      "2018-04-18 17:05:31 iteration 350/859: loss=0.075690, accuracy=98.193%\n",
      "2018-04-18 17:05:48 iteration 400/859: loss=0.019052, accuracy=98.718%\n",
      "2018-04-18 17:06:04 iteration 450/859: loss=0.075502, accuracy=99.219%\n",
      "2018-04-18 17:06:20 iteration 500/859: loss=0.044425, accuracy=97.668%\n",
      "2018-04-18 17:06:37 iteration 550/859: loss=0.040617, accuracy=97.888%\n",
      "2018-04-18 17:06:53 iteration 600/859: loss=0.081307, accuracy=96.692%\n",
      "2018-04-18 17:07:09 iteration 650/859: loss=0.048030, accuracy=97.827%\n",
      "2018-04-18 17:07:26 iteration 700/859: loss=0.133438, accuracy=98.022%\n",
      "2018-04-18 17:07:43 iteration 750/859: loss=0.082370, accuracy=98.364%\n",
      "2018-04-18 17:07:59 iteration 800/859: loss=0.049989, accuracy=98.730%\n",
      "2018-04-18 17:08:16 iteration 850/859: loss=0.084458, accuracy=97.595%\n",
      "2018-04-18 17:08:19 iteration 859/859: loss=0.112949, accuracy=96.313%\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-18 17:08:29 end epoch 19/50: acc_train=97.921% acc_val=98.117% acc_test=97.837%\n",
      "2018-04-18 17:08:29 start epoch 20/50, with learning rate = 0.000135\n",
      "2018-04-18 17:08:29 iteration 1/859: loss=0.094028, accuracy=99.048%\n",
      "2018-04-18 17:08:45 iteration 50/859: loss=0.089377, accuracy=97.205%\n",
      "2018-04-18 17:09:02 iteration 100/859: loss=0.032941, accuracy=98.975%\n",
      "2018-04-18 17:09:18 iteration 150/859: loss=0.070896, accuracy=98.022%\n",
      "2018-04-18 17:09:35 iteration 200/859: loss=0.071949, accuracy=97.705%\n",
      "2018-04-18 17:09:51 iteration 250/859: loss=0.048556, accuracy=98.291%\n",
      "2018-04-18 17:10:08 iteration 300/859: loss=0.038699, accuracy=98.914%\n",
      "2018-04-18 17:10:24 iteration 350/859: loss=0.087343, accuracy=98.108%\n",
      "2018-04-18 17:10:40 iteration 400/859: loss=0.061699, accuracy=98.120%\n",
      "2018-04-18 17:10:57 iteration 450/859: loss=0.039567, accuracy=98.474%\n",
      "2018-04-18 17:11:13 iteration 500/859: loss=0.085987, accuracy=98.804%\n",
      "2018-04-18 17:11:30 iteration 550/859: loss=0.053113, accuracy=98.853%\n",
      "2018-04-18 17:11:47 iteration 600/859: loss=0.141127, accuracy=96.863%\n",
      "2018-04-18 17:12:03 iteration 650/859: loss=0.103024, accuracy=97.302%\n",
      "2018-04-18 17:12:20 iteration 700/859: loss=0.088724, accuracy=97.742%\n",
      "2018-04-18 17:12:36 iteration 750/859: loss=0.044073, accuracy=99.072%\n",
      "2018-04-18 17:12:53 iteration 800/859: loss=0.069197, accuracy=99.487%\n",
      "2018-04-18 17:13:09 iteration 850/859: loss=0.067685, accuracy=98.340%\n",
      "2018-04-18 17:13:12 iteration 859/859: loss=0.029260, accuracy=98.621%\n",
      "2018-04-18 17:13:23 end epoch 20/50: acc_train=98.088% acc_val=97.776% acc_test=97.997%\n",
      "2018-04-18 17:13:23 start epoch 21/50, with learning rate = 0.000122\n",
      "2018-04-18 17:13:23 iteration 1/859: loss=0.083982, accuracy=98.315%\n",
      "2018-04-18 17:13:40 iteration 50/859: loss=0.046606, accuracy=99.109%\n",
      "2018-04-18 17:13:56 iteration 100/859: loss=0.021312, accuracy=98.962%\n",
      "2018-04-18 17:14:12 iteration 150/859: loss=0.096045, accuracy=98.718%\n",
      "2018-04-18 17:14:29 iteration 200/859: loss=0.067604, accuracy=97.803%\n",
      "2018-04-18 17:14:46 iteration 250/859: loss=0.053738, accuracy=97.571%\n",
      "2018-04-18 17:15:02 iteration 300/859: loss=0.030003, accuracy=99.146%\n",
      "2018-04-18 17:15:19 iteration 350/859: loss=0.013580, accuracy=99.292%\n",
      "2018-04-18 17:15:35 iteration 400/859: loss=0.114374, accuracy=96.289%\n",
      "2018-04-18 17:15:52 iteration 450/859: loss=0.057525, accuracy=98.718%\n",
      "2018-04-18 17:16:08 iteration 500/859: loss=0.074607, accuracy=97.144%\n",
      "2018-04-18 17:16:25 iteration 550/859: loss=0.048259, accuracy=97.693%\n",
      "2018-04-18 17:16:42 iteration 600/859: loss=0.055057, accuracy=97.864%\n",
      "2018-04-18 17:16:58 iteration 650/859: loss=0.043618, accuracy=98.486%\n",
      "2018-04-18 17:17:14 iteration 700/859: loss=0.073357, accuracy=96.704%\n",
      "2018-04-18 17:17:31 iteration 750/859: loss=0.041088, accuracy=98.315%\n",
      "2018-04-18 17:17:47 iteration 800/859: loss=0.036274, accuracy=99.084%\n",
      "2018-04-18 17:18:03 iteration 850/859: loss=0.060629, accuracy=98.657%\n",
      "2018-04-18 17:18:06 iteration 859/859: loss=0.021770, accuracy=99.414%\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-18 17:18:17 end epoch 21/50: acc_train=98.423% acc_val=98.438% acc_test=98.367%\n",
      "2018-04-18 17:18:17 start epoch 22/50, with learning rate = 0.000109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-18 17:18:18 iteration 1/859: loss=0.086411, accuracy=97.388%\n",
      "2018-04-18 17:18:34 iteration 50/859: loss=0.047742, accuracy=99.109%\n",
      "2018-04-18 17:18:50 iteration 100/859: loss=0.058215, accuracy=98.767%\n",
      "2018-04-18 17:19:07 iteration 150/859: loss=0.134244, accuracy=96.118%\n",
      "2018-04-18 17:19:23 iteration 200/859: loss=0.074539, accuracy=97.192%\n",
      "2018-04-18 17:19:39 iteration 250/859: loss=0.030241, accuracy=99.219%\n",
      "2018-04-18 17:19:55 iteration 300/859: loss=0.077910, accuracy=97.473%\n",
      "2018-04-18 17:20:12 iteration 350/859: loss=0.065415, accuracy=99.170%\n",
      "2018-04-18 17:20:28 iteration 400/859: loss=0.082548, accuracy=97.986%\n",
      "2018-04-18 17:20:44 iteration 450/859: loss=0.132677, accuracy=96.729%\n",
      "2018-04-18 17:21:01 iteration 500/859: loss=0.056940, accuracy=97.925%\n",
      "2018-04-18 17:21:18 iteration 550/859: loss=0.095903, accuracy=98.828%\n",
      "2018-04-18 17:21:34 iteration 600/859: loss=0.092102, accuracy=97.913%\n",
      "2018-04-18 17:21:50 iteration 650/859: loss=0.108568, accuracy=97.058%\n",
      "2018-04-18 17:22:07 iteration 700/859: loss=0.066841, accuracy=98.926%\n",
      "2018-04-18 17:22:24 iteration 750/859: loss=0.088633, accuracy=97.571%\n",
      "2018-04-18 17:22:40 iteration 800/859: loss=0.076534, accuracy=97.278%\n",
      "2018-04-18 17:22:56 iteration 850/859: loss=0.079109, accuracy=97.742%\n",
      "2018-04-18 17:22:59 iteration 859/859: loss=0.128189, accuracy=97.144%\n",
      "2018-04-18 17:23:10 end epoch 22/50: acc_train=98.126% acc_val=97.997% acc_test=97.947%\n",
      "2018-04-18 17:23:10 start epoch 23/50, with learning rate = 0.000098\n",
      "2018-04-18 17:23:10 iteration 1/859: loss=0.040177, accuracy=99.011%\n",
      "2018-04-18 17:23:26 iteration 50/859: loss=0.112307, accuracy=97.327%\n",
      "2018-04-18 17:23:43 iteration 100/859: loss=0.033974, accuracy=99.207%\n",
      "2018-04-18 17:24:00 iteration 150/859: loss=0.068549, accuracy=98.193%\n",
      "2018-04-18 17:24:16 iteration 200/859: loss=0.067452, accuracy=98.523%\n",
      "2018-04-18 17:24:32 iteration 250/859: loss=0.072005, accuracy=98.511%\n",
      "2018-04-18 17:24:49 iteration 300/859: loss=0.051816, accuracy=99.475%\n",
      "2018-04-18 17:25:05 iteration 350/859: loss=0.053808, accuracy=99.084%\n",
      "2018-04-18 17:25:22 iteration 400/859: loss=0.083085, accuracy=98.047%\n",
      "2018-04-18 17:25:38 iteration 450/859: loss=0.058064, accuracy=99.219%\n",
      "2018-04-18 17:25:54 iteration 500/859: loss=0.070629, accuracy=99.231%\n",
      "2018-04-18 17:26:11 iteration 550/859: loss=0.104132, accuracy=98.401%\n",
      "2018-04-18 17:26:27 iteration 600/859: loss=0.067356, accuracy=97.546%\n",
      "2018-04-18 17:26:43 iteration 650/859: loss=0.065423, accuracy=98.547%\n",
      "2018-04-18 17:27:00 iteration 700/859: loss=0.106394, accuracy=96.240%\n",
      "2018-04-18 17:27:17 iteration 750/859: loss=0.099784, accuracy=98.730%\n",
      "2018-04-18 17:27:33 iteration 800/859: loss=0.023202, accuracy=99.365%\n",
      "2018-04-18 17:27:49 iteration 850/859: loss=0.103201, accuracy=96.582%\n",
      "2018-04-18 17:27:52 iteration 859/859: loss=0.055504, accuracy=97.864%\n",
      "2018-04-18 17:28:02 end epoch 23/50: acc_train=98.450% acc_val=98.357% acc_test=98.187%\n",
      "2018-04-18 17:28:02 start epoch 24/50, with learning rate = 0.000089\n",
      "2018-04-18 17:28:03 iteration 1/859: loss=0.065004, accuracy=99.170%\n",
      "2018-04-18 17:28:19 iteration 50/859: loss=0.115035, accuracy=98.279%\n",
      "2018-04-18 17:28:36 iteration 100/859: loss=0.064940, accuracy=98.157%\n",
      "2018-04-18 17:28:52 iteration 150/859: loss=0.066993, accuracy=96.863%\n",
      "2018-04-18 17:29:08 iteration 200/859: loss=0.031071, accuracy=98.950%\n",
      "2018-04-18 17:29:25 iteration 250/859: loss=0.040561, accuracy=98.669%\n",
      "2018-04-18 17:29:41 iteration 300/859: loss=0.103199, accuracy=97.705%\n",
      "2018-04-18 17:29:58 iteration 350/859: loss=0.051194, accuracy=98.547%\n",
      "2018-04-18 17:30:14 iteration 400/859: loss=0.059952, accuracy=99.170%\n",
      "2018-04-18 17:30:31 iteration 450/859: loss=0.034860, accuracy=98.828%\n",
      "2018-04-18 17:30:47 iteration 500/859: loss=0.045123, accuracy=98.499%\n",
      "2018-04-18 17:31:03 iteration 550/859: loss=0.031695, accuracy=98.254%\n",
      "2018-04-18 17:31:20 iteration 600/859: loss=0.092063, accuracy=97.327%\n",
      "2018-04-18 17:31:36 iteration 650/859: loss=0.028456, accuracy=98.914%\n",
      "2018-04-18 17:31:52 iteration 700/859: loss=0.104353, accuracy=97.546%\n",
      "2018-04-18 17:32:08 iteration 750/859: loss=0.071736, accuracy=98.035%\n",
      "2018-04-18 17:32:25 iteration 800/859: loss=0.070488, accuracy=98.315%\n",
      "2018-04-18 17:32:41 iteration 850/859: loss=0.061754, accuracy=98.779%\n",
      "2018-04-18 17:32:44 iteration 859/859: loss=0.021684, accuracy=99.243%\n",
      "2018-04-18 17:32:55 end epoch 24/50: acc_train=98.354% acc_val=98.417% acc_test=98.337%\n",
      "2018-04-18 17:32:55 start epoch 25/50, with learning rate = 0.000080\n",
      "2018-04-18 17:32:55 iteration 1/859: loss=0.078106, accuracy=98.108%\n",
      "2018-04-18 17:33:11 iteration 50/859: loss=0.051784, accuracy=98.157%\n",
      "2018-04-18 17:33:27 iteration 100/859: loss=0.078928, accuracy=98.267%\n",
      "2018-04-18 17:33:44 iteration 150/859: loss=0.065955, accuracy=99.292%\n",
      "2018-04-18 17:34:01 iteration 200/859: loss=0.054087, accuracy=98.999%\n",
      "2018-04-18 17:34:17 iteration 250/859: loss=0.105415, accuracy=98.828%\n",
      "2018-04-18 17:34:34 iteration 300/859: loss=0.030695, accuracy=99.255%\n",
      "2018-04-18 17:34:50 iteration 350/859: loss=0.041029, accuracy=98.767%\n",
      "2018-04-18 17:35:06 iteration 400/859: loss=0.083270, accuracy=97.583%\n",
      "2018-04-18 17:35:23 iteration 450/859: loss=0.113778, accuracy=97.778%\n",
      "2018-04-18 17:35:40 iteration 500/859: loss=0.097529, accuracy=96.387%\n",
      "2018-04-18 17:35:56 iteration 550/859: loss=0.101679, accuracy=97.461%\n",
      "2018-04-18 17:36:13 iteration 600/859: loss=0.083856, accuracy=98.010%\n",
      "2018-04-18 17:36:29 iteration 650/859: loss=0.043680, accuracy=98.291%\n",
      "2018-04-18 17:36:46 iteration 700/859: loss=0.006367, accuracy=99.243%\n",
      "2018-04-18 17:37:02 iteration 750/859: loss=0.039842, accuracy=98.865%\n",
      "2018-04-18 17:37:19 iteration 800/859: loss=0.071007, accuracy=97.620%\n",
      "2018-04-18 17:37:35 iteration 850/859: loss=0.057267, accuracy=98.608%\n",
      "2018-04-18 17:37:38 iteration 859/859: loss=0.068960, accuracy=98.169%\n",
      "2018-04-18 17:37:49 end epoch 25/50: acc_train=98.388% acc_val=98.337% acc_test=98.247%\n",
      "2018-04-18 17:37:49 start epoch 26/50, with learning rate = 0.000072\n",
      "2018-04-18 17:37:49 iteration 1/859: loss=0.062867, accuracy=98.108%\n",
      "2018-04-18 17:38:05 iteration 50/859: loss=0.063329, accuracy=98.425%\n",
      "2018-04-18 17:38:22 iteration 100/859: loss=0.045506, accuracy=98.987%\n",
      "2018-04-18 17:38:38 iteration 150/859: loss=0.063432, accuracy=99.609%\n",
      "2018-04-18 17:38:54 iteration 200/859: loss=0.041652, accuracy=99.182%\n",
      "2018-04-18 17:39:11 iteration 250/859: loss=0.019598, accuracy=99.561%\n",
      "2018-04-18 17:39:27 iteration 300/859: loss=0.031739, accuracy=99.072%\n",
      "2018-04-18 17:39:44 iteration 350/859: loss=0.035123, accuracy=98.669%\n",
      "2018-04-18 17:40:00 iteration 400/859: loss=0.058727, accuracy=98.462%\n",
      "2018-04-18 17:40:16 iteration 450/859: loss=0.127203, accuracy=97.498%\n",
      "2018-04-18 17:40:33 iteration 500/859: loss=0.055430, accuracy=98.401%\n",
      "2018-04-18 17:40:49 iteration 550/859: loss=0.061893, accuracy=97.217%\n",
      "2018-04-18 17:41:05 iteration 600/859: loss=0.065053, accuracy=98.462%\n",
      "2018-04-18 17:41:22 iteration 650/859: loss=0.036717, accuracy=98.145%\n",
      "2018-04-18 17:41:38 iteration 700/859: loss=0.059747, accuracy=99.109%\n",
      "2018-04-18 17:41:54 iteration 750/859: loss=0.040030, accuracy=99.670%\n",
      "2018-04-18 17:42:11 iteration 800/859: loss=0.034648, accuracy=99.158%\n",
      "2018-04-18 17:42:28 iteration 850/859: loss=0.064029, accuracy=98.206%\n",
      "2018-04-18 17:42:31 iteration 859/859: loss=0.075606, accuracy=98.022%\n",
      "2018-04-18 17:42:41 end epoch 26/50: acc_train=98.607% acc_val=98.317% acc_test=98.247%\n",
      "2018-04-18 17:42:41 start epoch 27/50, with learning rate = 0.000065\n",
      "2018-04-18 17:42:42 iteration 1/859: loss=0.045764, accuracy=98.242%\n",
      "2018-04-18 17:42:58 iteration 50/859: loss=0.000429, accuracy=99.365%\n",
      "2018-04-18 17:43:14 iteration 100/859: loss=0.076563, accuracy=97.839%\n",
      "2018-04-18 17:43:30 iteration 150/859: loss=0.069329, accuracy=98.047%\n",
      "2018-04-18 17:43:46 iteration 200/859: loss=0.024504, accuracy=99.463%\n",
      "2018-04-18 17:44:03 iteration 250/859: loss=0.010881, accuracy=98.975%\n",
      "2018-04-18 17:44:19 iteration 300/859: loss=0.007414, accuracy=98.022%\n",
      "2018-04-18 17:44:36 iteration 350/859: loss=0.015474, accuracy=99.304%\n",
      "2018-04-18 17:44:52 iteration 400/859: loss=0.083872, accuracy=98.169%\n",
      "2018-04-18 17:45:09 iteration 450/859: loss=0.046772, accuracy=99.463%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-18 17:45:25 iteration 500/859: loss=0.077315, accuracy=97.974%\n",
      "2018-04-18 17:45:41 iteration 550/859: loss=0.054120, accuracy=97.754%\n",
      "2018-04-18 17:45:58 iteration 600/859: loss=0.139620, accuracy=96.716%\n",
      "2018-04-18 17:46:14 iteration 650/859: loss=0.049086, accuracy=98.645%\n",
      "2018-04-18 17:46:31 iteration 700/859: loss=0.066396, accuracy=98.181%\n",
      "2018-04-18 17:46:47 iteration 750/859: loss=0.062182, accuracy=97.925%\n",
      "2018-04-18 17:47:04 iteration 800/859: loss=0.042208, accuracy=99.304%\n",
      "2018-04-18 17:47:20 iteration 850/859: loss=0.063604, accuracy=99.048%\n",
      "2018-04-18 17:47:23 iteration 859/859: loss=0.061599, accuracy=97.668%\n",
      "2018-04-18 17:47:34 end epoch 27/50: acc_train=98.459% acc_val=98.297% acc_test=98.277%\n",
      "2018-04-18 17:47:34 start epoch 28/50, with learning rate = 0.000058\n",
      "2018-04-18 17:47:34 iteration 1/859: loss=0.096011, accuracy=97.754%\n",
      "2018-04-18 17:47:50 iteration 50/859: loss=0.041289, accuracy=99.548%\n",
      "2018-04-18 17:48:06 iteration 100/859: loss=0.072064, accuracy=97.241%\n",
      "2018-04-18 17:48:23 iteration 150/859: loss=0.087978, accuracy=97.839%\n",
      "2018-04-18 17:48:39 iteration 200/859: loss=0.047265, accuracy=99.377%\n",
      "2018-04-18 17:48:55 iteration 250/859: loss=0.093937, accuracy=97.449%\n",
      "2018-04-18 17:49:12 iteration 300/859: loss=0.035111, accuracy=99.316%\n",
      "2018-04-18 17:49:28 iteration 350/859: loss=0.079300, accuracy=97.070%\n",
      "2018-04-18 17:49:44 iteration 400/859: loss=0.055024, accuracy=98.523%\n",
      "2018-04-18 17:50:01 iteration 450/859: loss=0.076572, accuracy=97.656%\n",
      "2018-04-18 17:50:17 iteration 500/859: loss=0.107567, accuracy=98.145%\n",
      "2018-04-18 17:50:34 iteration 550/859: loss=0.089392, accuracy=98.474%\n",
      "2018-04-18 17:50:50 iteration 600/859: loss=0.098087, accuracy=98.157%\n",
      "2018-04-18 17:51:06 iteration 650/859: loss=0.075604, accuracy=98.547%\n",
      "2018-04-18 17:51:23 iteration 700/859: loss=0.032379, accuracy=99.219%\n",
      "2018-04-18 17:51:39 iteration 750/859: loss=0.080263, accuracy=98.328%\n",
      "2018-04-18 17:51:55 iteration 800/859: loss=0.031191, accuracy=99.011%\n",
      "2018-04-18 17:52:12 iteration 850/859: loss=0.008840, accuracy=99.414%\n",
      "2018-04-18 17:52:15 iteration 859/859: loss=0.069101, accuracy=98.828%\n",
      "2018-04-18 17:52:25 end epoch 28/50: acc_train=98.367% acc_val=98.197% acc_test=98.187%\n",
      "2018-04-18 17:52:25 start epoch 29/50, with learning rate = 0.000052\n",
      "2018-04-18 17:52:26 iteration 1/859: loss=0.062779, accuracy=97.522%\n",
      "2018-04-18 17:52:42 iteration 50/859: loss=0.060199, accuracy=99.097%\n",
      "2018-04-18 17:52:58 iteration 100/859: loss=0.069216, accuracy=98.840%\n",
      "2018-04-18 17:53:14 iteration 150/859: loss=0.086377, accuracy=98.132%\n",
      "2018-04-18 17:53:31 iteration 200/859: loss=0.079585, accuracy=96.924%\n",
      "2018-04-18 17:53:47 iteration 250/859: loss=0.105268, accuracy=97.314%\n",
      "2018-04-18 17:54:03 iteration 300/859: loss=0.085771, accuracy=98.206%\n",
      "2018-04-18 17:54:20 iteration 350/859: loss=0.097870, accuracy=97.278%\n",
      "2018-04-18 17:54:36 iteration 400/859: loss=0.017928, accuracy=99.170%\n",
      "2018-04-18 17:54:52 iteration 450/859: loss=0.063372, accuracy=99.121%\n",
      "2018-04-18 17:55:09 iteration 500/859: loss=0.157427, accuracy=96.655%\n",
      "2018-04-18 17:55:25 iteration 550/859: loss=0.021600, accuracy=98.877%\n",
      "2018-04-18 17:55:42 iteration 600/859: loss=0.050421, accuracy=99.292%\n",
      "2018-04-18 17:55:58 iteration 650/859: loss=0.040478, accuracy=98.865%\n",
      "2018-04-18 17:56:15 iteration 700/859: loss=0.043587, accuracy=99.231%\n",
      "2018-04-18 17:56:31 iteration 750/859: loss=0.087461, accuracy=98.157%\n",
      "2018-04-18 17:56:47 iteration 800/859: loss=0.029750, accuracy=98.523%\n",
      "2018-04-18 17:57:04 iteration 850/859: loss=0.063933, accuracy=98.059%\n",
      "2018-04-18 17:57:07 iteration 859/859: loss=0.057370, accuracy=97.449%\n",
      "2018-04-18 17:57:17 end epoch 29/50: acc_train=98.412% acc_val=98.217% acc_test=98.337%\n",
      "2018-04-18 17:57:17 start epoch 30/50, with learning rate = 0.000047\n",
      "2018-04-18 17:57:17 iteration 1/859: loss=0.069939, accuracy=99.231%\n",
      "2018-04-18 17:57:34 iteration 50/859: loss=0.017076, accuracy=99.304%\n",
      "2018-04-18 17:57:50 iteration 100/859: loss=0.055052, accuracy=98.584%\n",
      "2018-04-18 17:58:07 iteration 150/859: loss=0.097656, accuracy=98.059%\n",
      "2018-04-18 17:58:23 iteration 200/859: loss=0.066818, accuracy=98.218%\n",
      "2018-04-18 17:58:40 iteration 250/859: loss=0.048818, accuracy=99.451%\n",
      "2018-04-18 17:58:56 iteration 300/859: loss=0.153693, accuracy=95.215%\n",
      "2018-04-18 17:59:13 iteration 350/859: loss=0.080254, accuracy=98.901%\n",
      "2018-04-18 17:59:29 iteration 400/859: loss=0.080546, accuracy=98.071%\n",
      "2018-04-18 17:59:45 iteration 450/859: loss=0.081380, accuracy=98.047%\n",
      "2018-04-18 18:00:02 iteration 500/859: loss=0.038736, accuracy=98.511%\n",
      "2018-04-18 18:00:19 iteration 550/859: loss=0.048283, accuracy=98.865%\n",
      "2018-04-18 18:00:35 iteration 600/859: loss=0.015469, accuracy=99.377%\n",
      "2018-04-18 18:00:51 iteration 650/859: loss=0.023244, accuracy=99.341%\n",
      "2018-04-18 18:01:08 iteration 700/859: loss=0.082661, accuracy=97.314%\n",
      "2018-04-18 18:01:24 iteration 750/859: loss=0.084496, accuracy=98.950%\n",
      "2018-04-18 18:01:40 iteration 800/859: loss=0.029624, accuracy=98.596%\n",
      "2018-04-18 18:01:57 iteration 850/859: loss=0.068612, accuracy=98.108%\n",
      "2018-04-18 18:02:00 iteration 859/859: loss=0.066692, accuracy=99.023%\n",
      "2018-04-18 18:02:10 end epoch 30/50: acc_train=98.681% acc_val=98.337% acc_test=98.488%\n",
      "2018-04-18 18:02:10 start epoch 31/50, with learning rate = 0.000042\n",
      "2018-04-18 18:02:10 iteration 1/859: loss=0.042701, accuracy=97.839%\n",
      "2018-04-18 18:02:27 iteration 50/859: loss=0.064422, accuracy=98.047%\n",
      "2018-04-18 18:02:43 iteration 100/859: loss=0.064271, accuracy=97.766%\n",
      "2018-04-18 18:02:59 iteration 150/859: loss=0.050976, accuracy=98.328%\n",
      "2018-04-18 18:03:16 iteration 200/859: loss=0.055776, accuracy=99.255%\n",
      "2018-04-18 18:03:32 iteration 250/859: loss=0.145935, accuracy=97.266%\n",
      "2018-04-18 18:03:48 iteration 300/859: loss=0.054897, accuracy=98.901%\n",
      "2018-04-18 18:04:05 iteration 350/859: loss=0.060023, accuracy=98.315%\n",
      "2018-04-18 18:04:22 iteration 400/859: loss=0.033124, accuracy=98.926%\n",
      "2018-04-18 18:04:38 iteration 450/859: loss=0.091019, accuracy=97.473%\n",
      "2018-04-18 18:04:55 iteration 500/859: loss=0.008240, accuracy=99.219%\n",
      "2018-04-18 18:05:11 iteration 550/859: loss=0.056617, accuracy=98.657%\n",
      "2018-04-18 18:05:28 iteration 600/859: loss=0.031222, accuracy=99.060%\n",
      "2018-04-18 18:05:44 iteration 650/859: loss=0.026086, accuracy=99.646%\n",
      "2018-04-18 18:06:01 iteration 700/859: loss=0.051808, accuracy=98.938%\n",
      "2018-04-18 18:06:17 iteration 750/859: loss=0.034958, accuracy=99.170%\n",
      "2018-04-18 18:06:33 iteration 800/859: loss=0.063785, accuracy=98.376%\n",
      "2018-04-18 18:06:50 iteration 850/859: loss=0.036663, accuracy=98.999%\n",
      "2018-04-18 18:06:53 iteration 859/859: loss=0.066027, accuracy=97.461%\n",
      "2018-04-18 18:07:04 end epoch 31/50: acc_train=98.463% acc_val=98.277% acc_test=98.237%\n",
      "2018-04-18 18:07:04 start epoch 32/50, with learning rate = 0.000038\n",
      "2018-04-18 18:07:04 iteration 1/859: loss=0.081695, accuracy=99.072%\n",
      "2018-04-18 18:07:20 iteration 50/859: loss=0.038321, accuracy=99.219%\n",
      "2018-04-18 18:07:37 iteration 100/859: loss=0.039132, accuracy=98.657%\n",
      "2018-04-18 18:07:54 iteration 150/859: loss=0.168696, accuracy=95.496%\n",
      "2018-04-18 18:08:10 iteration 200/859: loss=0.055106, accuracy=99.329%\n",
      "2018-04-18 18:08:27 iteration 250/859: loss=0.074719, accuracy=97.534%\n",
      "2018-04-18 18:08:43 iteration 300/859: loss=0.005417, accuracy=99.634%\n",
      "2018-04-18 18:08:59 iteration 350/859: loss=0.054056, accuracy=98.364%\n",
      "2018-04-18 18:09:16 iteration 400/859: loss=0.123350, accuracy=96.118%\n",
      "2018-04-18 18:09:33 iteration 450/859: loss=0.062957, accuracy=98.547%\n",
      "2018-04-18 18:09:49 iteration 500/859: loss=0.142530, accuracy=96.448%\n",
      "2018-04-18 18:10:06 iteration 550/859: loss=0.022607, accuracy=98.364%\n",
      "2018-04-18 18:10:22 iteration 600/859: loss=0.092617, accuracy=97.668%\n",
      "2018-04-18 18:10:38 iteration 650/859: loss=0.049768, accuracy=98.462%\n",
      "2018-04-18 18:10:55 iteration 700/859: loss=0.124071, accuracy=96.582%\n",
      "2018-04-18 18:11:11 iteration 750/859: loss=0.043879, accuracy=99.451%\n",
      "2018-04-18 18:11:28 iteration 800/859: loss=0.063127, accuracy=97.681%\n",
      "2018-04-18 18:11:44 iteration 850/859: loss=0.069176, accuracy=97.498%\n",
      "2018-04-18 18:11:47 iteration 859/859: loss=0.091300, accuracy=97.107%\n",
      "2018-04-18 18:11:57 end epoch 32/50: acc_train=98.454% acc_val=98.357% acc_test=98.157%\n",
      "2018-04-18 18:11:57 start epoch 33/50, with learning rate = 0.000034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-18 18:11:57 iteration 1/859: loss=0.070875, accuracy=98.633%\n",
      "2018-04-18 18:12:14 iteration 50/859: loss=0.073622, accuracy=98.914%\n",
      "2018-04-18 18:12:30 iteration 100/859: loss=0.028134, accuracy=99.072%\n",
      "2018-04-18 18:12:47 iteration 150/859: loss=0.031103, accuracy=99.072%\n",
      "2018-04-18 18:13:03 iteration 200/859: loss=0.083170, accuracy=98.450%\n",
      "2018-04-18 18:13:20 iteration 250/859: loss=0.043271, accuracy=99.622%\n",
      "2018-04-18 18:13:36 iteration 300/859: loss=0.057028, accuracy=98.499%\n",
      "2018-04-18 18:13:53 iteration 350/859: loss=0.067404, accuracy=98.596%\n",
      "2018-04-18 18:14:09 iteration 400/859: loss=0.059619, accuracy=98.157%\n",
      "2018-04-18 18:14:25 iteration 450/859: loss=0.042940, accuracy=98.938%\n",
      "2018-04-18 18:14:42 iteration 500/859: loss=0.034454, accuracy=99.133%\n",
      "2018-04-18 18:14:58 iteration 550/859: loss=0.046026, accuracy=98.645%\n",
      "2018-04-18 18:15:15 iteration 600/859: loss=0.048839, accuracy=98.975%\n",
      "2018-04-18 18:15:31 iteration 650/859: loss=0.041986, accuracy=99.231%\n",
      "2018-04-18 18:15:47 iteration 700/859: loss=0.079173, accuracy=96.912%\n",
      "2018-04-18 18:16:03 iteration 750/859: loss=0.085588, accuracy=97.437%\n",
      "2018-04-18 18:16:20 iteration 800/859: loss=0.058023, accuracy=98.511%\n",
      "2018-04-18 18:16:36 iteration 850/859: loss=0.058521, accuracy=98.584%\n",
      "2018-04-18 18:16:39 iteration 859/859: loss=0.000572, accuracy=99.524%\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-18 18:16:49 end epoch 33/50: acc_train=98.568% acc_val=98.478% acc_test=98.377%\n",
      "2018-04-18 18:16:49 start epoch 34/50, with learning rate = 0.000031\n",
      "2018-04-18 18:16:50 iteration 1/859: loss=0.078682, accuracy=97.998%\n",
      "2018-04-18 18:17:05 iteration 50/859: loss=0.074963, accuracy=97.778%\n",
      "2018-04-18 18:17:22 iteration 100/859: loss=0.027307, accuracy=98.645%\n",
      "2018-04-18 18:17:38 iteration 150/859: loss=0.063336, accuracy=98.425%\n",
      "2018-04-18 18:17:54 iteration 200/859: loss=0.036052, accuracy=98.059%\n",
      "2018-04-18 18:18:11 iteration 250/859: loss=0.011071, accuracy=99.402%\n",
      "2018-04-18 18:18:27 iteration 300/859: loss=0.034950, accuracy=99.146%\n",
      "2018-04-18 18:18:44 iteration 350/859: loss=0.051432, accuracy=99.060%\n",
      "2018-04-18 18:19:00 iteration 400/859: loss=0.056892, accuracy=98.047%\n",
      "2018-04-18 18:19:17 iteration 450/859: loss=0.059300, accuracy=98.474%\n",
      "2018-04-18 18:19:33 iteration 500/859: loss=0.055685, accuracy=98.145%\n",
      "2018-04-18 18:19:50 iteration 550/859: loss=0.035200, accuracy=99.072%\n",
      "2018-04-18 18:20:06 iteration 600/859: loss=0.023305, accuracy=98.694%\n",
      "2018-04-18 18:20:23 iteration 650/859: loss=0.099651, accuracy=97.302%\n",
      "2018-04-18 18:20:39 iteration 700/859: loss=0.196315, accuracy=97.839%\n",
      "2018-04-18 18:20:56 iteration 750/859: loss=0.038389, accuracy=99.146%\n",
      "2018-04-18 18:21:12 iteration 800/859: loss=0.047694, accuracy=98.169%\n",
      "2018-04-18 18:21:29 iteration 850/859: loss=0.032200, accuracy=99.683%\n",
      "2018-04-18 18:21:32 iteration 859/859: loss=0.037505, accuracy=98.755%\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-18 18:21:43 end epoch 34/50: acc_train=98.652% acc_val=98.658% acc_test=98.327%\n",
      "2018-04-18 18:21:43 start epoch 35/50, with learning rate = 0.000028\n",
      "2018-04-18 18:21:43 iteration 1/859: loss=0.118230, accuracy=96.948%\n",
      "2018-04-18 18:21:59 iteration 50/859: loss=0.067234, accuracy=99.377%\n",
      "2018-04-18 18:22:16 iteration 100/859: loss=0.094255, accuracy=98.242%\n",
      "2018-04-18 18:22:32 iteration 150/859: loss=0.032295, accuracy=98.950%\n",
      "2018-04-18 18:22:49 iteration 200/859: loss=0.022204, accuracy=98.975%\n",
      "2018-04-18 18:23:05 iteration 250/859: loss=0.056006, accuracy=98.828%\n",
      "2018-04-18 18:23:21 iteration 300/859: loss=0.081080, accuracy=97.815%\n",
      "2018-04-18 18:23:38 iteration 350/859: loss=0.038079, accuracy=97.791%\n",
      "2018-04-18 18:23:54 iteration 400/859: loss=0.066547, accuracy=98.389%\n",
      "2018-04-18 18:24:11 iteration 450/859: loss=0.045795, accuracy=99.719%\n",
      "2018-04-18 18:24:27 iteration 500/859: loss=0.045716, accuracy=99.158%\n",
      "2018-04-18 18:24:44 iteration 550/859: loss=0.056323, accuracy=98.840%\n",
      "2018-04-18 18:25:00 iteration 600/859: loss=0.014078, accuracy=98.853%\n",
      "2018-04-18 18:25:16 iteration 650/859: loss=0.116481, accuracy=97.412%\n",
      "2018-04-18 18:25:33 iteration 700/859: loss=0.049275, accuracy=98.523%\n",
      "2018-04-18 18:25:49 iteration 750/859: loss=0.026749, accuracy=98.962%\n",
      "2018-04-18 18:26:06 iteration 800/859: loss=0.052252, accuracy=98.132%\n",
      "2018-04-18 18:26:23 iteration 850/859: loss=0.059108, accuracy=98.291%\n",
      "2018-04-18 18:26:26 iteration 859/859: loss=0.016693, accuracy=98.779%\n",
      "2018-04-18 18:26:36 end epoch 35/50: acc_train=98.641% acc_val=98.598% acc_test=98.538%\n",
      "2018-04-18 18:26:36 start epoch 36/50, with learning rate = 0.000025\n",
      "2018-04-18 18:26:37 iteration 1/859: loss=0.051338, accuracy=99.512%\n",
      "2018-04-18 18:26:53 iteration 50/859: loss=0.047549, accuracy=98.499%\n",
      "2018-04-18 18:27:09 iteration 100/859: loss=0.058309, accuracy=98.206%\n",
      "2018-04-18 18:27:26 iteration 150/859: loss=0.059387, accuracy=98.315%\n",
      "2018-04-18 18:27:42 iteration 200/859: loss=0.046956, accuracy=99.268%\n",
      "2018-04-18 18:27:58 iteration 250/859: loss=0.009881, accuracy=98.901%\n",
      "2018-04-18 18:28:15 iteration 300/859: loss=0.112524, accuracy=97.278%\n",
      "2018-04-18 18:28:31 iteration 350/859: loss=0.086545, accuracy=98.645%\n",
      "2018-04-18 18:28:48 iteration 400/859: loss=0.018618, accuracy=99.146%\n",
      "2018-04-18 18:29:04 iteration 450/859: loss=0.045032, accuracy=98.792%\n",
      "2018-04-18 18:29:20 iteration 500/859: loss=0.010975, accuracy=99.023%\n",
      "2018-04-18 18:29:37 iteration 550/859: loss=0.057089, accuracy=98.254%\n",
      "2018-04-18 18:29:53 iteration 600/859: loss=0.009194, accuracy=99.438%\n",
      "2018-04-18 18:30:10 iteration 650/859: loss=-0.005014, accuracy=99.597%\n",
      "2018-04-18 18:30:26 iteration 700/859: loss=0.035942, accuracy=99.243%\n",
      "2018-04-18 18:30:42 iteration 750/859: loss=0.066039, accuracy=98.938%\n",
      "2018-04-18 18:30:59 iteration 800/859: loss=0.122948, accuracy=95.605%\n",
      "2018-04-18 18:31:15 iteration 850/859: loss=0.012915, accuracy=99.512%\n",
      "2018-04-18 18:31:18 iteration 859/859: loss=0.071971, accuracy=96.948%\n",
      "2018-04-18 18:31:28 end epoch 36/50: acc_train=98.514% acc_val=98.498% acc_test=98.277%\n",
      "2018-04-18 18:31:28 start epoch 37/50, with learning rate = 0.000023\n",
      "2018-04-18 18:31:29 iteration 1/859: loss=0.105669, accuracy=97.913%\n",
      "2018-04-18 18:31:45 iteration 50/859: loss=0.046960, accuracy=98.071%\n",
      "2018-04-18 18:32:01 iteration 100/859: loss=0.061265, accuracy=98.438%\n",
      "2018-04-18 18:32:17 iteration 150/859: loss=0.063512, accuracy=97.644%\n",
      "2018-04-18 18:32:34 iteration 200/859: loss=0.115158, accuracy=98.059%\n",
      "2018-04-18 18:32:50 iteration 250/859: loss=0.006504, accuracy=99.158%\n",
      "2018-04-18 18:33:06 iteration 300/859: loss=0.023474, accuracy=98.889%\n",
      "2018-04-18 18:33:23 iteration 350/859: loss=0.076966, accuracy=97.229%\n",
      "2018-04-18 18:33:39 iteration 400/859: loss=0.071146, accuracy=97.656%\n",
      "2018-04-18 18:33:55 iteration 450/859: loss=0.037767, accuracy=98.853%\n",
      "2018-04-18 18:34:12 iteration 500/859: loss=0.028133, accuracy=98.804%\n",
      "2018-04-18 18:34:28 iteration 550/859: loss=0.047636, accuracy=98.938%\n",
      "2018-04-18 18:34:45 iteration 600/859: loss=0.008548, accuracy=98.950%\n",
      "2018-04-18 18:35:01 iteration 650/859: loss=0.061804, accuracy=98.035%\n",
      "2018-04-18 18:35:18 iteration 700/859: loss=0.023023, accuracy=99.622%\n",
      "2018-04-18 18:35:34 iteration 750/859: loss=0.075510, accuracy=98.523%\n",
      "2018-04-18 18:35:50 iteration 800/859: loss=0.059252, accuracy=99.524%\n",
      "2018-04-18 18:36:07 iteration 850/859: loss=0.063851, accuracy=98.804%\n",
      "2018-04-18 18:36:10 iteration 859/859: loss=0.056525, accuracy=98.828%\n",
      "2018-04-18 18:36:21 end epoch 37/50: acc_train=98.679% acc_val=98.658% acc_test=98.458%\n",
      "2018-04-18 18:36:21 start epoch 38/50, with learning rate = 0.000020\n",
      "2018-04-18 18:36:21 iteration 1/859: loss=0.041949, accuracy=98.413%\n",
      "2018-04-18 18:36:37 iteration 50/859: loss=0.047767, accuracy=97.925%\n",
      "2018-04-18 18:36:54 iteration 100/859: loss=0.020273, accuracy=98.328%\n",
      "2018-04-18 18:37:10 iteration 150/859: loss=0.031830, accuracy=99.023%\n",
      "2018-04-18 18:37:27 iteration 200/859: loss=0.081603, accuracy=97.913%\n",
      "2018-04-18 18:37:43 iteration 250/859: loss=0.053992, accuracy=98.926%\n",
      "2018-04-18 18:38:00 iteration 300/859: loss=0.027889, accuracy=99.805%\n",
      "2018-04-18 18:38:16 iteration 350/859: loss=0.051382, accuracy=98.975%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-18 18:38:33 iteration 400/859: loss=0.066264, accuracy=98.389%\n",
      "2018-04-18 18:38:49 iteration 450/859: loss=0.016403, accuracy=99.390%\n",
      "2018-04-18 18:39:06 iteration 500/859: loss=0.065929, accuracy=99.329%\n",
      "2018-04-18 18:39:22 iteration 550/859: loss=0.015156, accuracy=99.231%\n",
      "2018-04-18 18:39:38 iteration 600/859: loss=0.051864, accuracy=98.767%\n",
      "2018-04-18 18:39:55 iteration 650/859: loss=0.081653, accuracy=98.108%\n",
      "2018-04-18 18:40:12 iteration 700/859: loss=0.058491, accuracy=98.682%\n",
      "2018-04-18 18:40:28 iteration 750/859: loss=0.038480, accuracy=99.231%\n",
      "2018-04-18 18:40:45 iteration 800/859: loss=0.046363, accuracy=98.975%\n",
      "2018-04-18 18:41:01 iteration 850/859: loss=0.059827, accuracy=99.146%\n",
      "2018-04-18 18:41:04 iteration 859/859: loss=0.036233, accuracy=98.779%\n",
      "2018-04-18 18:41:14 end epoch 38/50: acc_train=98.612% acc_val=98.417% acc_test=98.468%\n",
      "2018-04-18 18:41:14 start epoch 39/50, with learning rate = 0.000018\n",
      "2018-04-18 18:41:15 iteration 1/859: loss=0.123071, accuracy=96.143%\n",
      "2018-04-18 18:41:31 iteration 50/859: loss=0.050501, accuracy=99.243%\n",
      "2018-04-18 18:41:47 iteration 100/859: loss=0.040616, accuracy=99.219%\n",
      "2018-04-18 18:42:03 iteration 150/859: loss=0.046503, accuracy=99.414%\n",
      "2018-04-18 18:42:20 iteration 200/859: loss=0.009483, accuracy=98.865%\n",
      "2018-04-18 18:42:36 iteration 250/859: loss=0.073053, accuracy=97.461%\n",
      "2018-04-18 18:42:53 iteration 300/859: loss=0.032653, accuracy=99.084%\n",
      "2018-04-18 18:43:09 iteration 350/859: loss=0.024926, accuracy=99.243%\n",
      "2018-04-18 18:43:25 iteration 400/859: loss=0.027068, accuracy=99.353%\n",
      "2018-04-18 18:43:42 iteration 450/859: loss=0.035858, accuracy=98.413%\n",
      "2018-04-18 18:43:58 iteration 500/859: loss=0.071753, accuracy=99.072%\n",
      "2018-04-18 18:44:15 iteration 550/859: loss=0.046243, accuracy=98.743%\n",
      "2018-04-18 18:44:31 iteration 600/859: loss=0.051165, accuracy=99.036%\n",
      "2018-04-18 18:44:47 iteration 650/859: loss=0.040124, accuracy=99.390%\n",
      "2018-04-18 18:45:03 iteration 700/859: loss=0.041589, accuracy=99.463%\n",
      "2018-04-18 18:45:20 iteration 750/859: loss=0.137818, accuracy=96.558%\n",
      "2018-04-18 18:45:36 iteration 800/859: loss=0.011682, accuracy=99.170%\n",
      "2018-04-18 18:45:53 iteration 850/859: loss=0.033957, accuracy=98.779%\n",
      "2018-04-18 18:45:56 iteration 859/859: loss=0.052379, accuracy=98.315%\n",
      "2018-04-18 18:46:07 end epoch 39/50: acc_train=98.769% acc_val=98.618% acc_test=98.568%\n",
      "2018-04-18 18:46:07 start epoch 40/50, with learning rate = 0.000016\n",
      "2018-04-18 18:46:07 iteration 1/859: loss=0.094317, accuracy=97.864%\n",
      "2018-04-18 18:46:23 iteration 50/859: loss=0.046407, accuracy=99.072%\n",
      "2018-04-18 18:46:40 iteration 100/859: loss=0.018335, accuracy=98.987%\n",
      "2018-04-18 18:46:56 iteration 150/859: loss=0.054736, accuracy=98.376%\n",
      "2018-04-18 18:47:13 iteration 200/859: loss=0.019286, accuracy=98.999%\n",
      "2018-04-18 18:47:29 iteration 250/859: loss=0.036837, accuracy=99.329%\n",
      "2018-04-18 18:47:46 iteration 300/859: loss=0.061969, accuracy=98.840%\n",
      "2018-04-18 18:48:03 iteration 350/859: loss=0.040681, accuracy=99.194%\n",
      "2018-04-18 18:48:19 iteration 400/859: loss=0.052609, accuracy=98.706%\n",
      "2018-04-18 18:48:35 iteration 450/859: loss=0.023152, accuracy=98.596%\n",
      "2018-04-18 18:48:52 iteration 500/859: loss=0.083992, accuracy=97.375%\n",
      "2018-04-18 18:49:08 iteration 550/859: loss=0.084829, accuracy=98.633%\n",
      "2018-04-18 18:49:25 iteration 600/859: loss=0.071106, accuracy=97.778%\n",
      "2018-04-18 18:49:41 iteration 650/859: loss=0.082377, accuracy=98.145%\n",
      "2018-04-18 18:49:57 iteration 700/859: loss=0.036268, accuracy=99.329%\n",
      "2018-04-18 18:50:14 iteration 750/859: loss=0.048292, accuracy=99.268%\n",
      "2018-04-18 18:50:30 iteration 800/859: loss=0.070060, accuracy=98.633%\n",
      "2018-04-18 18:50:47 iteration 850/859: loss=0.062235, accuracy=98.328%\n",
      "2018-04-18 18:50:50 iteration 859/859: loss=0.007525, accuracy=99.377%\n",
      "2018-04-18 18:51:00 end epoch 40/50: acc_train=98.698% acc_val=98.558% acc_test=98.488%\n",
      "2018-04-18 18:51:00 start epoch 41/50, with learning rate = 0.000015\n",
      "2018-04-18 18:51:01 iteration 1/859: loss=0.055502, accuracy=98.608%\n",
      "2018-04-18 18:51:17 iteration 50/859: loss=0.041937, accuracy=99.524%\n",
      "2018-04-18 18:51:34 iteration 100/859: loss=0.044433, accuracy=99.243%\n",
      "2018-04-18 18:51:50 iteration 150/859: loss=0.038869, accuracy=98.743%\n",
      "2018-04-18 18:52:07 iteration 200/859: loss=0.023347, accuracy=98.950%\n",
      "2018-04-18 18:52:23 iteration 250/859: loss=0.034120, accuracy=99.084%\n",
      "2018-04-18 18:52:40 iteration 300/859: loss=0.091595, accuracy=97.839%\n",
      "2018-04-18 18:52:56 iteration 350/859: loss=-0.000067, accuracy=99.548%\n",
      "2018-04-18 18:53:13 iteration 400/859: loss=0.072702, accuracy=97.644%\n",
      "2018-04-18 18:53:30 iteration 450/859: loss=0.039152, accuracy=99.097%\n",
      "2018-04-18 18:53:46 iteration 500/859: loss=0.057079, accuracy=98.315%\n",
      "2018-04-18 18:54:02 iteration 550/859: loss=0.054453, accuracy=99.548%\n",
      "2018-04-18 18:54:19 iteration 600/859: loss=0.046415, accuracy=99.377%\n",
      "2018-04-18 18:54:35 iteration 650/859: loss=0.053551, accuracy=99.341%\n",
      "2018-04-18 18:54:52 iteration 700/859: loss=0.045553, accuracy=99.377%\n",
      "2018-04-18 18:55:08 iteration 750/859: loss=0.110363, accuracy=97.021%\n",
      "2018-04-18 18:55:25 iteration 800/859: loss=0.040069, accuracy=99.182%\n",
      "2018-04-18 18:55:41 iteration 850/859: loss=0.010231, accuracy=99.207%\n",
      "2018-04-18 18:55:44 iteration 859/859: loss=0.058870, accuracy=98.621%\n",
      "2018-04-18 18:55:55 end epoch 41/50: acc_train=98.665% acc_val=98.417% acc_test=98.548%\n",
      "2018-04-18 18:55:55 start epoch 42/50, with learning rate = 0.000013\n",
      "2018-04-18 18:55:55 iteration 1/859: loss=0.041763, accuracy=98.645%\n",
      "2018-04-18 18:56:12 iteration 50/859: loss=0.048265, accuracy=99.243%\n",
      "2018-04-18 18:56:28 iteration 100/859: loss=0.061655, accuracy=98.877%\n",
      "2018-04-18 18:56:45 iteration 150/859: loss=0.047837, accuracy=99.072%\n",
      "2018-04-18 18:57:01 iteration 200/859: loss=0.055084, accuracy=99.255%\n",
      "2018-04-18 18:57:17 iteration 250/859: loss=0.122283, accuracy=96.204%\n",
      "2018-04-18 18:57:34 iteration 300/859: loss=0.074730, accuracy=97.913%\n",
      "2018-04-18 18:57:50 iteration 350/859: loss=0.062371, accuracy=96.875%\n",
      "2018-04-18 18:58:07 iteration 400/859: loss=0.033900, accuracy=98.730%\n",
      "2018-04-18 18:58:23 iteration 450/859: loss=0.031562, accuracy=99.243%\n",
      "2018-04-18 18:58:40 iteration 500/859: loss=0.063684, accuracy=98.486%\n",
      "2018-04-18 18:58:56 iteration 550/859: loss=0.044189, accuracy=98.462%\n",
      "2018-04-18 18:59:13 iteration 600/859: loss=0.043819, accuracy=99.207%\n",
      "2018-04-18 18:59:29 iteration 650/859: loss=0.046347, accuracy=98.254%\n",
      "2018-04-18 18:59:45 iteration 700/859: loss=0.030569, accuracy=99.438%\n",
      "2018-04-18 19:00:02 iteration 750/859: loss=0.011034, accuracy=98.718%\n",
      "2018-04-18 19:00:18 iteration 800/859: loss=0.042545, accuracy=98.682%\n",
      "2018-04-18 19:00:35 iteration 850/859: loss=0.050862, accuracy=99.585%\n",
      "2018-04-18 19:00:38 iteration 859/859: loss=0.062028, accuracy=98.303%\n",
      "2018-04-18 19:00:48 end epoch 42/50: acc_train=98.794% acc_val=98.377% acc_test=98.197%\n",
      "2018-04-18 19:00:48 start epoch 43/50, with learning rate = 0.000012\n",
      "2018-04-18 19:00:49 iteration 1/859: loss=0.037442, accuracy=99.390%\n",
      "2018-04-18 19:01:05 iteration 50/859: loss=0.057344, accuracy=98.987%\n",
      "2018-04-18 19:01:21 iteration 100/859: loss=0.039565, accuracy=99.426%\n",
      "2018-04-18 19:01:38 iteration 150/859: loss=0.030534, accuracy=99.243%\n",
      "2018-04-18 19:01:54 iteration 200/859: loss=0.068290, accuracy=98.755%\n",
      "2018-04-18 19:02:10 iteration 250/859: loss=0.049659, accuracy=99.158%\n",
      "2018-04-18 19:02:27 iteration 300/859: loss=0.027220, accuracy=99.280%\n",
      "2018-04-18 19:02:43 iteration 350/859: loss=0.094765, accuracy=98.083%\n",
      "2018-04-18 19:02:59 iteration 400/859: loss=0.035214, accuracy=99.426%\n",
      "2018-04-18 19:03:16 iteration 450/859: loss=0.093606, accuracy=97.388%\n",
      "2018-04-18 19:03:32 iteration 500/859: loss=0.048573, accuracy=99.268%\n",
      "2018-04-18 19:03:48 iteration 550/859: loss=0.034559, accuracy=99.170%\n",
      "2018-04-18 19:04:04 iteration 600/859: loss=0.065114, accuracy=98.376%\n",
      "2018-04-18 19:04:21 iteration 650/859: loss=0.019601, accuracy=99.622%\n",
      "2018-04-18 19:04:37 iteration 700/859: loss=0.089836, accuracy=96.399%\n",
      "2018-04-18 19:04:54 iteration 750/859: loss=0.046643, accuracy=99.646%\n",
      "2018-04-18 19:05:10 iteration 800/859: loss=0.112636, accuracy=98.083%\n",
      "2018-04-18 19:05:26 iteration 850/859: loss=0.025346, accuracy=99.097%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-18 19:05:29 iteration 859/859: loss=0.130881, accuracy=96.887%\n",
      "2018-04-18 19:05:40 end epoch 43/50: acc_train=98.438% acc_val=98.197% acc_test=98.357%\n",
      "2018-04-18 19:05:40 start epoch 44/50, with learning rate = 0.000011\n",
      "2018-04-18 19:05:40 iteration 1/859: loss=0.060039, accuracy=98.254%\n",
      "2018-04-18 19:05:56 iteration 50/859: loss=0.024969, accuracy=98.779%\n",
      "2018-04-18 19:06:13 iteration 100/859: loss=0.055566, accuracy=99.207%\n",
      "2018-04-18 19:06:29 iteration 150/859: loss=0.043700, accuracy=99.243%\n",
      "2018-04-18 19:06:46 iteration 200/859: loss=0.064969, accuracy=97.888%\n",
      "2018-04-18 19:07:02 iteration 250/859: loss=0.017977, accuracy=99.475%\n",
      "2018-04-18 19:07:18 iteration 300/859: loss=0.038747, accuracy=98.511%\n",
      "2018-04-18 19:07:35 iteration 350/859: loss=0.081434, accuracy=97.290%\n",
      "2018-04-18 19:07:51 iteration 400/859: loss=0.003693, accuracy=99.219%\n",
      "2018-04-18 19:08:08 iteration 450/859: loss=0.030837, accuracy=99.170%\n",
      "2018-04-18 19:08:24 iteration 500/859: loss=0.034554, accuracy=99.353%\n",
      "2018-04-18 19:08:41 iteration 550/859: loss=0.059976, accuracy=99.060%\n",
      "2018-04-18 19:08:57 iteration 600/859: loss=0.082612, accuracy=97.839%\n",
      "2018-04-18 19:09:13 iteration 650/859: loss=0.012328, accuracy=99.744%\n",
      "2018-04-18 19:09:30 iteration 700/859: loss=0.087245, accuracy=98.633%\n",
      "2018-04-18 19:09:46 iteration 750/859: loss=0.007095, accuracy=98.889%\n",
      "2018-04-18 19:10:02 iteration 800/859: loss=0.090215, accuracy=97.571%\n",
      "2018-04-18 19:10:19 iteration 850/859: loss=0.041958, accuracy=98.291%\n",
      "2018-04-18 19:10:22 iteration 859/859: loss=0.041986, accuracy=99.487%\n",
      "2018-04-18 19:10:32 end epoch 44/50: acc_train=98.883% acc_val=98.578% acc_test=98.438%\n",
      "2018-04-18 19:10:32 start epoch 45/50, with learning rate = 0.000010\n",
      "2018-04-18 19:10:32 iteration 1/859: loss=0.016931, accuracy=98.608%\n",
      "2018-04-18 19:10:49 iteration 50/859: loss=0.032202, accuracy=99.268%\n",
      "2018-04-18 19:11:05 iteration 100/859: loss=0.023226, accuracy=98.767%\n",
      "2018-04-18 19:11:22 iteration 150/859: loss=0.138982, accuracy=95.569%\n",
      "2018-04-18 19:11:38 iteration 200/859: loss=0.013406, accuracy=99.377%\n",
      "2018-04-18 19:11:54 iteration 250/859: loss=0.036994, accuracy=99.219%\n",
      "2018-04-18 19:12:11 iteration 300/859: loss=0.056277, accuracy=98.511%\n",
      "2018-04-18 19:12:27 iteration 350/859: loss=0.039791, accuracy=99.817%\n",
      "2018-04-18 19:12:43 iteration 400/859: loss=0.034411, accuracy=99.158%\n",
      "2018-04-18 19:13:00 iteration 450/859: loss=0.050686, accuracy=98.901%\n",
      "2018-04-18 19:13:16 iteration 500/859: loss=0.022370, accuracy=99.438%\n",
      "2018-04-18 19:13:33 iteration 550/859: loss=0.000859, accuracy=99.402%\n",
      "2018-04-18 19:13:50 iteration 600/859: loss=0.050683, accuracy=98.914%\n",
      "2018-04-18 19:14:06 iteration 650/859: loss=0.099293, accuracy=97.156%\n",
      "2018-04-18 19:14:22 iteration 700/859: loss=0.017038, accuracy=99.536%\n",
      "2018-04-18 19:14:39 iteration 750/859: loss=0.150987, accuracy=96.619%\n",
      "2018-04-18 19:14:55 iteration 800/859: loss=0.073420, accuracy=98.767%\n",
      "2018-04-18 19:15:11 iteration 850/859: loss=0.038257, accuracy=99.072%\n",
      "2018-04-18 19:15:14 iteration 859/859: loss=0.071014, accuracy=98.645%\n",
      "2018-04-18 19:15:25 end epoch 45/50: acc_train=98.823% acc_val=98.558% acc_test=98.427%\n",
      "2018-04-18 19:15:25 start epoch 46/50, with learning rate = 0.000009\n",
      "2018-04-18 19:15:25 iteration 1/859: loss=0.034343, accuracy=98.523%\n",
      "2018-04-18 19:15:41 iteration 50/859: loss=0.005809, accuracy=99.670%\n",
      "2018-04-18 19:15:58 iteration 100/859: loss=0.036400, accuracy=98.218%\n",
      "2018-04-18 19:16:14 iteration 150/859: loss=0.016773, accuracy=99.670%\n",
      "2018-04-18 19:16:30 iteration 200/859: loss=0.073730, accuracy=98.279%\n",
      "2018-04-18 19:16:47 iteration 250/859: loss=0.044403, accuracy=98.047%\n",
      "2018-04-18 19:17:03 iteration 300/859: loss=0.026207, accuracy=98.694%\n",
      "2018-04-18 19:17:20 iteration 350/859: loss=0.108843, accuracy=97.437%\n",
      "2018-04-18 19:17:36 iteration 400/859: loss=-0.005623, accuracy=99.377%\n",
      "2018-04-18 19:17:53 iteration 450/859: loss=0.004303, accuracy=99.524%\n",
      "2018-04-18 19:18:09 iteration 500/859: loss=0.025265, accuracy=98.987%\n",
      "2018-04-18 19:18:26 iteration 550/859: loss=0.031736, accuracy=99.255%\n",
      "2018-04-18 19:18:42 iteration 600/859: loss=0.100836, accuracy=97.839%\n",
      "2018-04-18 19:18:58 iteration 650/859: loss=0.057468, accuracy=99.268%\n",
      "2018-04-18 19:19:14 iteration 700/859: loss=0.020446, accuracy=99.158%\n",
      "2018-04-18 19:19:30 iteration 750/859: loss=0.009972, accuracy=99.548%\n",
      "2018-04-18 19:19:47 iteration 800/859: loss=0.044450, accuracy=98.523%\n",
      "2018-04-18 19:20:03 iteration 850/859: loss=0.032888, accuracy=98.303%\n",
      "2018-04-18 19:20:06 iteration 859/859: loss=0.075119, accuracy=98.865%\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-18 19:20:17 end epoch 46/50: acc_train=98.669% acc_val=98.718% acc_test=98.317%\n",
      "2018-04-18 19:20:17 start epoch 47/50, with learning rate = 0.000008\n",
      "2018-04-18 19:20:17 iteration 1/859: loss=0.062939, accuracy=97.241%\n",
      "2018-04-18 19:20:34 iteration 50/859: loss=0.031358, accuracy=98.572%\n",
      "2018-04-18 19:20:50 iteration 100/859: loss=0.035866, accuracy=99.158%\n",
      "2018-04-18 19:21:06 iteration 150/859: loss=0.025154, accuracy=99.268%\n",
      "2018-04-18 19:21:22 iteration 200/859: loss=0.091369, accuracy=97.949%\n",
      "2018-04-18 19:21:39 iteration 250/859: loss=0.030761, accuracy=99.805%\n",
      "2018-04-18 19:21:55 iteration 300/859: loss=0.051087, accuracy=98.926%\n",
      "2018-04-18 19:22:12 iteration 350/859: loss=0.032222, accuracy=99.512%\n",
      "2018-04-18 19:22:28 iteration 400/859: loss=0.052236, accuracy=98.242%\n",
      "2018-04-18 19:22:44 iteration 450/859: loss=0.072076, accuracy=98.730%\n",
      "2018-04-18 19:23:01 iteration 500/859: loss=0.042995, accuracy=99.121%\n",
      "2018-04-18 19:23:17 iteration 550/859: loss=0.067609, accuracy=98.914%\n",
      "2018-04-18 19:23:34 iteration 600/859: loss=0.008608, accuracy=98.962%\n",
      "2018-04-18 19:23:50 iteration 650/859: loss=0.029938, accuracy=99.719%\n",
      "2018-04-18 19:24:07 iteration 700/859: loss=0.056773, accuracy=98.889%\n",
      "2018-04-18 19:24:23 iteration 750/859: loss=0.031327, accuracy=98.840%\n",
      "2018-04-18 19:24:40 iteration 800/859: loss=0.035130, accuracy=99.292%\n",
      "2018-04-18 19:24:56 iteration 850/859: loss=0.074019, accuracy=98.767%\n",
      "2018-04-18 19:24:59 iteration 859/859: loss=0.042651, accuracy=99.121%\n",
      "2018-04-18 19:25:10 end epoch 47/50: acc_train=98.743% acc_val=98.658% acc_test=98.558%\n",
      "2018-04-18 19:25:10 start epoch 48/50, with learning rate = 0.000007\n",
      "2018-04-18 19:25:10 iteration 1/859: loss=0.016887, accuracy=99.268%\n",
      "2018-04-18 19:25:27 iteration 50/859: loss=0.006538, accuracy=99.744%\n",
      "2018-04-18 19:25:43 iteration 100/859: loss=0.011746, accuracy=99.084%\n",
      "2018-04-18 19:25:59 iteration 150/859: loss=0.091298, accuracy=97.485%\n",
      "2018-04-18 19:26:15 iteration 200/859: loss=0.038592, accuracy=98.987%\n",
      "2018-04-18 19:26:31 iteration 250/859: loss=0.010218, accuracy=99.512%\n",
      "2018-04-18 19:26:48 iteration 300/859: loss=0.035406, accuracy=98.816%\n",
      "2018-04-18 19:27:04 iteration 350/859: loss=0.057126, accuracy=99.414%\n",
      "2018-04-18 19:27:20 iteration 400/859: loss=0.053272, accuracy=98.865%\n",
      "2018-04-18 19:27:37 iteration 450/859: loss=0.034899, accuracy=98.657%\n",
      "2018-04-18 19:27:53 iteration 500/859: loss=0.028770, accuracy=99.634%\n",
      "2018-04-18 19:28:09 iteration 550/859: loss=0.043621, accuracy=98.926%\n",
      "2018-04-18 19:28:26 iteration 600/859: loss=0.055376, accuracy=98.914%\n",
      "2018-04-18 19:28:42 iteration 650/859: loss=0.033681, accuracy=99.133%\n",
      "2018-04-18 19:28:58 iteration 700/859: loss=0.036869, accuracy=99.695%\n",
      "2018-04-18 19:29:15 iteration 750/859: loss=0.042641, accuracy=99.048%\n",
      "2018-04-18 19:29:31 iteration 800/859: loss=0.079443, accuracy=98.718%\n",
      "2018-04-18 19:29:48 iteration 850/859: loss=0.058885, accuracy=98.755%\n",
      "2018-04-18 19:29:51 iteration 859/859: loss=0.024631, accuracy=99.182%\n",
      "2018-04-18 19:30:01 end epoch 48/50: acc_train=98.819% acc_val=98.518% acc_test=98.528%\n",
      "2018-04-18 19:30:01 start epoch 49/50, with learning rate = 0.000006\n",
      "2018-04-18 19:30:01 iteration 1/859: loss=0.045853, accuracy=98.950%\n",
      "2018-04-18 19:30:17 iteration 50/859: loss=-0.008752, accuracy=99.561%\n",
      "2018-04-18 19:30:34 iteration 100/859: loss=0.035303, accuracy=98.779%\n",
      "2018-04-18 19:30:50 iteration 150/859: loss=0.030408, accuracy=99.121%\n",
      "2018-04-18 19:31:06 iteration 200/859: loss=0.074023, accuracy=98.633%\n",
      "2018-04-18 19:31:23 iteration 250/859: loss=0.046464, accuracy=98.450%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-18 19:31:39 iteration 300/859: loss=0.004340, accuracy=99.146%\n",
      "2018-04-18 19:31:55 iteration 350/859: loss=0.049514, accuracy=99.377%\n",
      "2018-04-18 19:32:12 iteration 400/859: loss=0.044098, accuracy=98.621%\n",
      "2018-04-18 19:32:28 iteration 450/859: loss=0.031705, accuracy=98.865%\n",
      "2018-04-18 19:32:44 iteration 500/859: loss=0.110511, accuracy=97.339%\n",
      "2018-04-18 19:33:01 iteration 550/859: loss=0.000900, accuracy=99.072%\n",
      "2018-04-18 19:33:18 iteration 600/859: loss=0.088500, accuracy=98.047%\n",
      "2018-04-18 19:33:34 iteration 650/859: loss=0.120836, accuracy=97.192%\n",
      "2018-04-18 19:33:50 iteration 700/859: loss=0.052625, accuracy=97.961%\n",
      "2018-04-18 19:34:06 iteration 750/859: loss=0.035634, accuracy=98.511%\n",
      "2018-04-18 19:34:22 iteration 800/859: loss=0.047373, accuracy=99.561%\n",
      "2018-04-18 19:34:39 iteration 850/859: loss=0.044413, accuracy=99.023%\n",
      "2018-04-18 19:34:42 iteration 859/859: loss=0.049370, accuracy=99.353%\n",
      "2018-04-18 19:34:52 end epoch 49/50: acc_train=98.812% acc_val=98.578% acc_test=98.668%\n",
      "2018-04-18 19:34:52 start epoch 50/50, with learning rate = 0.000006\n",
      "2018-04-18 19:34:52 iteration 1/859: loss=-0.010339, accuracy=99.695%\n",
      "2018-04-18 19:35:08 iteration 50/859: loss=0.009678, accuracy=99.377%\n",
      "2018-04-18 19:35:24 iteration 100/859: loss=0.020746, accuracy=99.353%\n",
      "2018-04-18 19:35:41 iteration 150/859: loss=0.120239, accuracy=96.545%\n",
      "2018-04-18 19:35:58 iteration 200/859: loss=0.020111, accuracy=99.036%\n",
      "2018-04-18 19:36:14 iteration 250/859: loss=0.037971, accuracy=99.194%\n",
      "2018-04-18 19:36:30 iteration 300/859: loss=0.063880, accuracy=98.340%\n",
      "2018-04-18 19:36:46 iteration 350/859: loss=-0.000326, accuracy=99.646%\n",
      "2018-04-18 19:37:02 iteration 400/859: loss=0.062989, accuracy=98.596%\n",
      "2018-04-18 19:37:19 iteration 450/859: loss=0.037822, accuracy=98.828%\n",
      "2018-04-18 19:37:35 iteration 500/859: loss=0.043092, accuracy=99.390%\n",
      "2018-04-18 19:37:51 iteration 550/859: loss=0.034246, accuracy=98.901%\n",
      "2018-04-18 19:38:08 iteration 600/859: loss=0.040549, accuracy=98.816%\n",
      "2018-04-18 19:38:24 iteration 650/859: loss=0.093650, accuracy=97.192%\n",
      "2018-04-18 19:38:41 iteration 700/859: loss=0.047267, accuracy=98.572%\n",
      "2018-04-18 19:38:57 iteration 750/859: loss=0.039133, accuracy=98.340%\n",
      "2018-04-18 19:39:14 iteration 800/859: loss=0.012282, accuracy=99.207%\n",
      "2018-04-18 19:39:30 iteration 850/859: loss=0.004357, accuracy=99.475%\n",
      "2018-04-18 19:39:33 iteration 859/859: loss=0.010601, accuracy=99.500%\n",
      "2018-04-18 19:39:44 end epoch 50/50: acc_train=98.850% acc_val=98.618% acc_test=98.458%\n"
     ]
    }
   ],
   "source": [
    "acc_train_his=[]\n",
    "acc_val_his=[]\n",
    "acc_test_his=[]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     print(tf.global_variables())\n",
    "    tf.global_variables_initializer().run()\n",
    "    max_acc=None\n",
    "    for epoch in range(max_epoch):\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "              'start epoch %d/%d, with learning rate = %f' % (epoch+1,max_epoch,sess.run(learning_rate)))\n",
    "        train()\n",
    "        loss_train,acc_train=eval(mnist.train,num_train//batch_size)\n",
    "        loss_val,acc_val=eval(mnist.validation,num_val//batch_size)\n",
    "        loss_test,acc_test=eval(mnist.test,num_test//batch_size)\n",
    "        acc_train_his.append(acc_train)\n",
    "        acc_val_his.append(acc_val)\n",
    "        acc_test_his.append(acc_test)\n",
    "        \n",
    "        if max_acc==None or acc_val>max_acc:\n",
    "            max_acc=acc_val\n",
    "            save_path = saver.save(sess, \"model/RAM/RAM.ckpt\")\n",
    "            print(\"Currently maximum accuracy on validation set, model saved in path: %s\" % save_path)\n",
    "        \n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'end epoch %d/%d:' % (epoch+1,max_epoch),\n",
    "             'acc_train=%.3f%% acc_val=%.3f%% acc_test=%.3f%%' % (acc_train*100.0,acc_val*100.0,acc_test*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(2)\n",
    "ptr,=plt.plot(range(max_epoch),acc_train_his,'r-')\n",
    "pva,=plt.plot(range(max_epoch),acc_val_his,'b-')\n",
    "pte,=plt.plot(range(max_epoch),acc_test_his,'y-')\n",
    "plt.xlabel('training epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('accuracy on three sets')\n",
    "plt.legend((ptr,pva,pte),('train','validation','test'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/RAM/RAM.ckpt\n",
      "Accuracy on training set is 98.714%\n",
      "Accuracy on validation set is 98.357%\n",
      "Accuracy on testing set is 98.458%\n"
     ]
    }
   ],
   "source": [
    "saver=tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, \"model/RAM/RAM.ckpt\")\n",
    "    _,acc_train=eval(mnist.train,num_train//batch_size)\n",
    "    _,acc_val=eval(mnist.validation,num_val//batch_size)\n",
    "    _,acc_test=eval(mnist.test,num_test//batch_size)\n",
    "    print('Accuracy on training set is %.3f%%' % (acc_train*100.0))\n",
    "    print('Accuracy on validation set is %.3f%%' % (acc_val*100.0))\n",
    "    print('Accuracy on testing set is %.3f%%' % (acc_test*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean=tf.zeros((100,2),dtype=tf.float32)\n",
    "# std=tf.constant([1,1],dtype=tf.float32)\n",
    "# gaussian=tf.distributions.Normal(loc=mean,scale=std)\n",
    "# rand=tf.random_normal(shape=(100,2),mean=0,stddev=1)\n",
    "# sampled=mean+rand\n",
    "# prob=-gaussian.log_prob(sampled)\n",
    "# prob=tf.reduce_mean(tf.reduce_sum(prob,1))\n",
    "# with tf.Session() as sess:\n",
    "#     out=sess.run([prob])\n",
    "#     print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
