{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"mnist/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train=mnist.train.num_examples\n",
    "num_val=mnist.validation.images.shape\n",
    "num_test=mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baseline: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py:666: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.5/site-packages/tensorflow/python/ops/losses/losses_impl.py:691: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X=tf.placeholder(dtype=tf.float32,shape=[None,28,28,1],name='X')\n",
    "y=tf.placeholder(dtype=tf.int64,shape=[None,10],name='y')\n",
    "is_training=tf.placeholder(dtype=tf.bool,name='is_training')\n",
    "cnn_out1=tf.layers.conv2d(X,128,kernel_size=3,strides=(1, 1),padding='same')\n",
    "bn_out1=tf.layers.batch_normalization(cnn_out1,axis=3,training=is_training)\n",
    "relu_out1=tf.nn.relu(bn_out1)\n",
    "pool_out1=tf.layers.max_pooling2d(relu_out1,[2,2],[2,2])\n",
    "cnn_out2=tf.layers.conv2d(pool_out1,128,kernel_size=3,strides=(1, 1),padding='same')\n",
    "bn_out2=tf.layers.batch_normalization(cnn_out2,axis=3,training=is_training)\n",
    "relu_out1=tf.nn.relu(bn_out2)\n",
    "pool_out2=tf.layers.max_pooling2d(relu_out1,[2,2],[2,2])\n",
    "flt=tf.layers.flatten(pool_out2)\n",
    "out1=tf.layers.dense(flt,1024)\n",
    "bn_out3=tf.layers.batch_normalization(out1,axis=1,training=is_training)\n",
    "score=tf.layers.dense(out1,10)\n",
    "predictions = tf.argmax(score, 1)\n",
    "correct_predictions = tf.equal(predictions, tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "\n",
    "loss=tf.losses.softmax_cross_entropy(onehot_labels=y,logits=score)\n",
    "optimizier=tf.train.AdamOptimizer(learning_rate=1e-5)\n",
    "train_step = optimizier.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11 13:33:19 start epoch 1/10:\n",
      "2018-04-11 13:33:23 iteration 1/6875: current training loss = 2.888147\n",
      "2018-04-11 13:33:26 iteration 625/6875: current training loss = 0.721611\n",
      "2018-04-11 13:33:28 iteration 1250/6875: current training loss = 0.213359\n",
      "2018-04-11 13:33:31 iteration 1875/6875: current training loss = 0.331920\n",
      "2018-04-11 13:33:33 iteration 2500/6875: current training loss = 0.262631\n",
      "2018-04-11 13:33:36 iteration 3125/6875: current training loss = 0.006284\n",
      "2018-04-11 13:33:39 iteration 3750/6875: current training loss = 0.224568\n",
      "2018-04-11 13:33:41 iteration 4375/6875: current training loss = 0.032076\n",
      "2018-04-11 13:33:44 iteration 5000/6875: current training loss = 0.920667\n",
      "2018-04-11 13:33:46 iteration 5625/6875: current training loss = 0.134122\n",
      "2018-04-11 13:33:49 iteration 6250/6875: current training loss = 0.210579\n",
      "2018-04-11 13:33:52 iteration 6875/6875: current training loss = 0.000864\n",
      "2018-04-11 13:34:00 end epoch 1/10: acc_train=88.538% acc_val=88.275% acc_test=88.650%\n",
      "2018-04-11 13:34:00 start epoch 2/10:\n",
      "2018-04-11 13:34:00 iteration 1/6875: current training loss = 0.052102\n",
      "2018-04-11 13:34:03 iteration 625/6875: current training loss = 0.020814\n",
      "2018-04-11 13:34:05 iteration 1250/6875: current training loss = 0.003365\n",
      "2018-04-11 13:34:08 iteration 1875/6875: current training loss = 0.379662\n",
      "2018-04-11 13:34:10 iteration 2500/6875: current training loss = 0.010123\n",
      "2018-04-11 13:34:13 iteration 3125/6875: current training loss = 0.000553\n",
      "2018-04-11 13:34:16 iteration 3750/6875: current training loss = 0.013526\n",
      "2018-04-11 13:34:18 iteration 4375/6875: current training loss = 0.015698\n",
      "2018-04-11 13:34:21 iteration 5000/6875: current training loss = 0.000185\n",
      "2018-04-11 13:34:24 iteration 5625/6875: current training loss = 0.034693\n",
      "2018-04-11 13:34:27 iteration 6250/6875: current training loss = 0.000452\n",
      "2018-04-11 13:34:29 iteration 6875/6875: current training loss = 0.003374\n",
      "2018-04-11 13:34:36 end epoch 2/10: acc_train=89.900% acc_val=89.950% acc_test=90.112%\n",
      "2018-04-11 13:34:36 start epoch 3/10:\n",
      "2018-04-11 13:34:36 iteration 1/6875: current training loss = 0.004037\n",
      "2018-04-11 13:34:39 iteration 625/6875: current training loss = 0.000072\n",
      "2018-04-11 13:34:41 iteration 1250/6875: current training loss = 0.169955\n",
      "2018-04-11 13:34:44 iteration 1875/6875: current training loss = 0.199038\n",
      "2018-04-11 13:34:46 iteration 2500/6875: current training loss = 0.001625\n",
      "2018-04-11 13:34:51 iteration 3125/6875: current training loss = 0.001226\n",
      "2018-04-11 13:34:54 iteration 3750/6875: current training loss = 0.184510\n",
      "2018-04-11 13:34:56 iteration 4375/6875: current training loss = 0.000291\n",
      "2018-04-11 13:34:59 iteration 5000/6875: current training loss = 0.001921\n",
      "2018-04-11 13:35:02 iteration 5625/6875: current training loss = 0.000047\n",
      "2018-04-11 13:35:04 iteration 6250/6875: current training loss = 0.053897\n",
      "2018-04-11 13:35:07 iteration 6875/6875: current training loss = 0.000011\n",
      "2018-04-11 13:35:13 end epoch 3/10: acc_train=93.237% acc_val=92.650% acc_test=93.000%\n",
      "2018-04-11 13:35:13 start epoch 4/10:\n",
      "2018-04-11 13:35:13 iteration 1/6875: current training loss = 0.005370\n",
      "2018-04-11 13:35:16 iteration 625/6875: current training loss = 0.000653\n",
      "2018-04-11 13:35:19 iteration 1250/6875: current training loss = 0.013331\n",
      "2018-04-11 13:35:22 iteration 1875/6875: current training loss = 0.000803\n",
      "2018-04-11 13:35:25 iteration 2500/6875: current training loss = 0.004953\n",
      "2018-04-11 13:35:27 iteration 3125/6875: current training loss = 0.000888\n",
      "2018-04-11 13:35:30 iteration 3750/6875: current training loss = 0.000231\n",
      "2018-04-11 13:35:33 iteration 4375/6875: current training loss = 0.000142\n",
      "2018-04-11 13:35:35 iteration 5000/6875: current training loss = 0.000547\n",
      "2018-04-11 13:35:38 iteration 5625/6875: current training loss = 0.000824\n",
      "2018-04-11 13:35:40 iteration 6250/6875: current training loss = 0.001674\n",
      "2018-04-11 13:35:43 iteration 6875/6875: current training loss = 0.000806\n",
      "2018-04-11 13:35:51 end epoch 4/10: acc_train=96.475% acc_val=95.725% acc_test=96.300%\n",
      "2018-04-11 13:35:51 start epoch 5/10:\n",
      "2018-04-11 13:35:51 iteration 1/6875: current training loss = 0.000027\n",
      "2018-04-11 13:35:54 iteration 625/6875: current training loss = 0.001545\n",
      "2018-04-11 13:35:56 iteration 1250/6875: current training loss = 0.000049\n",
      "2018-04-11 13:35:59 iteration 1875/6875: current training loss = 0.008378\n",
      "2018-04-11 13:36:02 iteration 2500/6875: current training loss = 0.000149\n",
      "2018-04-11 13:36:04 iteration 3125/6875: current training loss = 0.281488\n",
      "2018-04-11 13:36:07 iteration 3750/6875: current training loss = 0.000231\n",
      "2018-04-11 13:36:10 iteration 4375/6875: current training loss = 0.475010\n",
      "2018-04-11 13:36:12 iteration 5000/6875: current training loss = 0.000063\n",
      "2018-04-11 13:36:15 iteration 5625/6875: current training loss = 0.000298\n",
      "2018-04-11 13:36:20 iteration 6250/6875: current training loss = 0.002577\n",
      "2018-04-11 13:36:23 iteration 6875/6875: current training loss = 0.023456\n",
      "2018-04-11 13:36:29 end epoch 5/10: acc_train=97.400% acc_val=97.275% acc_test=97.488%\n",
      "2018-04-11 13:36:29 start epoch 6/10:\n",
      "2018-04-11 13:36:29 iteration 1/6875: current training loss = 0.000149\n",
      "2018-04-11 13:36:32 iteration 625/6875: current training loss = 0.007247\n",
      "2018-04-11 13:36:34 iteration 1250/6875: current training loss = 0.060669\n",
      "2018-04-11 13:36:37 iteration 1875/6875: current training loss = 0.005978\n",
      "2018-04-11 13:36:39 iteration 2500/6875: current training loss = 0.001021\n",
      "2018-04-11 13:36:42 iteration 3125/6875: current training loss = 0.001742\n",
      "2018-04-11 13:36:44 iteration 3750/6875: current training loss = 0.001972\n",
      "2018-04-11 13:36:47 iteration 4375/6875: current training loss = 0.066102\n",
      "2018-04-11 13:36:50 iteration 5000/6875: current training loss = 0.000874\n",
      "2018-04-11 13:36:53 iteration 5625/6875: current training loss = 0.000377\n",
      "2018-04-11 13:36:55 iteration 6250/6875: current training loss = 0.000129\n",
      "2018-04-11 13:36:58 iteration 6875/6875: current training loss = 0.000054\n",
      "2018-04-11 13:37:05 end epoch 6/10: acc_train=98.731% acc_val=97.950% acc_test=98.325%\n",
      "2018-04-11 13:37:05 start epoch 7/10:\n",
      "2018-04-11 13:37:05 iteration 1/6875: current training loss = 0.114442\n",
      "2018-04-11 13:37:07 iteration 625/6875: current training loss = 0.000064\n",
      "2018-04-11 13:37:10 iteration 1250/6875: current training loss = 0.000012\n",
      "2018-04-11 13:37:14 iteration 1875/6875: current training loss = 0.168729\n",
      "2018-04-11 13:37:17 iteration 2500/6875: current training loss = 0.000006\n",
      "2018-04-11 13:37:19 iteration 3125/6875: current training loss = 0.000815\n",
      "2018-04-11 13:37:22 iteration 3750/6875: current training loss = 0.012163\n",
      "2018-04-11 13:37:25 iteration 4375/6875: current training loss = 0.000005\n",
      "2018-04-11 13:37:27 iteration 5000/6875: current training loss = 0.000109\n",
      "2018-04-11 13:37:30 iteration 5625/6875: current training loss = 0.001699\n",
      "2018-04-11 13:37:33 iteration 6250/6875: current training loss = 0.000042\n",
      "2018-04-11 13:37:35 iteration 6875/6875: current training loss = 0.000043\n",
      "2018-04-11 13:37:43 end epoch 7/10: acc_train=98.069% acc_val=97.425% acc_test=97.875%\n",
      "2018-04-11 13:37:43 start epoch 8/10:\n",
      "2018-04-11 13:37:43 iteration 1/6875: current training loss = 0.092289\n",
      "2018-04-11 13:37:46 iteration 625/6875: current training loss = 0.000046\n",
      "2018-04-11 13:37:48 iteration 1250/6875: current training loss = 0.107554\n",
      "2018-04-11 13:37:51 iteration 1875/6875: current training loss = 0.000231\n",
      "2018-04-11 13:37:54 iteration 2500/6875: current training loss = 0.001563\n",
      "2018-04-11 13:37:56 iteration 3125/6875: current training loss = 0.000133\n",
      "2018-04-11 13:37:59 iteration 3750/6875: current training loss = 0.000259\n",
      "2018-04-11 13:38:02 iteration 4375/6875: current training loss = 0.000073\n",
      "2018-04-11 13:38:04 iteration 5000/6875: current training loss = 0.000049\n",
      "2018-04-11 13:38:07 iteration 5625/6875: current training loss = 0.045239\n",
      "2018-04-11 13:38:10 iteration 6250/6875: current training loss = 0.097180\n",
      "2018-04-11 13:38:14 iteration 6875/6875: current training loss = 0.000035\n",
      "2018-04-11 13:38:21 end epoch 8/10: acc_train=98.531% acc_val=97.850% acc_test=98.050%\n",
      "2018-04-11 13:38:21 start epoch 9/10:\n",
      "2018-04-11 13:38:21 iteration 1/6875: current training loss = 0.000203\n",
      "2018-04-11 13:38:23 iteration 625/6875: current training loss = 0.019578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11 13:38:26 iteration 1250/6875: current training loss = 0.000145\n",
      "2018-04-11 13:38:29 iteration 1875/6875: current training loss = 0.000116\n",
      "2018-04-11 13:38:31 iteration 2500/6875: current training loss = 0.000003\n",
      "2018-04-11 13:38:34 iteration 3125/6875: current training loss = 0.000178\n",
      "2018-04-11 13:38:37 iteration 3750/6875: current training loss = 0.000510\n",
      "2018-04-11 13:38:39 iteration 4375/6875: current training loss = 0.014593\n",
      "2018-04-11 13:38:43 iteration 5000/6875: current training loss = 0.000005\n",
      "2018-04-11 13:38:46 iteration 5625/6875: current training loss = 0.007059\n",
      "2018-04-11 13:38:49 iteration 6250/6875: current training loss = 0.000002\n",
      "2018-04-11 13:38:51 iteration 6875/6875: current training loss = 0.000013\n",
      "2018-04-11 13:38:58 end epoch 9/10: acc_train=98.713% acc_val=97.975% acc_test=98.138%\n",
      "2018-04-11 13:38:58 start epoch 10/10:\n",
      "2018-04-11 13:38:58 iteration 1/6875: current training loss = 0.050795\n",
      "2018-04-11 13:39:01 iteration 625/6875: current training loss = 0.000074\n",
      "2018-04-11 13:39:03 iteration 1250/6875: current training loss = 0.000001\n",
      "2018-04-11 13:39:06 iteration 1875/6875: current training loss = 0.002381\n",
      "2018-04-11 13:39:09 iteration 2500/6875: current training loss = 0.000001\n",
      "2018-04-11 13:39:13 iteration 3125/6875: current training loss = 0.000034\n",
      "2018-04-11 13:39:15 iteration 3750/6875: current training loss = 0.000126\n",
      "2018-04-11 13:39:18 iteration 4375/6875: current training loss = 0.000181\n",
      "2018-04-11 13:39:20 iteration 5000/6875: current training loss = 0.034099\n",
      "2018-04-11 13:39:23 iteration 5625/6875: current training loss = 0.007979\n",
      "2018-04-11 13:39:26 iteration 6250/6875: current training loss = 0.000325\n",
      "2018-04-11 13:39:28 iteration 6875/6875: current training loss = 0.000000\n",
      "2018-04-11 13:39:35 end epoch 10/10: acc_train=98.231% acc_val=97.925% acc_test=97.737%\n"
     ]
    }
   ],
   "source": [
    "max_epoch=10\n",
    "batch_size=8\n",
    "print_every=625\n",
    "\n",
    "def train():\n",
    "    num_iteration=num_train//batch_size\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=mnist.train.next_batch(batch_size)\n",
    "        loss_num,_ = sess.run([loss,train_step],feed_dict={X:images.reshape(-1,28,28,1),y:labels,is_training:True})\n",
    "        if it==0 or (it+1)%print_every==0 or it==num_iteration-1:\n",
    "            print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "                  'iteration %d/%d:' % (it+1,num_iteration),'current training loss = %f' % (loss_num))\n",
    "            \n",
    "def eval(dataset,num_iteration):\n",
    "    total_loss=0\n",
    "    total_accuracy=0\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=dataset.next_batch(batch_size)\n",
    "        loss_num,accuracy_num = sess.run([loss,accuracy],feed_dict={X:images.reshape(-1,28,28,1),y:labels,is_training:False})\n",
    "        total_loss+=loss_num\n",
    "        total_accuracy+=accuracy_num\n",
    "    total_loss/=num_iteration\n",
    "    total_accuracy/=num_iteration\n",
    "    return total_loss,total_accuracy\n",
    "    \n",
    "acc_train_his=[]\n",
    "acc_val_his=[]\n",
    "acc_test_his=[]\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(max_epoch):\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'start epoch %d/%d:' % (epoch+1,max_epoch))\n",
    "        train()\n",
    "        loss_train,acc_train=eval(mnist.train,2000)\n",
    "        loss_val,acc_val=eval(mnist.validation,500)\n",
    "        loss_test,acc_test=eval(mnist.test,1000)\n",
    "        acc_train_his.append(acc_train)\n",
    "        acc_val_his.append(acc_val)\n",
    "        acc_test_his.append(acc_test)\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'end epoch %d/%d:' % (epoch+1,max_epoch),\n",
    "             'acc_train=%.3f%% acc_val=%.3f%% acc_test=%.3f%%' % (acc_train*100.0,acc_val*100.0,acc_test*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3Xd4lGXWwOHfSSekkgQkJJDQew0o\niygWFMFFsaLiCor4ib2wgA1EUVREZG1rAQFFF7CLDRRERZQgIEgnlARCKEkICaQ/3x/PJAwhkICZ\nTMq5r+u9mHnLzJkB3jNPF2MMSiml1Kl4uDsApZRSVZ8mC6WUUmXSZKGUUqpMmiyUUkqVSZOFUkqp\nMmmyUEopVSZNFkpVUSLSR0SS3B2HUqDJQqkqQ0SMiDR3dxx/hya4mkuThapxxKp1/7ZFxNPdMaia\nq9b9h1KVQ0TGiMg2ETksIutFZFCJ47eLyAan410d+6NF5GMR2S8iB0XkFcf+8SLyntP1MY5f4l6O\n50tEZKKI/AIcAZqKyDCn90gQkTtKxHCFiKwWkQxHrP1E5FoRWVnivAdF5LOTfM5IEflcRFJFZKuI\n3O50bLyIzBWRWY4Y/hKRuJO8zlLHwzUikiki1zsde0hE9olIsogMc9r/roi8LiJfiUgWcIGI+IrI\nZBHZJSIpIvKGiNRxuuZyx2dOF5FlItLxJPGIiLzkeN8MEVkrIu0dx0p9DxGpC3wNRDo+Q6bj++kh\nIvGO10kRkSmlvaeq4owxuulW4RtwLRCJ/UFyPZAFNHQ6thvoDgjQHGgCeAJrgJeAuoAfcK7jmvHA\ne06vHwMYwMvxfAmwC2gHeAHewACgmeM9zscmka6O83sAh4C+jhgbAa0BXyAVaOP0XquAq0/yOZcC\nrzli7QzsBy50ijkb6O/4bM8Cy0/xnRmgudPzPkA+MMHxefo7PkOo4/i7js/Qy/EZ/Bzf3edAPSAQ\n+AJ41nF+F2AfcLYjnluAHYBvKbFcCqwEQhzfXxunv79TvUcfIKnEa/0K3Ox4HACc4+5/n7qdwf9p\ndwegW+3YgNXAFY7H3wL3lXJOT8fN1quUY+VJFhPKiOHTovcF/gu8dJLzXgcmOh63A9JOckONBgqA\nQKd9zwLvOsW8yOlYW+DoKeIrLVkcdf4+HDf7cxyP3wVmOR0TbFJuVuI73e70uZ4q8Z6bgPNLieVC\nYDNwDuBxGu9RWrJYCjwJhLv736FuZ75pNZRyCRH5l1N1RzrQHgh3HI4GtpVyWTSw0xiTf4Zvm1gi\nhstEZLmjiigd+8u8rBgAZgI3iogANwNzjTE5pZwXCaQaYw477duJLaUU2ev0+AjgV1R1Vk4HS3wf\nR7C/zos4f+YIwB9Y6fS9f+PYD7b09lDRMcfxaMfnOI4x5gfgFeBVYJ+IvCkiQeV4j9LcBrQENorI\nChG5vNyfXlUZmixUhRORJsBbwN1AmDEmBFiH/VUK9gbXrJRLE4HGJ7mZZmFvUkXOKuWc4imURcQX\n+AiYDDRwxPBVOWLAGLMcyAV6AzcCs0s7D9gD1BORQKd9jbFVbJXFedroA9iSSDtjTIhjCzbGFCWX\nRGyJKcRp8zfGfFDqCxszzRjTDVsiagmMKsd7nDCNtTFmizHmBqA+8Bww39G+oaoRTRbKFepibxr7\nARyNsu2djr8NPCwi3RwNqc0dCeZ3IBmYJCJ1RcRPRHo5rlkNnCcijUUkGBhbRgw+2PaH/UC+iFwG\nXOJ0/B1gmIhcJCIeItJIRFo7HZ+F/WWdZ4z5ubQ3MMYkAsuAZx2xdsT+in6vtPPLIQVoeobXYowp\nxCbpl0SkPoDjc13qOOUt4P9E5GzH915XRAaUSHY4ruvuOM8bm6izgcJyvEcKEOb4Oyp6rSEiEuG4\nNt2xu/BMP6dyD00WqsIZY9YDL2IbNlOADsAvTsfnAROBOcBhbFtCPWNMAfBPbIP3LiAJ2ziOMWYh\n8D/gT2zD65dlxHAYuBeYi21zuBHbKFt0/HdgGLax9hDwI7aapshsbIIr68Z/A7b9ZA/wCTDOGLOo\njGtOZjww01G9c90ZvsZoYCuwXEQygEVAKwBjTDxwOzYJpjnOG3qS1wnCJoU0bNXaQeCFcrzHRuAD\nIMHxOSKBfsBfIpIJvAwMNsYcPcPPp9xEjNHFj5QqydHddB+299QWd8ejlLtpyUKp0t0JrNBEoZR1\nOr0ylKoVRGQHtiH8SjeHolSVodVQSimlyqTVUEoppcpUY6qhwsPDTUxMjLvDUEqpamXlypUHjDGn\nGlQJ1KBkERMTQ3x8vLvDUEqpakVEdpbnPK2GUkopVSZNFkoppcqkyUIppVSZakybRWny8vJISkoi\nOzvb3aHUGH5+fkRFReHt7e3uUJRSlahGJ4ukpCQCAwOJiYnBzjat/g5jDAcPHiQpKYnY2Fh3h6OU\nqkQ1uhoqOzubsLAwTRQVREQICwvTkppStVCNThaAJooKpt+nUrVTjU8WSlV7BQXw3nvw9deQf6aL\nCKoKVVgI+/bZrZao0W0WVUF6ejpz5sxh5MiRp3Vd//79mTNnDiEhIS6KTFULe/bATTfBkiX2+Vln\nwZAh8K9/QYcObg2tRjIGUlPt9+68JSef+LwocbdpA3372u388yHwhLWkaoQaM5FgXFycKTmCe8OG\nDbRp08ZNEVk7duzg8ssvZ926dcftz8/Px8ureubqqvC91grffgs33wxZWfDKKxAcDDNnwldf2RtV\nly5wyy1w440QUeZsDbWbMZCRcWISKG3LzT3x+tBQiIw8cTt6FBYtgqVLITsbvLygZ89jySMuzu6r\nwkRkpTEmrszzNFm41uDBg/nss89o1aoV3t7e+Pn5ERoaysaNG9m8eTNXXnkliYmJZGdnc9999zFi\nxAjg2PQlmZmZXHbZZZx77rksW7aMRo0a8dlnn1GnTh23faaq8L3WaPn58PjjMGkStG8Pc+faX69F\n9u+HDz6wieOPP+zNqH9/mzgGDABfX/fF7g6ZmWUngORkOHLkxGuDguxNv2HD0pNB0bGy/r9lZ8Mv\nv9jEsXCh/Xsxxib4Cy44ljyaN4cq1u6nyYISN7X774fVqyv2TTt3hqlTT3mKc8liyZIlDBgwgHXr\n1hV3PU1NTaVevXocPXqU7t278+OPPxIWFnZcsmjevDnx8fF07tyZ6667joEDBzJkyJCK/SynQZOF\nCyUmwg032BvP7bfDyy+f+ka1bp1NGu+9B3v3Qr16MHiwTRzdu1e5G9PfcvSo/V5++AF++w1277aJ\n4PDhE8/19z/5zd85CQQEuCbWAwdsnAsX2m2nY/qlJk2OJY6LLoKwMNe8/2kob7Ko2uWjGqhHjx7H\njVGYNm0an3zyCQCJiYls2bKFsBL/gGJjY+ncuTMA3bp1Y8eOHZUWr6pEX35pb/K5uTBnjk0aZWnf\nHl54AZ591v6qnTkTpk+H116D1q3t6w0ZAlFRro+/ouXlwe+/25vuDz/AsmX2u/H0hK5doWNH6Nfv\n+Jt/0eOgIPcmyvBwuO46uxkD27YdSxzz5sHbb9v4unaFiy+2yaNXL/Dzc1/MZag9yaKMEkBlqVu3\nbvHjJUuWsGjRIn799Vf8/f3p06dPqWMYfJ2qFTw9PTl6VNe6r1Fyc2HsWJgyxZZW586FFi1O7zW8\nvOyNs18/OHTIvsbMmfZ1H3nE/oq95RYYNAic/g1WKQUFsGbNseSwdKltrxGx38s998CFF0Lv3tWr\nEVnEVj81bw533mmrGePjjyWPF1+E556zJcjevY+VPDp0AI+q02G19iQLNwkMDORwacVk4NChQ4SG\nhuLv78/GjRtZvnx5JUen3G77dltt9PvvcNddMHny3/91GRxsq7Buv93+op01y24332yrXa691vam\nOu88996MjIGNG21i+P572+MrLc0eKyoVXXSR7WFUBaprKoyXF5xzjt0ef9y2ufz447HkMWqUPa9+\nfVvqKCp5uLl0qMnCxcLCwujVqxft27enTp06NGjQoPhYv379eOONN2jTpg2tWrXinHPOcWOkqtJ9\n/DHceqt9PH8+XH31CacYU8iBA5+xZ8/reHkFExJyASEhffD3b1O+AZLNmsGTT8K4cfDTT7a0MW8e\nzJgBMTE2gfzrX/ZXb2XYscMmhqLSw969dn/jxnDllTY5XHCBrUqqLQICbMeEAQPs8927bZViUWP5\nnDl2f+vWx0odffpUeumq9jRwqwqj3+vflJMDDz9su8N27w4ffghNmx53SmFhLikp77Nr13McPboJ\nP78YjCkgJycRAG/v+oSE9HFsF+Dv36r8o+uzsuDTT23iWLTI/sLv1cv+kr/uOlsyqSjJybB48bHk\nsH273d+gga1SKtpiY2tWY3xFMcZ2Yigqdfz4o23oLyqd9O1rSx49epxxF13tDYXe1FxFv9e/YetW\nuP5627XygQds91gfn+LD+fmHSU5+i8TEKeTm7iYgoAuNG48mPPxqRDzJzt5OevoS0tMXk5a2mNzc\n3QD4+Jx1XPKoU6dF+ZJHUpLtSTVzpq0S8vODK66wiaNv39O/AaWm2htaUXJYv97uDwmxv4aLkkPb\ntpoczkRODvz667HkER9vE0qXLvbf1BnQZIHe1FxFv9cz9L//2XYELy94910YOLD4UG7ufnbv/g+7\nd79Cfn4aISEX0LjxGEJD+570pm+M4ejRbY7kYRNIbu4eAHx8GhYnjpCQPtSp0/zUycMYe+OZOdOO\n4UhNPTZa/JZbbK+r0mRmws8/H0sOReML/P1tY21RcujSxfZiqqaMKaCgIIuCgsMUFGSSn2//9PDw\nISCgC56ebhr3lJpqv/ecHDvS/wxUiWQhIv2AlwFP4G1jzKQSx5sA04EIIBUYYoxJchx7HhiAnb9q\nIXCfOUWwmiwqj36vp+noUTvO58037ejeDz+0dfRAdvZOEhNfJDn5bQoLswkPv5LGjUcTFHT2ab+N\nTR5bSU9f7JQ8bJuAj0+j4pJHaOgF+Pk1PXnyyMmBBQuOHy3etatt27j6attoXtQo/dtv9ri3t/1s\nF15o2x169DiuxFTZCgtzim/oRTf4kn+e6njJY4WFpQzocxDxIiCgC0FBPQkK6klwcE98fRtXm0k3\n3Z4sRMQT2Az0BZKAFcANxpj1TufMA740xswUkQuBYcaYm0XkH8ALwHmOU38Gxhpjlpzs/TRZVB79\nXk/Dxo22HWDtWhg9Gp56Cry9ycxcR2Lic6SkfICIBw0a3Ex09Cjq1m1dYW9tk8fm4pJHWtpi8vJS\nAPD1jSoudYSE9MHPL7b0m1vJ0eJFPDygWzebGC680LZ5+PtXWOxF8RcUZJCTk0xu7rEtJyeZvLx9\npd7Ui/40Jq+c7+KBp2cgnp4BeHnZP4ueO/95smMFBYfJyFhORsavZGT8XpxUfHwiixNHUFBPAgK6\n4ulZNcdQVIVBeT2ArcaYBEdAHwJXAOudzmkLPOh4vBj41PHYAH6ADyCAN5DiwliVqnizZ9t+9XXq\n2Blj+/Xj0KFf2LVrEgcPfomHR12iou4jKuoB/PwqvlukiODv3wp//1ZERt6BMYYjRzYVlzxSU78l\nJWU2AL6+jUtUW8XYF4mIgHvvtdu6dbak0bq17XZ7hpNcGmPIyzvouPnvKTUZFD0uLDxxTJGHhx/e\n3g3w8gpy3MiD8fWNcrrhl35jL+2G7+Hhd9olgJwc28N33z47g4iPz0Dq1IGQkHzgT/LyfiU7+1cy\nM5dx4MBHjr8LHwIDuxaXPoKCerrk79yVXJksGgGJTs+TgJJl6zXAVdiqqkFAoIiEGWN+FZHFQDI2\nWbxijNlQ8g1EZAQwAqCxo1ivlNtlZcHdd9t2ifPOw7z/Pql11rBrVW8OHfoZb+9wYmIm0KjRXXh7\n1zvlS+Xm2hqhefNsE8KAAbYp4ExqeESEunVbU7duaxo1utORPDYUV1mlpn5FSsosAPz8Yo5rMPfz\na2zbLU7WdgEUFuaTl7fvhBv+ic/3lvrL39MzCB+fhvj6NiQo6Gx8fBoWPy967OPTEC+v4L9dxWOM\n/WtKS7PV/qfaSp6TlXWyV/UCujq2uwAID99Lp07LadfuV9q0WUbTpq/j4/MSAOnp0SQm9iQ5uSf7\n9vXk0KEueHv74Odnp/dy3krb57w/PNzOWehKrqyGugboZ4wZ7nh+M3C2MeZup3MigVeAWGApcDXQ\nHgjHJpDrHacuBP5tjPnpZO9XU6qhAgICyMzMZM+ePdx7773Mnz//hHP69OnD5MmTiTvFv46pU6cy\nYsQI/B1VAxU55Xl1/F4rzV9/2WqnDRsofHws++5oSeLuyWRlrcPXtzHR0aNo2PBWPD1PXWWzZo0d\nCvH++3aaofBwO2lqbq7tXn/JJTZxXHaZTSIVwSaP9aSlLS6uusrPPwiAn18sISF9CA7uRWFh9kmq\nhvYDhSe8rrd3+HE3+5I3/6LnZX0npSkstAPWy3OTL7nlnaKmysfHTrN1qi001Na85eba0kbRlp19\n/HPn/Xl5uQQGriEs7FcaNFhGZOSvhITsAiAvz5cdO+LYvLknGzb0ZN26niQnNyx1EtySzj4bznRM\nb1WohtoNRDs9j3LsK2aM2YMtWSAiAcDVxph0EbkdWG6MyXQc+xroCZw0WdQ0kZGRpSaK8po6dSpD\nhgwpThZfffVVRYWmSmOMnZPpnnsoCA8keeFIEv3fJ2fzTvz929G69Wzq178eDw/vk77EgQN2/NWM\nGXbOSx8f22Fq2DCbHLKzbZvyggV2+8jWcBAXd2xMV7duZz4o25Y82lG3bjuiou7GmEKysv4qLnkc\nOPAZe/fOcJztgY9PA8cNvxGBgd1Pkgwa4OHx9xu6CwpsLdhPP9lt1So4eNAmhFP93g0IOP4G367d\nsRv9qRJBnTqu6tnrA3R3bPcCkJOzh4yMXzl0aBlhYb/SsuU0BgyYDNgSXlBQT/z9e+Lr2xNPz07k\n5nqfkJQqY0opVyaLFUALEYnFJonBwI3OJ4hIOJBqjCkExmJ7RgHsAm4XkWex1VDnA1VjcqfTNGbM\nGKKjo7nrLlssHT9+PF5eXixevJi0tDTy8vJ4+umnueKKK467znm22qNHjzJs2DDWrFlD69atj5sb\n6s4772TFihUcPXqUa665hieffJJp06axZ88eLrjgAsLDw1m8eHHxLLbh4eFMmTKF6dPtVz18+HDu\nv/9+duzYUeWmQq82Dh+GO+8k77P32f3vpuy+KJ28glcJ8u1FixavEBbWH5HS7+D5+fDNNzZBfPGF\n/bXbrRv85z92HkHnWS4CAuwQiCuusDfINWuOJY4JE+xA7QYN7GzlAwbYYRJBQWf+sUQ8CAjoQEBA\nB6Ki7sGYQo4eTcDTMwAfnwhsHxbXyMmBFSuOJYdly2wJAuysF2efbUtUp/rVHxrq1g5Z5ebrG0lE\nxNVERNgR/IWFORw+vMrRaL6M9PSl7Nv3AQAeHnUIDOxOUFBPGjSwbR8+PvUrJU5Xd53tj73JewLT\njTETRWQCEG+M+dxRVfUstkF7KXCXMSbH0ZPqNWxvKAN8Y4x5sPR3scqqhnLTDOWsWrWK+++/nx9/\n/BGAtm3b8u233xIcHExQUBAHDhzgnHPOYcuWLYhIcTWUc7KYMmUK69atY/r06fz555907dqV5cuX\nExcXVzzFeUFBARdddBHTpk2jY8eOxyUHOLY+xs6dOxk6dCjLly/HGMPZZ5/Ne++9R2hoaLmnQtdq\nKCerV5N9x1UkxW0neZAPBV65hIVdTnT0aEJCzj3pZX/9ZZs0Zs+GlBTbjjxkCAwdaidTPV0HDtik\ns2CB/TM93fZm7d37WKmjZcuqOw7u0CGbEH7+2SaH33+3CQPsUh7nnms/S+/edpbvqvo5XCU7O7G4\n9JGR8SuZmauK2338/JoRFnY5LVqc2e/pqlANhTHmK+CrEvuecHo8HzihrsUYUwDc4crYKkuXLl3Y\nt28fe/bsYf/+/YSGhnLWWWfxwAMPsHTpUjw8PNi9ezcpKSmcdZLK56VLl3LvvbbI2rFjRzo63U3m\nzp3Lm2++SX5+PsnJyaxfv/644yX9/PPPDBo0qHj226uuuoqffvqJgQMH6lTop8MYsqaPJ3Hr06RM\nLMR4edCgwXVER/+bgIDSlztNS7NDLGbMsL+avbzg8sttgujf397cz1R4uE02Q4bY0sqyZcdKHQ89\nZLdmzez7DRhgOzO5c42kvXuPlRp++gn+/NO2PxTNPn7XXTYx9OqliwAC+PlF4+cXTf361wFQUHCU\nw4dXOkofv1JQUPpkpRWp1kwk6M4Zyq+99lrmz5/P3r17uf7663n//ffZv38/K1euxNvbm5iYmFKn\nJi/L9u3bmTx5MitWrCA0NJShQ4ee0esU0anQyydjzw/s+vpfHIjdjUdjDyLr30ZUi8eOdTd1UlBg\nZ2V49107HVNOjp15esoUO+C2vgtqELy8bDI47zw78/WOHbbH64IF8N//2vWU6ta11VQDBthE5cp5\n+4yxs5w4J4dt2+wxf/9jk6/27m0fV9UZ1KsST886hISce8rSa0WrNcnCna6//npuv/12Dhw4wI8/\n/sjcuXOpX78+3t7eLF68mJ1Fq2idxHnnncecOXO48MILWbduHX/++ScAGRkZ1K1bl+DgYFJSUvj6\n66/p06cPcGxq9KJqqCK9e/dm6NChjBkzBmMMn3zyCbNnz3bJ565JjDGkpS1k11+Pkl4Qj1cENEm+\niEZXv4+PX4MTzt+0ySaIWbPsYm716tmZPoYNszNfVGY1SkwMjBxptyNH7Lx+X35pk8enjpFNXboc\nK3V07/73Zi4vWpbip5+OVSulOEZJhYXZKqU777R/du3690pUqvJosqgE7dq14/DhwzRq1IiGDRty\n00038c9//pMOHToQFxdH69anHrV75513MmzYMNq0aUObNm3o1q0bAJ06daJLly60bt2a6OhoevXq\nVXzNiBEj6NevH5GRkSxevLh4f9euXRk6dCg9evQAbAN3ly5dtMrpJIwpYP/+j9i1axKZmavwOQDN\nvguh4S1z8RrY97hzMzLs9E8zZti53jw8bNfWl1+Gf/6zaiyN7e9/rA2jaELTouqqiRPtAPOICBv3\ngAG2F1ZZva2zs20bg3NjdNESLkWriBa1ObRuXaXW81GnQScSVKettnyvGRnxbNo0jKysddRJDaDx\n25k08BuAx9uzbFEBW8++eLFNEB9/bKeBat3aliBuvtmu9FldpKbCt9/axPH11/a5l5e90RclmNat\nbWP0L78cSw7x8RSPBWjf/vjG6OjoU7+ncr8q0cCtVHVUUJDNzp1PsmvXC/hQj7avhhPxRToy6SW4\n7z4QISHBVjPNnAm7dtklIG65xTZW9+hRPXvr1Ktnu+vecIOtSlq+/FipY9Qou9Wvb6eLMsYmkrg4\n+5UUNUbXO/WAdFWNabJQyklGxu9s3DiMI0fW03BtY5qN3YVX/Vj46Ssy23Rn/kxbili61CaEvn3t\nkhRXXmkHctUUnp725t+rFzzzDCQm2kbyn3+2XXB797ZJsYLnDlRVmCYLpbCliR07xpGYOBnfw350\nfArqbT6EGfM0P3Z7gHdf82fePDsvUPPmtn7/5ptrTzVLdDTccYfdVO2kyULVeocOLWfT2iEcyd9G\nwy+h2UzwuuNRkmc+zJVDQ/j9cTt6evBg2xbxj39Uz2ompf4OTRaq1iooOMqOtQ+QmPomvvsNHad6\nU+8f98D6Mfy1L4L+/ezI6DffhBtv1P7/qnbTZKFqpUM7FrBx3c0cDUgjcoEHTfNuxWv+k9CoEYsW\n2QXh/P1t24Sjp7JStZr2eHax9PR0XnvttTO6durUqRw5cvLlHNXpKzi4m60zz2ZVwuUUZqbR6btL\naXn3VrymvQ2NGjFjhh1jEB1tVwzVRKGUpcnCxTRZVBGHD5P+8nDiv2lMUpPfidzQnO6d4gl95huI\njcUYO+XErbdCnz52HIGup6XUMVoN5WJjxoxh27ZtdO7cmb59+1K/fn3mzp1LTk4OgwYN4sknnyQr\nK4vrrruOpKQkCgoKePzxx0lJSTlhmnF1Bo4coeCNqWzf8xRJ/bPxO+xPp8CXCb1rePEpOTlw2212\noaFbb4U33tApKJQqqdYkiy1b7iczs2LnKA8I6FzmtMCTJk1i3bp1rF69mu+++4758+fz+++/Y4xh\n4MCBLF26lP379xMZGcmCBQsAOHToEMHBwUyZMoXFixefML+TKoecHHj7bdI/eoKNt6WS3RUaeV1L\n7IDpeHkFFJ+WmgqDBtm2iaefhkce0Z5OSpVGq6Eq0Xfffcd3331Hly5d6Nq1Kxs3bmTLli106NCB\nhQsXMnr0aH766SeCg4PdHWr1lZ8P06dT0L45WzbdzeonUqFRJJ07L6HFuXOPSxQJCbYb7PLltlTx\n6KOaKJQ6mVpTsjjThUEqkjGGsWPHckcpI5v++OMPvvrqKx577DEuuuginnjiiVJeQZ1UYaFdLGL8\neNL9t7Bxki/ZYdCo0T00bfosnp7H93v97Tc7uV9+vp1C/Lzz3BS3UtWElixcrGiqcIBLL72U6dOn\nk5mZCcDu3buLF0by9/dnyJAhjBo1ij/++OOEa9VJGAOffAKdOpF/201sHpLK6qkgUdF07ryUFi2m\nnZAoPv7YNmIHBtrZYTVRKFW2WlOycJewsDB69epF+/btueyyy7jxxhvp2bMnAAEBAbz33nts3bqV\nUaNG4eHhgbe3N6+//jpw8mnGFTZJfPON7cK0ciVpl0exaWoE2Z4HiIq6n9jYiXh6+p9wydSpdtW4\nHj3g889ds/iQUjWRTlGuTpvbv9clS+Cxx+CXX8hv3ZiE55qxJ2gxdeq0oHXrGQQH9zrhkvx8uw77\nq6/aAXezZ9esif+UOlPlnaJcq6FU9bF8uZ3m9YILYPt20t69lxXThT1BS4iKepC4uNWlJorMTNvj\n6dVX4eGHYe5cTRRKnS6thlJV3+rVtrrpyy8hIoL8qc+wre82kvdNo45HS7p0+Zng4H+Uemlysl0u\ndPVqmyxGjqzk2JWqIWp8sjDGINofssJUarXlhg0wbhzMm2fX9pw4kdRb2rNp1z3k7EsiOnoUMTFP\n4ulZejFh3Tro39+Opfj8c7vSm1LqzNToaig/Pz8OHjxYuTe4GswYw8GDB/Hz83P9m02datfo/Ppr\neOwx8reuZtPVO/hzyxV4evoSaLjuAAAgAElEQVTTpcsvNGv2/EkTxaJFduGe/Hw74E4ThVJ/T40u\nWURFRZGUlMT+/fvdHUqN4efnR1RUlGvfJCkJxo6FSy6BWbNI9fyDTZvOJSdnD9HRo4mJGY+n58kT\n1vTpdpGe1q3t6m61ZYEipVypRicLb29vYmNj3R2GOl2PPQaFheS/+hxbD45l79538PdvS9euHxEU\n1OOklxkDTzxhp+3o29fWXulgeKUqRo1OFqoaWr0aZs3i0MTB/LW3P7m5yTRuPJYmTZ44ZWnCeTLA\n226D11/XyQCVqkiaLFTVYQyMGkVhRAgbzv8FDw8funZdTlBQ91Ne5jwZ4MSJtgZL+zQoVbE0Waiq\n49tvYdEi9rw3iOzcT+jY8dsyE0VCgu3xtH07zJkDN9xQSbEqVctoslBVQ0EBjBpFfvsYdjRZSmjg\nxdSrd8kpL1m+HAYOtJcuWgS9e1dSrErVQi7tOisi/URkk4hsFZExpRxvIiLfi8ifIrJERKKcjjUW\nke9EZIOIrBeRGFfGqtzs3Xdh3Tp2TepCfv5BmjZ97pSnf/SRHcgdGAjLlmmiUMrVXJYsRMQTeBW4\nDGgL3CAibUucNhmYZYzpCEwAnnU6Ngt4wRjTBugB7HNVrMrNsrLg8cfJuaQrSYHfUL/+jQQGdi31\nVGNgyhS49lro3NmWLlq1quR4laqFXFmy6AFsNcYkGGNygQ+BK0qc0xb4wfF4cdFxR1LxMsYsBDDG\nZBpjdDHqmurFFyE5mR2PRmFMPrGxT5d6Wn4+3H23nTX2qqvghx8gIqKSY1WqlnJlsmgEJDo9T3Ls\nc7YGuMrxeBAQKCJhQEsgXUQ+FpFVIvKCo6RyHBEZISLxIhKvA++qqb174fnnyRrel+TCL4mMHEmd\nOieOjcnMhCuvhNdeg1GjdDJApSqbu6f7eBg4X0RWAecDu4ECbMN7b8fx7kBTYGjJi40xbxpj4owx\ncRH6E7N6GjcOcnJIuK0QT88AmjR57IRT9uyxCxR9/bVNFs8/Dx7u/perVC3jyt5QuwHniRaiHPuK\nGWP24ChZiEgAcLUxJl1EkoDVxpgEx7FPgXOAd1wYr6ps69fD229z6ImrOJg9n9jYifj4hB93ytq1\ndl6n1FT44gvbTVYpVflc+ftsBdBCRGJFxAcYDHzufIKIhItIUQxjgelO14aISFFx4UJgvQtjVe7w\n739jAuqy7bKd+Pg0JCrq/uMOL1oE555r2yp++kkThVLu5LJkYYzJB+4GvgU2AHONMX+JyAQRGeg4\nrQ+wSUQ2Aw2AiY5rC7BVUN+LyFpAgLdcFatygx9+gAULOPDCIDKOrHBMNX5sGdR58+Cyy6BJE/jt\nN+jSxY2xKqVq9rKqqooqLIS4OArTDhD/gT94CHFxa/HwsLWiqanQooXdvvsOgoLcHK9SNZguq6qq\nrjlzYNUq9k65mCPZm2jadFJxogA7c2x6Orz1liYKpaoKne5DVa6jR+GRRyg4pxM7GnxDkF8vwsIG\nFh9eu9bOGHvnndChgxvjVEodR0sWqnJNmwaJiSQ9043c3GSaNXu+eNlbY+C+++wKqhMmuDlOpdRx\ntGShKs+BA/DMM+Re25ddXvMID72S4OB/FB/++GNYvBheeQXq1XNjnEqpE2jJQlWeCRMgM5OdDzWg\noOAIsbHHpgI7etRO49Ghg10SVSlVtWjJQlWOLVvg9dc5ev/17Mn+Hw0b3kbduq2LD0+eDDt32h61\nXvqvUqkqR0sWqnKMGQO+vmy/KRsRL2JixhUfSkyEZ5+Fq6+2044rpaoeTRbK9X75BT7+mMMThrAv\n4xOioh7E1zey+PC//20btydPdmOMSqlT0mShXMsYePhhTMOz2NZnE97e4TRu/O/iwz/9BB9+aGeS\njYlxX5hKqVPTZKFca/58WL6ctMmDSc9YQpMmj+PlZUfaFRTYrrJRUTB6tJvjVEqdkjYlKtfJyYEx\nYzAd2rGtxWL8CmKJjDzW1Wn6dFi1Cj74AOrWdWOcSqkyaclCuc7rr0NCAimT+5OVtYbY2Il4ePgC\ndjqPRx6xa2dff72b41RKlUlLFso10tJgwgQK+l3I9sC5BHh3o379Y1nhySfh4EF4+WVwDOBWSlVh\nWrJQrvHMM5Cezp7xXcjJ2UmzZs9RtHTJhg12lPbtt+vU40pVF5osVMXbsQOmTSNv+GB25s0gNPRS\nQkMvAmznqPvvt20UTz/t3jCVUuWn1VCq4j3yCHh6kjiyHvnpaTRtOqn40Bdf2DUqpk4FXTZdqepD\nSxaqYq1YAR98QPaY20jKeIcGDW4iMLAzYDtHPfggtGkDI0e6OU6l1GnRkoWqOI4BeEREsGNQOia1\nkJiYp4oPv/QSbNsG334L3t5ujFMpddq0ZKEqzhdfwNKlZD07gr0H59Co0d3UqRMDwJ49to1i4EC4\n5BL3hqmUOn2aLFTFyMuzkzy1akVC3Go8PQNp0uSR4sNjxthTpkxxY4xKqTOmyUJVjLffhk2bSJ/8\nLw6mLaBx4zF4e4cBsHw5zJ5t2yuaNXNznEqpM1KuZCEiH4vIACnqKK+Us4wMGDcOc15vEhp+gY9P\nI6Ki7gOgsBDuvRcaNrSdpJRS1VN5b/6vATcCW0Rkkoi0cmFMqrp5/nnYv58DkwaQcXg5sbET8PSs\nA8CsWbaD1HPPQWCgm+NUSp2xciULY8wiY8xNQFdgB7BIRJaJyDAR0X4ttVlSErz4IoU3Xk+Cxzv4\n+7elQYN/AbbAMWYMnHMO3HSTm+NUSv0t5a5WEpEwYCgwHFgFvIxNHgtdEpmqHh5/HAoLSR7dgaNH\nt9C06SQ8PGyP7KefhpQUmDYNPLQCU6lqrVzjLETkE6AVMBv4pzEm2XHofyIS76rgVBW3Zg3MnEn+\nqLvZcfg/BAf3JizscgA2b7ajtIcNg+7d3RynUupvK++gvGnGmMWlHTDGxFVgPKo6GTUKQkJIGlqX\nvJQUmjb9FHFMIfvgg+DnZ+cTVEpVf+WtHGgrIiFFT0QkVER0woba7NtvYeFCcp+8n8QDrxAefhXB\nwecA8PXXsGABPPEEnHWWm+NUSlWI8iaL240x6UVPjDFpwO2uCUlVeQUFdlqPpk3Z2XcvBQVHadrU\nFiFyc+2ssi1a2C6zSqmaobzJwlPk2BI1IuIJ+JR1kYj0E5FNIrJVRMaUcryJiHwvIn+KyBIRiSpx\nPEhEkkTklXLGqSrDzJmwbh1Hnr+PPSlvERl5O/7+tjf1f/5j2yteegl8yvwXopSqLsqbLL7BNmZf\nJCIXAR849p2UI6G8ClwGtAVuEJG2JU6bDMwyxnQEJgDPljj+FLC0nDGqypCVBY89BmefzfbWvyDi\nS5Mm4wDb82nCBLjsMhgwwM1xKqUqVHmTxWhgMXCnY/se+HcZ1/QAthpjEowxucCHwBUlzmkL/OB4\nvNj5uIh0AxoA35UzRlUZpkyB5GQyXhjO/v1ziY5+CF9f2zDxyCNw5IgtVSilapbyDsorNMa8boy5\nxrH91xhTUMZljYBEp+dJjn3O1gBXOR4PAgJFJMwxrciLwMOnegMRGSEi8SISv3///vJ8FPV37N0L\nzz2HuWoQCYHv4+0dQXS0/SuKj4cZM+C++6CVju9XqsYp79xQLURkvoisF5GEoq0C3v9h4HwRWQWc\nD+wGCoCRwFfGmKRTXWyMedMYE2eMiYvQZddcb/x4yMkhdXx/0tOX0KTJE3h5BWKMTRIREXaMnlKq\n5invOIsZwDjgJeACYBhlJ5rdQLTT8yjHvmLGmD04ShYiEgBcbYxJF5GeQG9H99wAwEdEMo0xJzSS\nq0qyfj289RZm5P+RkD0NP79mREaOAGDOHFi2DN55B4KD3RynUsolyttmUccY8z0gxpidxpjxQFlN\nmCuAFiISKyI+wGDgc+cTRCTcaSbbscB0AGPMTcaYxsaYGGzpY5YmCjcbPRoCAki5tzVZWWtp2vQZ\nPDx8yMy0y1jExcHQoe4OUinlKuUtWeQ4bupbRORubAkh4FQXGGPyHed+C3gC040xf4nIBCDeGPM5\n0Ad4VkQMttfTXWf4OZQrLV4MX35JwaQJbD/wAoGBcUREXAPAs8/aVfDmz9f5n5SqycQYU/ZJIt2B\nDUAItjtrEPCCMWa5a8Mrv7i4OBMfr9NUVbjCQju50/797FpyJwm7HqFTpx8IDb2AhARo2xauvdYu\nbqSUqn5EZGV5pm0qs2ThGC9xvTHmYSAT216haosPPoA//iBv9uvs2vMI9epdRmjoBQA89BB4edm1\nKpRSNVuZFQeOLrLnVkIsqqrJzraDJ7p0Ydc528jPT6dp00kALFoEn34Kjz4KkZFujlMp5XLlbbNY\nJSKfA/OArKKdxpiPXRKVqhqmTYNdu8ieMYmk3cNo0OBmAgI6kpdnu8o2bQoPPODuIJVSlaG8ycIP\nOAhc6LTPAJosaqoDB2DiRBgwgB2R30EKxMY+BcDrr9uetJ9+aqchV0rVfOVKFsYYbaeobZ56CjIz\nyXxmOHv3XkV09EP4+TVm/34YNw4uvhgGDnR3kEqpylLelfJmYEsSxzHG3FrhESn327IFXnsNbruN\nBN7CyyuYxo3HAnaE9uHD8PLLcGweYqVUTVfeaqgvnR77Yedx2lPx4agqYexY8PUlbfSlpCZeQ9Om\nz+HtXY/Vq+HNN+Gee2yXWaVU7VHeaqiPnJ+LyAfAzy6JSLnXsmXw0UeY8eNISH8OX98oGjW6p3j+\np3r17BRRSqnapbwli5JaAPUrMhBVBRhj5+446yz2D2vG4YQVtGo1A0/POsydC0uXwhtvQGiouwNV\nSlW28rZZHOb4Nou92DUuVE3y5Zfwyy8Uvv4K25MnULduB84662aOHLGrqHbuDMOHuztIpZQ7lLca\nKtDVgSg3KyiwbRUtWpDcv4CjCVvp0GEBIp48/zwkJsL774Onp7sDVUq5Q3nXsxgkIsFOz0NE5ErX\nhaUq3XvvwV9/kT/xUXYkPk1w8PnUq3cZO3fa6Tyuvx5693Z3kEopdynvPKHjjDGHip4YY9Kx61uo\nmiA72/aJ7daNXV03kpe3n2bNnkdEGDXKdpF94QV3B6mUcqfyJovSzjvTxnFV1bz2GiQmkvXcnSQm\nTaZBg1sICurBkiUwbx6MGQPR0WW+ilKqBitvsogXkSki0syxTQFWujIwVUkOHYKJEzF9L2ZzvXfx\n9AyiWbPJ5OfbrrJNmsCoUe4OUinlbuVNFvcAucD/gA+BbHShoprhhRcgNZW9E/7BoUM/06zZC/j4\nhPPWW/DnnzB5MtSp4+4glVLuVq7Fj6oDXfzoDCQnQ7Nm5F53Kb8PX0rdum3p3PlH0tI8aNECOnaE\nH37QaT2UqsnKu/hReXtDLRSREKfnoSLy7d8JUFUBEyZAXh4JIz0oKMigZcs3EPFg3DhIT9f5n5RS\nx5S3Girc0QMKAGNMGjqCu3rbsgXeeov0Rwaw98jHREc/TN267Vi3zk5BfscdtmShlFJQ/mRRKCKN\ni56ISAylzEKrqpHHHqOwrg+bL/0LP79YmjR5nIICO0I7ONjOUK6UUkXK2/31UeBnEfkREKA3MMJl\nUSnXio+HuXNJfLsPR3KX0KHDAjw9/Zk8GX77DebMgbAwdweplKpKyjvdxzciEodNEKuAT4GjrgxM\nudCYMRxtG8LO5suJCL+GsLD+bNoEjz0GV14Jgwe7O0ClVFVT3okEhwP3AVHAauAc4FeOX2ZVVQcL\nF2K+/57Nn7VCPPbQvPlUCgrg1lvB39+2V2ijtlKqpPK2WdwHdAd2GmMuALoA6ae+RFU5hYUwZgz7\nrw0nLWgTsbFP4+vbiGnT7DIW06bBWWe5O0ilVFVU3mSRbYzJBhARX2PMRqCV68JSLjFvHvmb/mDr\nHXkEBHSjUaO72LIFHn0ULr8cbrrJ3QEqpaqq8jZwJznGWXwKLBSRNGCn68JSFS4vDx59lIR/1yPX\nM50Orf6LMZ7ceiv4+sJ//6vVT0qpkytvA/cgx8PxIrIYCAa+cVlUquK99RYZ3tvY01to1OhuAgO7\nMW0a/PwzvPsuREa6O0ClVFWm033UBpmZFLZsxh+Ts8iNCaJHj43s3BlEx45w/vmwYIGWKpSqrSp0\nuo+/EUQ/EdkkIltFZEwpx5uIyPci8qeILBGRKMf+ziLyq4j85Th2vSvjrPGmTmX3P/aRGZlF8+Yv\n4+ERxG23gZcXvPmmJgqlVNlclixExBN4FbgMaAvcICJtS5w2GZhljOkITACedew/AvzLGNMO6AdM\ndZ6bSp2GAwfIfmcSO0Z4Uq/eZUREXMPrr8OPP8JLL0FUlLsDVEpVB64sWfQAthpjEowxudipza8o\ncU5b4AfH48VFx40xm40xWxyP9wD7gAgXxlpzPfMMW4dlYXy8aNHiVXbsEEaPhksvhWHD3B2cUqq6\ncGWyaAQkOj1Pcuxztga4yvF4EBAoIsdNNCEiPQAfYFvJNxCRESISLyLx+/fvr7DAa4ydOzkQ/x8O\nnAdNYsfh5xfL8OHg4aHVT0qp0+PSNotyeBg4X0RWAecDu4GCooMi0hCYDQwzxhSWvNgY86YxJs4Y\nExcRoQWPkgomjGXLXfn4e7cgOvoh3nzTrk8xeTI0blz29UopVcSV62jvBpxXbo5y7CvmqGK6CkBE\nAoCri6ZCF5EgYAHwqDFmuQvjrJnWrmWH1wfkNIDO7d4hMdGHhx+Giy+G2293d3BKqerGlSWLFUAL\nEYkVER9gMPC58wkiEi4iRTGMBaY79vsAn2Abv+e7MMYaK3PK3SRdC2eF3kRwcO/iBPHWW1r9pJQ6\nfS5LFsaYfOBu4FtgAzDXGPOXiEwQkYGO0/oAm0RkM9AAmOjYfx1wHjBURFY7ts6uirWmMUt/ZHO3\npXgaf5q2mco778DChfD88xAT4+7olFLVkQ7Kq2mMYc99zdl8VQKtYt8g3+MO2rWDuDhYtMg2biul\nVJHyDspzZZuFcoPcL2aRcHECwUda0iB6BAMGQEEBvP22Jgql1JnT20dNUlDAtg33UeAPLc+dz8yZ\nwjffwHPPQdOm7g5OKVWdabKoQdLmjiXl7ENE515F+uEOPPAAnHcejBzp7siUUtWdVkPVEIVZ6Wzm\nJfwO+tJ44GyuHAS5ufDOO1r9pJT6+zRZ1BC7Pr2eo43y6WgmMucDfxYsgKlToXlzd0emlKoJ9Ddn\nDXAkZSU7I74jYv1ZZLf6N/fdB716wT33uDsypVRNoSWLas4Yw5afr8HDF5r1mMl1/wfZ2TB9ulY/\nKaUqjiaLam7fptdIC9tBi5+78VHsJXzxBbz4IrRs6e7IlFI1iSaLaiwvL42tO0YRuBM8Lvof914O\nPXvCffe5OzKlVE2jyaIa275qJHneR+mQeB23fNOMrCxb/eTp6e7IlFI1jdZqV1OHDi1nT9aHRH3h\nxVfR/+XTT2HCBGjd2t2RKaVqIk0W1VBhYT6b1/wLnwNQ1/cJ7n40hB494MEH3R2ZUqqm0mqoamh3\n0lSyCrfQ7t1ARnqP4fBhmDEDvPRvUynlIlqyqGays3exfdvj1PsVltT/gPmfeTN+PLRt6+7IlFI1\nmf4WrWa2bLkH8nIIm9eOu5L6060bjBrl7qiUUjWdJotq5MCBzzh48HOazoCHfL4kPV344QetflJK\nuZ7eZqqJ/PxMtmy+h7pJ3sT/fif/S4jhqaegfXt3R6aUqg00WVQTO3aMJyc3kUbPBXJ56vN06QKj\nR7s7KqVUbaHJohrIzFxDUtJUGn7vx9h9H5Ga6cN3M8Db292RKaVqC00WVZwxhWze/H945/qx6eXe\nzDncl/HjoVMnd0emlKpNNFlUccnJb5GRsZzo/9Tjmtw5dOoEY8e6OyqlVG2jyaIKy81NISFhDCH7\no5jw5dMc8Arhqxng4+PuyJRStY0OyqvCtm59kIL8I+we1ZJZ3MLYsUKXLu6OSilVG2myqKJSUxex\nb98c6se3Z8TO2bRvlcdjj7k7KqVUbaXVUFVQQUE2W7aMxM8jmhcfuYMUacBn73lq9ZNSym20ZFEF\n7do1iaNHt3DojfN4J28Eox/IJS7O3VEppWozLVlUMUeObGLXrmcJzb+YAR9Mom39AzzxTLi7w1JK\n1XKaLKoQYwybN4/Ew6MOb4+8iWQa8vG8PHx93R2ZUqq202qoKiQl5X3S038gZ/1wXtkylIcvXUeP\n8/zcHZZSSrk2WYhIPxHZJCJbRWRMKcebiMj3IvKniCwRkSinY7eIyBbHdosr46wKcnL2sG3bg/j7\n92D4PffT2mcbT85v5+6wlFIKcGGyEBFP4FXgMqAtcIOIlFyiZzIwyxjTEZgAPOu4th4wDjgb6AGM\nE5FQV8XqbhkZK1i5sjsFBVl8/OIYEvMaMuPJXfgFaC2hUqpqcGXJogew1RiTYIzJBT4ErihxTlvg\nB8fjxU7HLwUWGmNSjTFpwEKgnwtjdZuUlPdZtao3It7kZS3hubmDeOCsDzlndB93h6aUUsVcmSwa\nAYlOz5Mc+5ytAa5yPB4EBIpIWDmvRURGiEi8iMTv37+/wgKvDMYUsG3baDZsGEJQ0Dm0aLGC4Te1\noCWbeGpmYxBxd4hKKVXM3Q3cDwPni8gq4HxgN1BQ3ouNMW8aY+KMMXERERGuirHC5ecfYu3agSQm\nPk9k5EgaNVrIJRcEk5RWl+nnvEWdS3q7O0SllDqOKyvFdwPRTs+jHPuKGWP24ChZiEgAcLUxJl1E\ndgN9Sly7xIWxVpojRzazdu1AsrO30bLlGxQW3sH53bPYscuDz3xupNebE9wdolJKncCVJYsVQAsR\niRURH2Aw8LnzCSISLiJFMYwFpjsefwtcIiKhjobtSxz7qrWDB79h5coe5OcfpFOn78k6fDvndkhn\nz658voseTv+VT0GHDu4OUymlTuCyZGGMyQfuxt7kNwBzjTF/icgEERnoOK0PsElENgMNgImOa1OB\np7AJZwUwwbGvWjLGkJj4ImvXDsDPrwldu65g158d6d3xEFnpeSy+ZBK9/3pDF9RWSlVZYoxxdwwV\nIi4uzsTHx7s7jBMUFGSzefMIUlJmEx5+NW3azOT3GTvoPyKKAJPBwsd/ovWTN2iDtlLKLURkpTGm\nzNnntCO/C+Xk7GHdukEcPvw7MTFP0qTxoyy6+3OufP0SIr32sejjwzT5543uDlMppcqkycJFMjJ+\nZ926K8nPz6Bdu4+J8L2IT3pPYfCye2kdtIdvfwvlrNax7g5TKaXKxd1dZ2ukvXtns2rVeXh4+NK1\n669EJDfj3ZYTuWbZg3RrcoAl22M4q3WIu8NUSqly02RRgexAu1Fs3PgvgoN70rXrCgLmruDlbrMY\nlvIcF8Zl8N26RoTW0/YJpVT1otVQFSQvL50NG24gNfUbIiPvonnkRGTE/UyYGc04JnPVgGzmfBSq\n040rpaolTRYV4MiRTY6Bdgm0bPlfIg+fT2HP83hw3TCmcj9Dbynkrbf98NJvWylVTWk11N908ODX\nrFx5Nvn5aXTq9AORP4eQ3+1shm/5N1O5n3vvhXeme2iiUEpVa5oszpAxhl27JrN27eXUqRNLtw4/\nE/L4XHKuv5nBvp8wI+cmxo2DqVPBQ79lpVQ1p793z4AdaHc7KSnvERFxLa39n8Tz4iFkrfiLq5r8\nyXc7W/HSS3D//e6OVCmlKoYmi9OUk7PbMdBuBTExT9Hkr87Izb1Izw9gQKtdLN8SxvTpMGyYuyNV\nSqmKoxUkpyEj4zdWruzOkSMbaNdmPjH/zUIu/ycpjbrSJ2oLKxLCmDdPE4VSqubRZFFOe/fOYtWq\n8/Hw8KNL9OdEXP8fmDSJnTeMoXf2d2zZ6cuXX8JVV5X9WkopVd1oNVQZCgvzSUgYTVLSFEJCLqBd\n2r14n3MDHD7Mpuc/o+9/BpKRAQsXwj/+4e5olVLKNTRZnEJeXhrr199AWtq3NIq8i2bzIvB44mpo\n2ZJVr/zCpSObIQI//gidOrk7WqWUch1NFieRlbWRdesGkp29g5aRU4i87zv45lW48UZ+vuUtBlzr\nT0iILVG0bOnuaJVSyrU0WZTi4MGvWL/+Bjw8/Ojk/R9CLp4IKSnw+ut80+QOrrpSaNzYJoro6LJf\nTymlqjtt4HZiB9o97xho14xua+4k5Py7wcsLli1jXtj/MfAKoVUrWLpUE4VSqvbQZOFQUHCUDRtu\nJiFhNBEhV9JlcmP87noSBgyAlSt5Z3U3Bg+Gs8+GxYuhfn13R6yUUpVHq6EoGmh3JYcPxxPrdzeN\nr/0a2b4DXngBHnqIF6cIDz8M/frBRx+Bv7+7I1ZKqcpV65PFkSNbWb26NwUFmbRPvpvwW9+GevVg\nyRJMr3N54gl4+mm49lp47z3w8XF3xEopVflqfbLw82tCvaBLiH47nbovvwIXXwzvv09heH3uuxde\neQWGD4c33gBPT3dHq5RS7lHrk4VHUjKtb/gD/voLxo2Dxx8n33hy61CYPRseesjWRokubqeUqsVq\nfbIgPNy2Vr/4IlxyCdnZMHgwfPaZrX565BFNFEoppcnC3x8WLQIRMjPhiivghx9s9dNdd7k7OKWU\nqho0WQCIkJoK/ftDfDzMmgU33+zuoJRSqurQZAEkJ8Mll8DmzbZr7BVXuDsipZSqWmp9skhMhAsu\ngL174euv4cIL3R2RUkpVPbV+BHe9etCmDXz/vSYKpZQ6mVpfsqhbF774wt1RKKVU1ebSkoWI9BOR\nTSKyVUTGlHK8sYgsFpFVIvKniPR37PcWkZkislZENojIWFfGqZRS6tRclixExBN4FbgMaAvcICJt\nS5z2GDDXGNMFGAy85th/LeBrjOkAdAPuEJEYV8WqlFLq1FxZsugBbDXGJBhjcoEPgZL9jAwQ5Hgc\nDOxx2l9XRLyAOkAukOHCWJVSSp2CK5NFIyDR6XmSY5+z8cAQEUkCvgLuceyfD2QBycAuYLIxJrXk\nG4jICBGJF5H4/fv3V3D4Simliri7N9QNwLvGmCigPzBbRDywpZICIBKIBR4SkaYlLzbGvGmMiTPG\nxEVERFRm3EopVau4Mp69OiIAAAZASURBVFnsBpzXkoty7HN2GzAXwBjzK+AHhAM3At8YY/KMMfuA\n/2/v3mLsquo4jn9/zniBYqCxxgextNFiERWsfWitMdg2vGh48RajhEuCiRdaETVqTAw8GTFGUUOi\nVTRam2gthhCCEIiGVMqlld4oL9qCGIitgYZgArb+fFjrZM5MZubMlB5We/bvk0xmzzpr7/mflZ75\nn7P37v+/HVg5xFgjImIWw0wWDwPLJC2V9BrKBezbp8x5ElgHIOkCSrI4XMfX1vEFwCrg8SHGGhER\nsxhasrB9DPgC8EfgAOWup/2SbpR0WZ12PXCNpN3AFuBK26bcRXWWpP2UpHOr7T3DijUiIman8rf5\n9CfpMPDEyzjEIuDISQrndJe1mCzrMVnWY8IorMV5tgde9B2ZZPFySXrEdq6LkLWYKusxWdZjQpfW\novXdUBERcRpIsoiIiIGSLCb8pHUAp5CsxWRZj8myHhM6sxa5ZhEREQPlk0VERAyUZBEREQN1PlkM\n6rnRJZLeUvuLPCZpv6SNrWNqTdJY7bdyR+tYWpN0jqStkh6vfWZWt46pJUnX1dfJPklbJL2udUzD\n1OlkMceeG11yDLje9jsoJVY+3/H1ANhIqUAQ8ANKzbblwEV0eF0kvRnYAKy0/U5gjFLSaGR1Olkw\nt54bnWH7adu76vbzlD8GU8vKd4akc4EPAZtax9KapLOBDwA/A7D9ku3n2kbV3DhwRu27cyYT/XhG\nUteTxVx6bnRS7Uz4HuDBtpE09X3gq8D/WgdyClhKKfJ5az0tt6kW+ewk2/8Evkspevo0cNT23W2j\nGq6uJ4uYhqSzgN8DX7TdyQ6Fkj4M/Mv2ztaxnCLGgRXALbUN8gtAZ6/xSVpIOQuxlNJ3Z4GkT7eN\nari6nizm0nOjUyS9mpIoNtve1jqehtYAl0k6RDk9uVbSr9uG1NRTwFO2e580t1KSR1etBw7aPmz7\nv8A24H2NYxqqrieLufTc6AxJopyTPmD7e63jacn2122fa3sJ5d/FfbZH+p3jbGw/A/xD0tvr0Drg\nsYYhtfYksErSmfV1s44Rv+A/3jqAlmwfk9TruTEG/Nz2/sZhtbQGuBzYK+nROvYN23c2jClOHdcC\nm+sbq78DVzWOpxnbD0raCuyi3EX4V0a89EfKfURExEBdPw0VERFzkGQREREDJVlERMRASRYRETFQ\nkkVERAyUZBEjqVZI/dwJ7nunpHMGzLlR0voTi+6VI2mJpH2t44jTX26djZFUa1vdUSuCTn1s3Pax\nVzyoBmZbh4j5yCeLGFXfBt4q6VFJN0m6RNL9km6n/s9jSX+QtLP2JPhMb0dJhyQtqu/KD0j6aZ1z\nt6Qz6pxfSPpo3/wbJO2StFfS8jr+Rkn31H03SXpC0qKpgUq6VNIDdf/f1dpcveN+px7zIUlvq+NL\nJN0naY+keyUtruNvknSbpN31q1d+Ymy65xAxH0kWMaq+BvzN9sW2v1LHVgAbbZ9ff77a9nuBlcAG\nSW+Y5jjLgB/bvhB4DvjIDL/viO0VwC3Al+vYtyhlQi6k1FJaPHWnmjy+Cayv+z8CfKlvylHb7wJ+\nRKmCC/BD4Je23w1sBm6u4zcDf7Z9UX2uvWoEc30OETNKsoguecj2wb6fN0jaDeygFJRcNs0+B233\nSp/sBJbMcOxt08x5P6UIIbbvAp6dZr9VlMZb22uJlSuA8/oe39L3vdeZbjXwm7r9q/p7ANZSkhW2\nj9s+Os/nEDGjTteGis55obch6RJK5dDVtv8j6U/AdG0xX+zbPg7MdArnxb4583ldCbjH9idneNwz\nbM/HXJ9DxIzyySJG1fPA62d5/Gzg2ZoollPe4Z9s24GPQ7kuASycZs4OYE3f9YgFks7ve/wTfd8f\nqNt/YaKF56eA++v2vcBn63HGane7iJMiySJGku1/U07t7JN00zRT7gLGJR2gXAzfMYQwbgAurbeu\nfgx4hpLE+uM8DFwJbJG0h5IQlvdNWVjHNwLX1bFrgavq+OX1Mer3D0raSznd1PX+6XES5dbZiCGR\n9FrgeC2Fv5rSZe7ieex/CFhp+8iwYoyYq1yziBiexcBvJb0KeAm4pnE8EScsnywiImKgXLOIiIiB\nkiwiImKgJIuIiBgoySIiIgZKsoiIiIH+D/LktBcZc50pAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6aa4633278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "ptr,=plt.plot(range(max_epoch),acc_train_his,'r-')\n",
    "pva,=plt.plot(range(max_epoch),acc_val_his,'b-')\n",
    "pte,=plt.plot(range(max_epoch),acc_test_his,'y-')\n",
    "plt.xlabel('training epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('accuracy on three sets')\n",
    "plt.legend((ptr,pva,pte),('train','validation','test'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Soft attention method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "img_size=28\n",
    "RNN_unit=img_size*img_size\n",
    "N_watch=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(dtype=tf.float32,shape=[None,img_size*img_size])\n",
    "y=tf.placeholder(dtype=tf.int64,shape=[None,10])\n",
    "\n",
    "predict_net=tf.layers.Dense(units=10)\n",
    "\n",
    "def get_next_input(output, i):\n",
    "    attention_weight=tf.nn.softmax(output)\n",
    "    weighted_graph=X*attention_weight\n",
    "    return weighted_graph\n",
    "\n",
    "lstm_cell = tf.nn.rnn_cell.LSTMCell(RNN_unit, state_is_tuple=True)\n",
    "init_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "inputs=[X]\n",
    "inputs.extend([0]*N_watch)\n",
    "outputs,_ = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, init_state, lstm_cell, loop_function=get_next_input)\n",
    "\n",
    "output=outputs[-1]\n",
    "score=predict_net(output)\n",
    "\n",
    "predictions = tf.argmax(score, 1)\n",
    "correct_predictions = tf.equal(predictions, tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "\n",
    "loss=tf.losses.softmax_cross_entropy(onehot_labels=y,logits=score)\n",
    "optimizier=tf.train.AdamOptimizer(learning_rate=1e-5)\n",
    "train_step = optimizier.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-11 14:37:58 start epoch 1/10:\n",
      "2018-04-11 14:37:58 iteration 1/859: current training loss = 2.301617\n",
      "2018-04-11 14:38:07 iteration 625/859: current training loss = 0.643018\n",
      "2018-04-11 14:38:09 iteration 859/859: current training loss = 0.261306\n",
      "2018-04-11 14:38:27 end epoch 1/10: acc_train=86.198% acc_val=87.225% acc_test=86.327%\n",
      "2018-04-11 14:38:27 start epoch 2/10:\n",
      "2018-04-11 14:38:27 iteration 1/859: current training loss = 0.455709\n",
      "2018-04-11 14:38:33 iteration 625/859: current training loss = 0.362654\n",
      "2018-04-11 14:38:37 iteration 859/859: current training loss = 0.316704\n",
      "2018-04-11 14:38:53 end epoch 2/10: acc_train=90.027% acc_val=90.669% acc_test=90.042%\n",
      "2018-04-11 14:38:53 start epoch 3/10:\n",
      "2018-04-11 14:38:53 iteration 1/859: current training loss = 0.243509\n",
      "2018-04-11 14:39:02 iteration 625/859: current training loss = 0.238555\n",
      "2018-04-11 14:39:04 iteration 859/859: current training loss = 0.185140\n",
      "2018-04-11 14:39:21 end epoch 3/10: acc_train=91.590% acc_val=91.787% acc_test=91.669%\n",
      "2018-04-11 14:39:21 start epoch 4/10:\n",
      "2018-04-11 14:39:21 iteration 1/859: current training loss = 0.312069\n",
      "2018-04-11 14:39:29 iteration 625/859: current training loss = 0.249153\n",
      "2018-04-11 14:39:31 iteration 859/859: current training loss = 0.604670\n",
      "2018-04-11 14:39:50 end epoch 4/10: acc_train=92.412% acc_val=92.862% acc_test=92.322%\n",
      "2018-04-11 14:39:50 start epoch 5/10:\n",
      "2018-04-11 14:39:50 iteration 1/859: current training loss = 0.276277\n",
      "2018-04-11 14:39:57 iteration 625/859: current training loss = 0.334301\n",
      "2018-04-11 14:40:01 iteration 859/859: current training loss = 0.461780\n",
      "2018-04-11 14:40:19 end epoch 5/10: acc_train=93.064% acc_val=93.494% acc_test=93.184%\n",
      "2018-04-11 14:40:19 start epoch 6/10:\n",
      "2018-04-11 14:40:19 iteration 1/859: current training loss = 0.135279\n",
      "2018-04-11 14:40:26 iteration 625/859: current training loss = 0.327032\n",
      "2018-04-11 14:40:29 iteration 859/859: current training loss = 0.067710\n",
      "2018-04-11 14:40:46 end epoch 6/10: acc_train=93.777% acc_val=93.800% acc_test=93.695%\n",
      "2018-04-11 14:40:46 start epoch 7/10:\n",
      "2018-04-11 14:40:46 iteration 1/859: current training loss = 0.275651\n",
      "2018-04-11 14:40:54 iteration 625/859: current training loss = 0.062595\n",
      "2018-04-11 14:40:56 iteration 859/859: current training loss = 0.142867\n",
      "2018-04-11 14:41:14 end epoch 7/10: acc_train=94.305% acc_val=94.337% acc_test=94.016%\n",
      "2018-04-11 14:41:14 start epoch 8/10:\n",
      "2018-04-11 14:41:14 iteration 1/859: current training loss = 0.166001\n",
      "2018-04-11 14:41:21 iteration 625/859: current training loss = 0.222460\n",
      "2018-04-11 14:41:24 iteration 859/859: current training loss = 0.123765\n",
      "2018-04-11 14:41:41 end epoch 8/10: acc_train=94.703% acc_val=94.478% acc_test=94.394%\n",
      "2018-04-11 14:41:41 start epoch 9/10:\n",
      "2018-04-11 14:41:41 iteration 1/859: current training loss = 0.184085\n",
      "2018-04-11 14:41:49 iteration 625/859: current training loss = 0.060120\n",
      "2018-04-11 14:41:51 iteration 859/859: current training loss = 0.040815\n",
      "2018-04-11 14:42:08 end epoch 9/10: acc_train=95.066% acc_val=94.763% acc_test=94.733%\n",
      "2018-04-11 14:42:08 start epoch 10/10:\n",
      "2018-04-11 14:42:08 iteration 1/859: current training loss = 0.115927\n",
      "2018-04-11 14:42:17 iteration 625/859: current training loss = 0.083176\n",
      "2018-04-11 14:42:19 iteration 859/859: current training loss = 0.112230\n",
      "2018-04-11 14:42:36 end epoch 10/10: acc_train=95.534% acc_val=95.209% acc_test=95.036%\n"
     ]
    }
   ],
   "source": [
    "max_epoch=10\n",
    "print_every=200\n",
    "\n",
    "def train():\n",
    "    num_iteration=num_train//batch_size\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=mnist.train.next_batch(batch_size)\n",
    "        loss_num,_ = sess.run([loss,train_step],feed_dict={X:images,y:labels})\n",
    "        if it==0 or (it+1)%print_every==0 or it==num_iteration-1:\n",
    "            print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "                  'iteration %d/%d:' % (it+1,num_iteration),'current training loss = %f' % (loss_num))\n",
    "            \n",
    "def eval(dataset,num_iteration):\n",
    "    total_loss=0\n",
    "    total_accuracy=0\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=dataset.next_batch(batch_size)\n",
    "        loss_num,accuracy_num = sess.run([loss,accuracy],feed_dict={X:images,y:labels})\n",
    "        total_loss+=loss_num\n",
    "        total_accuracy+=accuracy_num\n",
    "    total_loss/=num_iteration\n",
    "    total_accuracy/=num_iteration\n",
    "    return total_loss,total_accuracy\n",
    "    \n",
    "acc_train_his=[]\n",
    "acc_val_his=[]\n",
    "acc_test_his=[]\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(max_epoch):\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'start epoch %d/%d:' % (epoch+1,max_epoch))\n",
    "        train()\n",
    "        loss_train,acc_train=eval(mnist.train,2000)\n",
    "        loss_val,acc_val=eval(mnist.validation,500)\n",
    "        loss_test,acc_test=eval(mnist.test,1000)\n",
    "        acc_train_his.append(acc_train)\n",
    "        acc_val_his.append(acc_val)\n",
    "        acc_test_his.append(acc_test)\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'end epoch %d/%d:' % (epoch+1,max_epoch),\n",
    "             'acc_train=%.3f%% acc_val=%.3f%% acc_test=%.3f%%' % (acc_train*100.0,acc_val*100.0,acc_test*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hard attention REINFORCE method (Glimpse Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size=64\n",
    "img_size=28\n",
    "sensor_unit=256\n",
    "lstm_size=256\n",
    "N_glimpse=10\n",
    "MC_test=128\n",
    "loc_std=0.2\n",
    "tot_size=batch_size*MC_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glimpse_Network():\n",
    "    def __init__(self):\n",
    "        self.glimspe_size=[5,7,9]\n",
    "        self.concat_size=9\n",
    "        self.img_net=tf.layers.Dense(units=sensor_unit,name='glimpse_net/img_net')\n",
    "        self.loc_net=tf.layers.Dense(units=sensor_unit,name='glimpse_net/loc_net')\n",
    "        \n",
    "    def glimpse_sensor(self,image,loc):\n",
    "        glimpses_list=[tf.image.extract_glimpse(input=image,size=[gs,gs],offsets=loc) for gs in self.glimspe_size]\n",
    "        glimpses_norm=[tf.image.resize_bilinear(g,[self.concat_size,self.concat_size]) for g in glimpses_list]\n",
    "        glimpses=tf.concat(values=glimpses_norm,axis=3)  # batch_size*concat_size*concat_size*3\n",
    "        return glimpses\n",
    "    \n",
    "    def forward(self,image,loc):\n",
    "        glimpses=self.glimpse_sensor(image,loc) # tot_size*concat_size*concat_size*3\n",
    "        g_image=self.img_net(inputs=tf.layers.flatten(glimpses))\n",
    "        g_loc=self.loc_net(inputs=loc)\n",
    "        g_out=tf.nn.relu(g_image+g_loc)\n",
    "        return g_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(dtype=tf.float32,shape=[None,28,28,1])\n",
    "y=tf.placeholder(dtype=tf.int64,shape=[None,10])\n",
    "start_location=tf.random_uniform(shape=[tot_size,2],minval=-1.0,maxval=1.0)\n",
    "gNet=Glimpse_Network()\n",
    "\n",
    "lstm_cell = tf.contrib.rnn.LSTMCell(lstm_size)\n",
    "state = lstm_cell.zero_state(tot_size, tf.float32)\n",
    "\n",
    "emission_net=tf.layers.Dense(units=2,name='emission_net')\n",
    "baseline_net=tf.layers.Dense(units=1,name='baseline_net')\n",
    "predict_net=tf.layers.Dense(units=10,name='predict_net')\n",
    "\n",
    "def loglikelihood(sample,mean):\n",
    "    gaussian=tf.distributions.Normal(loc=mean,scale=tf.constant([loc_std,loc_std]))\n",
    "    llh=-gaussian.log_prob(sample)\n",
    "    return tf.reduce_sum(llh,axis=1)\n",
    "    \n",
    "loc_his=[]\n",
    "loglikelihood_his=[]\n",
    "baseline_his=[]\n",
    "normalized_loc=start_location\n",
    "for ng in range(N_glimpse):\n",
    "    loc_his.append(normalized_loc)\n",
    "    \n",
    "    # extract glimpse\n",
    "    glimpses_out=gNet.forward(X,normalized_loc)\n",
    "    \n",
    "    # RNN\n",
    "    lstm_output,state=lstm_cell(glimpses_out,state)\n",
    "    \n",
    "    # emit mean of location\n",
    "    loc_mean=emission_net(inputs=lstm_output)\n",
    "    \n",
    "    # sample next location by gaussian distribution centered at loc_mean\n",
    "    loc_sample=tf.random_normal(shape=(tot_size,2),mean=loc_mean,stddev=loc_std)\n",
    "    \n",
    "    # calculate the -loglikelihood of the sampled position\n",
    "    llh=loglikelihood(loc_sample,loc_mean)\n",
    "    loglikelihood_his.append(llh)\n",
    "    \n",
    "    # normalize the location for next input\n",
    "    normalized_loc=tf.tanh(loc_sample)\n",
    "    \n",
    "    # output time independent baseline\n",
    "    baseline=baseline_net(inputs=lstm_output)\n",
    "    baseline_his.append(tf.squeeze(baseline))\n",
    "\n",
    "# pack data for calculation\n",
    "baseline_his=tf.stack(baseline_his)\n",
    "loglikelihood_his=tf.stack(loglikelihood_his)\n",
    "\n",
    "# make prediction\n",
    "score=predict_net(inputs=lstm_output)\n",
    "prediction=tf.argmax(score,1)\n",
    "\n",
    "# calculate reward, do variance reduction and calculate reinforced loglikelihood\n",
    "reward=tf.cast(tf.equal(prediction,tf.argmax(y,1)),dtype=tf.float32)\n",
    "reduce_var_reward=reward-tf.stop_gradient(baseline_his)\n",
    "reinforce_llh=tf.reduce_mean(loglikelihood_his*reduce_var_reward)\n",
    "\n",
    "# regression baseline towards reward\n",
    "baseline_mse=tf.reduce_mean(tf.square(reward-baseline_his))\n",
    "\n",
    "# softmax to output\n",
    "softmax_loss=tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=y,logits=score))\n",
    "\n",
    "# summarize loss\n",
    "loss=reinforce_llh+baseline_mse+softmax_loss\n",
    "\n",
    "\n",
    "optimizier=tf.train.AdamOptimizer(learning_rate=1e-6)\n",
    "train_step = optimizier.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_epoch=100\n",
    "print_every=50\n",
    "num_iteration=num_train//batch_size\n",
    "loss_his=[]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     print(tf.global_variables())\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(max_epoch):\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'start epoch %d/%d' % (epoch+1,max_epoch))\n",
    "        tot_loss=0\n",
    "        for it in range(num_iteration):\n",
    "            images,labels=mnist.train.next_batch(batch_size)\n",
    "            # prepare data for monte carlo test\n",
    "            images=np.tile(images,(MC_test,1))\n",
    "            labels=np.tile(labels,(MC_test,1))\n",
    "            feed_dict={X:images.reshape(tot_size,28,28,1),y:labels}\n",
    "            loss_1,loss_2,loss_3,loss_out,_=sess.run([reinforce_llh,baseline_mse,softmax_loss,loss,train_step],\n",
    "                                                     feed_dict=feed_dict)\n",
    "            tot_loss+=loss_out\n",
    "            if it==0 or (it+1)%print_every==0 or it==num_iteration-1:\n",
    "                print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "                      'iter',it+1,': loss_1 =',loss_1,'loss_2 =',loss_2,'loss_3 =',loss_3,'total_loss =',loss_out)\n",
    "        loss_his.append(tot_loss/num_iteration)\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "              'end epoch, average loss =',(tot_loss/num_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "ptr,=plt.plot(range(max_epoch),loss_his)\n",
    "plt.xlabel('training epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mean=tf.zeros((100,2),dtype=tf.float32)\n",
    "# std=tf.constant([1,1],dtype=tf.float32)\n",
    "# gaussian=tf.distributions.Normal(loc=mean,scale=std)\n",
    "# rand=tf.random_normal(shape=(100,2),mean=0,stddev=1)\n",
    "# sampled=mean+rand\n",
    "# prob=-gaussian.log_prob(sampled)\n",
    "# prob=tf.reduce_mean(tf.reduce_sum(prob,1))\n",
    "# with tf.Session() as sess:\n",
    "#     out=sess.run([prob])\n",
    "#     print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
