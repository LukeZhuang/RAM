{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"mnist/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train=mnist.train.num_examples\n",
    "num_val=mnist.validation.num_examples\n",
    "num_test=mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size=64\n",
    "img_size=28\n",
    "sensor_unit=256\n",
    "lstm_size=256\n",
    "N_glimpse=10\n",
    "MC_test=128\n",
    "loc_std=0.2\n",
    "tot_size=batch_size*MC_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glimpse_Network():\n",
    "    def __init__(self):\n",
    "        self.glimspe_size=[5,10,15]\n",
    "        self.concat_size=5\n",
    "        self.img_net=tf.layers.Dense(units=sensor_unit,name='glimpse_net/img_net')\n",
    "        self.loc_net=tf.layers.Dense(units=sensor_unit,name='glimpse_net/loc_net')\n",
    "        \n",
    "    def glimpse_sensor(self,image,loc):\n",
    "        glimpses_list=[tf.image.extract_glimpse(input=image,size=[gs,gs],offsets=loc) for gs in self.glimspe_size]\n",
    "        glimpses_norm=[tf.image.resize_bilinear(g,[self.concat_size,self.concat_size]) for g in glimpses_list]\n",
    "        glimpses=tf.concat(values=glimpses_norm,axis=3)  # batch_size*concat_size*concat_size*3\n",
    "        return glimpses\n",
    "    \n",
    "    def forward(self,image,loc):\n",
    "        glimpses=self.glimpse_sensor(image,loc) # tot_size*concat_size*concat_size*3\n",
    "        glimpses=tf.stop_gradient(glimpses)  # gradient has no need to flow through glimpses\n",
    "        g_image=tf.nn.relu(self.img_net(inputs=tf.layers.flatten(glimpses)))\n",
    "        g_loc=tf.nn.relu(self.loc_net(inputs=loc))\n",
    "        g_out=tf.nn.relu(g_image+g_loc)\n",
    "        return g_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(dtype=tf.float32,shape=[None,28,28,1])\n",
    "y=tf.placeholder(dtype=tf.int64,shape=[None,10])\n",
    "this_size=tf.shape(X)[0]\n",
    "start_location=tf.random_uniform(shape=[this_size,2],minval=-1.0,maxval=1.0)\n",
    "gNet=Glimpse_Network()\n",
    "\n",
    "lstm_cell = tf.contrib.rnn.LSTMCell(lstm_size)\n",
    "state = lstm_cell.zero_state(this_size, tf.float32)\n",
    "\n",
    "emission_net=tf.layers.Dense(units=2,name='emission_net')\n",
    "baseline_net=tf.layers.Dense(units=1,name='baseline_net')\n",
    "predict_net=tf.layers.Dense(units=10,name='predict_net')\n",
    "\n",
    "def loglikelihood(sample,mean):\n",
    "    gaussian=tf.distributions.Normal(loc=mean,scale=tf.constant([loc_std,loc_std]))\n",
    "    llh=-gaussian.log_prob(sample)\n",
    "    return tf.reduce_sum(llh,axis=1)\n",
    "    \n",
    "loc_his=[]\n",
    "loglikelihood_his=[]\n",
    "baseline_his=[]\n",
    "normalized_loc=start_location\n",
    "for ng in range(N_glimpse):\n",
    "    loc_his.append(normalized_loc)\n",
    "    \n",
    "    # extract glimpse\n",
    "    glimpses_out=gNet.forward(X,normalized_loc)\n",
    "    \n",
    "    # RNN\n",
    "    lstm_output,state=lstm_cell(glimpses_out,state)\n",
    "    \n",
    "    # emit mean of location\n",
    "    loc_mean=emission_net(inputs=lstm_output)\n",
    "    \n",
    "    # sample next location by gaussian distribution centered at loc_mean\n",
    "    loc_sample=tf.random_normal(shape=(this_size,2),mean=loc_mean,stddev=loc_std)\n",
    "    loc_sample=tf.stop_gradient(loc_sample)\n",
    "    \n",
    "    # calculate the -loglikelihood of the sampled position\n",
    "    llh=loglikelihood(loc_sample,loc_mean)\n",
    "    loglikelihood_his.append(llh)\n",
    "    \n",
    "    # normalize the location for next input\n",
    "    normalized_loc=tf.tanh(loc_sample)\n",
    "    normalized_loc=tf.stop_gradient(normalized_loc)\n",
    "    \n",
    "    # output time independent baseline\n",
    "    # stop gradient on lstm output (************? require proof************)\n",
    "    baseline=baseline_net(inputs=tf.stop_gradient(lstm_output))\n",
    "    baseline_his.append(tf.squeeze(baseline))\n",
    "\n",
    "# pack data for calculation\n",
    "baseline_his=tf.stack(baseline_his)\n",
    "loglikelihood_his=tf.stack(loglikelihood_his)\n",
    "\n",
    "# make prediction\n",
    "score=predict_net(inputs=lstm_output)\n",
    "prediction=tf.argmax(score,1)\n",
    "\n",
    "# calculate reward, do variance reduction and calculate reinforced loglikelihood\n",
    "reward=tf.cast(tf.equal(prediction,tf.argmax(y,1)),dtype=tf.float32)\n",
    "# stop gradient on reward (************? require proof************)\n",
    "reward=tf.stop_gradient(reward)\n",
    "accuracy=tf.reduce_sum(reward)/tf.cast(this_size,dtype=tf.float32)\n",
    "reduce_var_reward=reward-tf.stop_gradient(baseline_his)\n",
    "reinforce_llh=tf.reduce_mean(loglikelihood_his*reduce_var_reward)\n",
    "\n",
    "# regression baseline towards reward\n",
    "baseline_mse=tf.reduce_mean(tf.square(reward-baseline_his))\n",
    "\n",
    "# softmax to output\n",
    "softmax_loss=tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=y,logits=score))\n",
    "\n",
    "# summarize loss\n",
    "loss=reinforce_llh+baseline_mse+softmax_loss\n",
    "\n",
    "\n",
    "optimizier=tf.train.RMSPropOptimizer(learning_rate=1e-4)\n",
    "train_step = optimizier.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch=50\n",
    "print_every=50\n",
    "\n",
    "def train():\n",
    "    num_iteration=num_train//batch_size\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=mnist.train.next_batch(batch_size)\n",
    "        # prepare data for monte carlo test\n",
    "        images=np.tile(images,(MC_test,1))\n",
    "        labels=np.tile(labels,(MC_test,1))\n",
    "        feed_dict={X:images.reshape(tot_size,28,28,1),y:labels}\n",
    "        l1,l2,l3,loss_num,acc_num,_ = sess.run([reinforce_llh,baseline_mse,softmax_loss,loss,accuracy,train_step],feed_dict=feed_dict)\n",
    "        if it==0 or (it+1)%print_every==0 or it==num_iteration-1:\n",
    "            print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),\n",
    "                  'iteration %d/%d:' % (it+1,num_iteration),'loss=%8f, accuracy=%.3f%%' % (loss_num,acc_num*100.0),l1,l2,l3)\n",
    "\n",
    "def eval(dataset,num_iteration):\n",
    "    total_loss=0\n",
    "    total_accuracy=0\n",
    "    for it in range(num_iteration):\n",
    "        images,labels=dataset.next_batch(batch_size)\n",
    "        # no Monte Carlo test during evaludation step\n",
    "        loss_num,accuracy_num = sess.run([loss,accuracy],feed_dict={X:images.reshape(batch_size,28,28,1),y:labels})\n",
    "        total_loss+=loss_num\n",
    "        total_accuracy+=accuracy_num\n",
    "    total_loss/=num_iteration\n",
    "    total_accuracy/=num_iteration\n",
    "    return total_loss,total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 15:05:05 start epoch 1/50\n",
      "2018-04-17 15:05:06 iteration 1/859: loss=2.340601, accuracy=7.764% -0.0660799 0.10659226 2.3000884\n",
      "2018-04-17 15:05:23 iteration 50/859: loss=2.371061, accuracy=6.567% -0.0579383 0.08750048 2.3414984\n",
      "2018-04-17 15:05:41 iteration 100/859: loss=2.351091, accuracy=8.826% -0.04838727 0.099885 2.2995932\n",
      "2018-04-17 15:05:58 iteration 150/859: loss=2.393584, accuracy=35.889% -0.13810106 0.3339675 2.197718\n",
      "2018-04-17 15:06:15 iteration 200/859: loss=2.194683, accuracy=41.321% -0.11717065 0.31690976 1.9949437\n",
      "2018-04-17 15:06:32 iteration 250/859: loss=1.867576, accuracy=44.202% -0.07630874 0.3228636 1.6210215\n",
      "2018-04-17 15:06:49 iteration 300/859: loss=1.741830, accuracy=47.742% -0.053873636 0.31629616 1.4794073\n",
      "2018-04-17 15:07:07 iteration 350/859: loss=1.475344, accuracy=60.913% -0.06830828 0.30933028 1.2343221\n",
      "2018-04-17 15:07:24 iteration 400/859: loss=1.289526, accuracy=65.466% -0.07854841 0.29451975 1.0735544\n",
      "2018-04-17 15:07:41 iteration 450/859: loss=1.516311, accuracy=57.776% -0.04840688 0.29431483 1.2704035\n",
      "2018-04-17 15:07:59 iteration 500/859: loss=1.155632, accuracy=67.664% -0.06955258 0.2696743 0.9555105\n",
      "2018-04-17 15:08:16 iteration 550/859: loss=1.220991, accuracy=66.101% -0.03472069 0.2742742 0.9814375\n",
      "2018-04-17 15:08:33 iteration 600/859: loss=1.333844, accuracy=63.281% -0.03835223 0.2802213 1.0919752\n",
      "2018-04-17 15:08:51 iteration 650/859: loss=1.120698, accuracy=71.191% -0.052586805 0.25523508 0.9180496\n",
      "2018-04-17 15:09:08 iteration 700/859: loss=1.035929, accuracy=73.352% -0.07209198 0.24862751 0.85939336\n",
      "2018-04-17 15:09:25 iteration 750/859: loss=1.149469, accuracy=69.470% -0.027492542 0.24908538 0.92787665\n",
      "2018-04-17 15:09:43 iteration 800/859: loss=1.161384, accuracy=67.896% -0.039813507 0.2633633 0.937834\n",
      "2018-04-17 15:10:00 iteration 850/859: loss=0.983294, accuracy=75.452% -0.055730898 0.24323578 0.7957889\n",
      "2018-04-17 15:10:03 iteration 859/859: loss=1.142400, accuracy=66.797% -0.034403175 0.24815564 0.92864704\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 15:10:15 end epoch 1/50: acc_train=67.591% acc_val=68.772% acc_test=67.604%\n",
      "2018-04-17 15:10:15 start epoch 2/50\n",
      "2018-04-17 15:10:15 iteration 1/859: loss=0.952388, accuracy=76.648% -0.058365118 0.23100963 0.7797438\n",
      "2018-04-17 15:10:33 iteration 50/859: loss=0.985548, accuracy=73.120% -0.054758407 0.23531453 0.8049917\n",
      "2018-04-17 15:10:50 iteration 100/859: loss=1.201245, accuracy=69.836% -0.023610601 0.24508151 0.979774\n",
      "2018-04-17 15:11:08 iteration 150/859: loss=1.053927, accuracy=74.097% -0.0200173 0.23684144 0.8371029\n",
      "2018-04-17 15:11:25 iteration 200/859: loss=0.894813, accuracy=76.648% -0.048746478 0.2179966 0.72556263\n",
      "2018-04-17 15:11:42 iteration 250/859: loss=1.031745, accuracy=72.229% -0.044975873 0.23551688 0.84120405\n",
      "2018-04-17 15:12:00 iteration 300/859: loss=1.003118, accuracy=74.231% -0.037277184 0.23458211 0.8058128\n",
      "2018-04-17 15:12:17 iteration 350/859: loss=0.892527, accuracy=75.635% -0.03995984 0.22173505 0.71075225\n",
      "2018-04-17 15:12:35 iteration 400/859: loss=0.679685, accuracy=81.775% -0.0591679 0.20423007 0.5346223\n",
      "2018-04-17 15:12:52 iteration 450/859: loss=1.102960, accuracy=71.338% -0.021964066 0.254069 0.8708555\n",
      "2018-04-17 15:13:10 iteration 500/859: loss=0.717530, accuracy=80.981% -0.04874457 0.20085278 0.56542224\n",
      "2018-04-17 15:13:27 iteration 550/859: loss=0.776368, accuracy=79.932% -0.044613406 0.19736047 0.6236212\n",
      "2018-04-17 15:13:44 iteration 600/859: loss=0.859568, accuracy=77.234% -0.026039222 0.2117434 0.6738641\n",
      "2018-04-17 15:14:02 iteration 650/859: loss=0.524787, accuracy=86.255% -0.053146742 0.17247498 0.4054584\n",
      "2018-04-17 15:14:19 iteration 700/859: loss=0.762051, accuracy=81.165% -0.03169211 0.1948168 0.5989268\n",
      "2018-04-17 15:14:36 iteration 750/859: loss=0.942538, accuracy=74.329% -0.023214908 0.21594858 0.7498039\n",
      "2018-04-17 15:14:54 iteration 800/859: loss=0.794590, accuracy=78.845% -0.013017086 0.20364623 0.60396063\n",
      "2018-04-17 15:15:11 iteration 850/859: loss=0.701421, accuracy=83.911% -0.03687234 0.18607424 0.5522196\n",
      "2018-04-17 15:15:14 iteration 859/859: loss=0.935431, accuracy=76.880% -0.015013464 0.20868716 0.74175763\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 15:15:25 end epoch 2/50: acc_train=79.669% acc_val=80.424% acc_test=80.417%\n",
      "2018-04-17 15:15:25 start epoch 3/50\n",
      "2018-04-17 15:15:26 iteration 1/859: loss=0.726188, accuracy=81.360% -0.031830706 0.18516028 0.57285863\n",
      "2018-04-17 15:15:43 iteration 50/859: loss=0.752020, accuracy=82.031% -0.03116779 0.18656032 0.5966279\n",
      "2018-04-17 15:16:00 iteration 100/859: loss=0.584337, accuracy=85.144% -0.055166155 0.17240751 0.46709526\n",
      "2018-04-17 15:16:18 iteration 150/859: loss=0.425027, accuracy=89.929% -0.06464443 0.14944656 0.3402248\n",
      "2018-04-17 15:16:35 iteration 200/859: loss=0.952189, accuracy=74.939% -0.004800401 0.2176446 0.73934436\n",
      "2018-04-17 15:16:53 iteration 250/859: loss=0.631384, accuracy=84.021% -0.02895621 0.1709257 0.4894144\n",
      "2018-04-17 15:17:10 iteration 300/859: loss=0.366873, accuracy=91.199% -0.06599886 0.13819866 0.29467356\n",
      "2018-04-17 15:17:27 iteration 350/859: loss=0.577081, accuracy=84.595% -0.036414526 0.16131301 0.4521829\n",
      "2018-04-17 15:17:45 iteration 400/859: loss=0.650148, accuracy=83.643% -0.024727818 0.17355928 0.5013162\n",
      "2018-04-17 15:18:03 iteration 450/859: loss=0.587759, accuracy=84.436% -0.030277472 0.16387682 0.45415974\n",
      "2018-04-17 15:18:20 iteration 500/859: loss=0.636254, accuracy=83.801% -0.021158073 0.17233188 0.4850803\n",
      "2018-04-17 15:18:38 iteration 550/859: loss=0.711600, accuracy=81.775% -0.020088078 0.17915642 0.5525315\n",
      "2018-04-17 15:18:56 iteration 600/859: loss=0.684984, accuracy=83.301% -0.022249924 0.17309546 0.53413814\n",
      "2018-04-17 15:19:13 iteration 650/859: loss=0.653613, accuracy=83.533% -0.01992167 0.16467282 0.50886214\n",
      "2018-04-17 15:19:31 iteration 700/859: loss=0.639321, accuracy=82.910% -0.022566926 0.16736731 0.4945205\n",
      "2018-04-17 15:19:49 iteration 750/859: loss=0.469402, accuracy=87.439% -0.04632659 0.1450284 0.37070006\n",
      "2018-04-17 15:20:06 iteration 800/859: loss=0.609522, accuracy=83.594% -0.027782386 0.16349262 0.47381136\n",
      "2018-04-17 15:20:24 iteration 850/859: loss=0.546405, accuracy=87.280% -0.038667284 0.1416898 0.443382\n",
      "2018-04-17 15:20:27 iteration 859/859: loss=0.520467, accuracy=86.963% -0.038041472 0.14668366 0.4118247\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 15:20:37 end epoch 3/50: acc_train=84.791% acc_val=85.156% acc_test=85.760%\n",
      "2018-04-17 15:20:37 start epoch 4/50\n",
      "2018-04-17 15:20:38 iteration 1/859: loss=0.609929, accuracy=85.828% -0.030127775 0.14907458 0.49098182\n",
      "2018-04-17 15:20:55 iteration 50/859: loss=0.657492, accuracy=83.606% -0.018991673 0.16562605 0.5108573\n",
      "2018-04-17 15:21:12 iteration 100/859: loss=0.567365, accuracy=85.730% -0.019682545 0.15274346 0.43430448\n",
      "2018-04-17 15:21:30 iteration 150/859: loss=0.438650, accuracy=89.014% -0.032832917 0.12710238 0.3443802\n",
      "2018-04-17 15:21:47 iteration 200/859: loss=0.610565, accuracy=84.875% -0.031278722 0.15729079 0.48455292\n",
      "2018-04-17 15:22:05 iteration 250/859: loss=0.618795, accuracy=84.143% -0.016279224 0.16361462 0.4714597\n",
      "2018-04-17 15:22:22 iteration 300/859: loss=0.866783, accuracy=75.879% 0.006786154 0.20043099 0.65956557\n",
      "2018-04-17 15:22:40 iteration 350/859: loss=0.525215, accuracy=86.755% -0.0316796 0.14482942 0.41206533\n",
      "2018-04-17 15:22:57 iteration 400/859: loss=0.476047, accuracy=88.037% -0.027232105 0.13570654 0.36757237\n",
      "2018-04-17 15:23:15 iteration 450/859: loss=0.586670, accuracy=85.449% -0.014907777 0.14592993 0.45564824\n",
      "2018-04-17 15:23:32 iteration 500/859: loss=0.583562, accuracy=85.071% -0.019640058 0.15352221 0.4496799\n",
      "2018-04-17 15:23:50 iteration 550/859: loss=0.502188, accuracy=86.890% -0.024436997 0.12883328 0.39779204\n",
      "2018-04-17 15:24:07 iteration 600/859: loss=0.479454, accuracy=89.563% -0.02847516 0.12032537 0.38760418\n",
      "2018-04-17 15:24:25 iteration 650/859: loss=0.460109, accuracy=87.561% -0.028266478 0.13480133 0.3535739\n",
      "2018-04-17 15:24:43 iteration 700/859: loss=0.687795, accuracy=82.043% -0.0013786526 0.16597232 0.5232011\n",
      "2018-04-17 15:25:00 iteration 750/859: loss=0.426816, accuracy=89.197% -0.03845533 0.122440435 0.3428313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 15:25:17 iteration 800/859: loss=0.505682, accuracy=87.952% -0.027253022 0.12966695 0.40326774\n",
      "2018-04-17 15:25:35 iteration 850/859: loss=0.472082, accuracy=88.245% -0.018628458 0.13155381 0.3591565\n",
      "2018-04-17 15:25:38 iteration 859/859: loss=0.546305, accuracy=85.645% -0.007301538 0.14814614 0.40546077\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 15:25:48 end epoch 4/50: acc_train=88.781% acc_val=89.464% acc_test=89.167%\n",
      "2018-04-17 15:25:48 start epoch 5/50\n",
      "2018-04-17 15:25:49 iteration 1/859: loss=0.540579, accuracy=86.548% -0.016203891 0.13807347 0.41870943\n",
      "2018-04-17 15:26:06 iteration 50/859: loss=0.400855, accuracy=89.697% -0.020073716 0.114096306 0.3068328\n",
      "2018-04-17 15:26:24 iteration 100/859: loss=0.354162, accuracy=91.040% -0.04145334 0.10858004 0.287035\n",
      "2018-04-17 15:26:41 iteration 150/859: loss=0.514729, accuracy=86.804% -0.017538672 0.13140342 0.40086478\n",
      "2018-04-17 15:26:59 iteration 200/859: loss=0.493016, accuracy=87.317% -0.017919403 0.12906197 0.38187313\n",
      "2018-04-17 15:27:16 iteration 250/859: loss=0.633899, accuracy=83.728% -0.0024042 0.15224099 0.4840622\n",
      "2018-04-17 15:27:33 iteration 300/859: loss=0.509947, accuracy=86.157% -0.016264703 0.13421462 0.39199722\n",
      "2018-04-17 15:27:50 iteration 350/859: loss=0.304893, accuracy=91.821% -0.03109314 0.102460705 0.23352566\n",
      "2018-04-17 15:28:08 iteration 400/859: loss=0.495475, accuracy=88.281% -0.01876319 0.12090768 0.39333087\n",
      "2018-04-17 15:28:26 iteration 450/859: loss=0.419619, accuracy=88.525% -0.027502608 0.12090175 0.3262199\n",
      "2018-04-17 15:28:43 iteration 500/859: loss=0.285136, accuracy=92.859% -0.034093678 0.090634 0.22859612\n",
      "2018-04-17 15:29:01 iteration 550/859: loss=0.487028, accuracy=87.646% -0.014427297 0.12126265 0.380193\n",
      "2018-04-17 15:29:19 iteration 600/859: loss=0.556599, accuracy=87.158% -0.013809537 0.13112488 0.43928385\n",
      "2018-04-17 15:29:37 iteration 650/859: loss=0.493288, accuracy=86.963% -0.017101694 0.1343915 0.37599814\n",
      "2018-04-17 15:29:54 iteration 700/859: loss=0.331287, accuracy=91.577% -0.031351544 0.09943117 0.26320755\n",
      "2018-04-17 15:30:12 iteration 750/859: loss=0.489967, accuracy=88.110% -0.013881581 0.12156594 0.38228238\n",
      "2018-04-17 15:30:29 iteration 800/859: loss=0.310474, accuracy=92.700% -0.028563822 0.09010634 0.24893124\n",
      "2018-04-17 15:30:47 iteration 850/859: loss=0.413830, accuracy=90.002% -0.016662564 0.1082088 0.32228398\n",
      "2018-04-17 15:30:50 iteration 859/859: loss=0.278640, accuracy=93.726% -0.031944804 0.07983936 0.230745\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 15:31:00 end epoch 5/50: acc_train=89.178% acc_val=89.911% acc_test=89.719%\n",
      "2018-04-17 15:31:00 start epoch 6/50\n",
      "2018-04-17 15:31:00 iteration 1/859: loss=0.517603, accuracy=87.476% -0.007266524 0.12646241 0.39840722\n",
      "2018-04-17 15:31:17 iteration 50/859: loss=0.559532, accuracy=86.133% -0.0022170085 0.1310916 0.43065745\n",
      "2018-04-17 15:31:36 iteration 100/859: loss=0.401996, accuracy=89.990% -0.01656353 0.10316869 0.31539133\n",
      "2018-04-17 15:31:53 iteration 150/859: loss=0.289928, accuracy=92.798% -0.04033313 0.08556335 0.24469775\n",
      "2018-04-17 15:32:11 iteration 200/859: loss=0.306536, accuracy=92.444% -0.028624117 0.09110347 0.24405643\n",
      "2018-04-17 15:32:29 iteration 250/859: loss=0.233962, accuracy=93.848% -0.02935741 0.07953935 0.18377984\n",
      "2018-04-17 15:32:46 iteration 300/859: loss=0.338040, accuracy=90.710% -0.018764202 0.101458035 0.25534585\n",
      "2018-04-17 15:33:04 iteration 350/859: loss=0.271310, accuracy=92.737% -0.028800106 0.08412282 0.21598724\n",
      "2018-04-17 15:33:21 iteration 400/859: loss=0.405032, accuracy=90.112% -0.024509734 0.10168707 0.32785428\n",
      "2018-04-17 15:33:38 iteration 450/859: loss=0.528187, accuracy=87.573% -0.002610967 0.12476353 0.40603393\n",
      "2018-04-17 15:33:56 iteration 500/859: loss=0.466274, accuracy=89.099% -0.012929576 0.11002543 0.3691778\n",
      "2018-04-17 15:34:14 iteration 550/859: loss=0.257589, accuracy=93.616% -0.029265339 0.07825486 0.20859951\n",
      "2018-04-17 15:34:31 iteration 600/859: loss=0.366461, accuracy=91.638% -0.02224651 0.08938074 0.29932708\n",
      "2018-04-17 15:34:48 iteration 650/859: loss=0.467255, accuracy=87.329% -0.013584636 0.11865511 0.36218414\n",
      "2018-04-17 15:35:06 iteration 700/859: loss=0.350063, accuracy=91.016% -0.020279735 0.097568214 0.27277452\n",
      "2018-04-17 15:35:23 iteration 750/859: loss=0.625950, accuracy=84.668% 0.006210917 0.13996549 0.47977313\n",
      "2018-04-17 15:35:41 iteration 800/859: loss=0.468894, accuracy=88.721% -0.009916469 0.10621755 0.37259313\n",
      "2018-04-17 15:35:58 iteration 850/859: loss=0.174057, accuracy=95.630% -0.028999522 0.06048255 0.14257437\n",
      "2018-04-17 15:36:01 iteration 859/859: loss=0.434194, accuracy=91.479% -0.02409127 0.09484485 0.36344045\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 15:36:13 end epoch 6/50: acc_train=90.859% acc_val=91.473% acc_test=91.656%\n",
      "2018-04-17 15:36:13 start epoch 7/50\n",
      "2018-04-17 15:36:13 iteration 1/859: loss=0.342466, accuracy=91.577% -0.018990908 0.0908546 0.27060276\n",
      "2018-04-17 15:36:30 iteration 50/859: loss=0.226142, accuracy=94.202% -0.02629752 0.07206062 0.18037845\n",
      "2018-04-17 15:36:48 iteration 100/859: loss=0.259029, accuracy=93.579% -0.027286153 0.077496395 0.20881832\n",
      "2018-04-17 15:37:06 iteration 150/859: loss=0.193742, accuracy=94.531% -0.034997463 0.06890598 0.15983307\n",
      "2018-04-17 15:37:23 iteration 200/859: loss=0.407076, accuracy=89.001% -0.0130096655 0.109284304 0.31080097\n",
      "2018-04-17 15:37:40 iteration 250/859: loss=0.460672, accuracy=88.293% -0.0008002726 0.11392114 0.3475513\n",
      "2018-04-17 15:37:58 iteration 300/859: loss=0.414354, accuracy=90.552% -0.017631743 0.09979307 0.33219278\n",
      "2018-04-17 15:38:15 iteration 350/859: loss=0.212705, accuracy=94.934% -0.026430298 0.0618489 0.17728657\n",
      "2018-04-17 15:38:32 iteration 400/859: loss=0.356856, accuracy=90.723% -0.013957715 0.098046854 0.27276683\n",
      "2018-04-17 15:38:51 iteration 450/859: loss=0.161737, accuracy=95.691% -0.03203819 0.0561138 0.1376614\n",
      "2018-04-17 15:39:09 iteration 500/859: loss=0.341402, accuracy=90.881% -0.010390421 0.09452186 0.25727028\n",
      "2018-04-17 15:39:26 iteration 550/859: loss=0.513770, accuracy=86.609% 0.0052098013 0.12733784 0.38122195\n",
      "2018-04-17 15:39:44 iteration 600/859: loss=0.371713, accuracy=91.431% -0.01163143 0.09170367 0.2916405\n",
      "2018-04-17 15:40:02 iteration 650/859: loss=0.527170, accuracy=88.293% -0.0020697701 0.11522691 0.41401258\n",
      "2018-04-17 15:40:20 iteration 700/859: loss=0.307479, accuracy=92.151% -0.012761226 0.08409507 0.23614481\n",
      "2018-04-17 15:40:37 iteration 750/859: loss=0.259637, accuracy=94.739% -0.025425356 0.06520604 0.21985638\n",
      "2018-04-17 15:40:55 iteration 800/859: loss=0.276040, accuracy=93.030% -0.019384943 0.07501836 0.22040698\n",
      "2018-04-17 15:41:13 iteration 850/859: loss=0.256121, accuracy=93.921% -0.022337627 0.06920165 0.2092571\n",
      "2018-04-17 15:41:16 iteration 859/859: loss=0.424592, accuracy=90.686% -0.0043775616 0.09585418 0.33311588\n",
      "2018-04-17 15:41:27 end epoch 7/50: acc_train=91.100% acc_val=91.406% acc_test=91.573%\n",
      "2018-04-17 15:41:27 start epoch 8/50\n",
      "2018-04-17 15:41:28 iteration 1/859: loss=0.466041, accuracy=90.808% -0.008996615 0.09284204 0.38219583\n",
      "2018-04-17 15:41:45 iteration 50/859: loss=0.354895, accuracy=91.028% -0.0076710484 0.08941982 0.27314672\n",
      "2018-04-17 15:42:03 iteration 100/859: loss=0.294060, accuracy=91.797% -0.008564053 0.080446735 0.22217733\n",
      "2018-04-17 15:42:20 iteration 150/859: loss=0.235012, accuracy=93.848% -0.022233639 0.06673743 0.19050801\n",
      "2018-04-17 15:42:38 iteration 200/859: loss=0.284333, accuracy=91.895% -0.017856032 0.083029225 0.21915936\n",
      "2018-04-17 15:42:56 iteration 250/859: loss=0.231265, accuracy=94.275% -0.0188674 0.06583165 0.18430048\n",
      "2018-04-17 15:43:13 iteration 300/859: loss=0.321809, accuracy=93.469% -0.020356148 0.075083874 0.2670809\n",
      "2018-04-17 15:43:31 iteration 350/859: loss=0.333949, accuracy=92.517% -0.01111454 0.079748094 0.26531523\n",
      "2018-04-17 15:43:49 iteration 400/859: loss=0.377746, accuracy=89.697% -0.008852672 0.09939654 0.28720182\n",
      "2018-04-17 15:44:06 iteration 450/859: loss=0.257776, accuracy=93.188% -0.0101995375 0.073183596 0.19479215\n",
      "2018-04-17 15:44:24 iteration 500/859: loss=0.379213, accuracy=90.271% -0.004436775 0.09354164 0.29010803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 15:44:42 iteration 550/859: loss=0.291814, accuracy=92.004% -0.0126995 0.08283742 0.22167647\n",
      "2018-04-17 15:45:00 iteration 600/859: loss=0.301310, accuracy=92.200% -0.016412694 0.08081417 0.23690878\n",
      "2018-04-17 15:45:18 iteration 650/859: loss=0.501947, accuracy=87.109% 0.013990265 0.12080003 0.36715657\n",
      "2018-04-17 15:45:36 iteration 700/859: loss=0.173380, accuracy=95.752% -0.016821077 0.05132284 0.13887802\n",
      "2018-04-17 15:45:53 iteration 750/859: loss=0.138957, accuracy=96.509% -0.021134503 0.045409434 0.11468211\n",
      "2018-04-17 15:46:12 iteration 800/859: loss=0.142729, accuracy=96.436% -0.02591003 0.045240976 0.123398244\n",
      "2018-04-17 15:46:30 iteration 850/859: loss=0.440164, accuracy=89.111% -0.00037973718 0.1015597 0.33898377\n",
      "2018-04-17 15:46:33 iteration 859/859: loss=0.162460, accuracy=95.911% -0.02332904 0.050132215 0.13565637\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 15:46:44 end epoch 8/50: acc_train=92.772% acc_val=93.036% acc_test=93.177%\n",
      "2018-04-17 15:46:44 start epoch 9/50\n",
      "2018-04-17 15:46:44 iteration 1/859: loss=0.201701, accuracy=94.897% -0.01877951 0.05853384 0.1619463\n",
      "2018-04-17 15:47:02 iteration 50/859: loss=0.515770, accuracy=88.672% 0.0080130305 0.107621886 0.40013558\n",
      "2018-04-17 15:47:20 iteration 100/859: loss=0.452294, accuracy=88.684% 0.00841666 0.10760565 0.33627182\n",
      "2018-04-17 15:47:37 iteration 150/859: loss=0.481219, accuracy=87.695% 0.012807203 0.11424897 0.35416293\n",
      "2018-04-17 15:47:55 iteration 200/859: loss=0.294545, accuracy=93.164% -0.010639998 0.07102399 0.23416051\n",
      "2018-04-17 15:48:13 iteration 250/859: loss=0.222729, accuracy=94.495% -0.010578506 0.058405478 0.17490186\n",
      "2018-04-17 15:48:32 iteration 300/859: loss=0.459362, accuracy=90.747% 0.00052019046 0.09224284 0.36659923\n",
      "2018-04-17 15:48:50 iteration 350/859: loss=0.238621, accuracy=93.323% -0.013983583 0.070041955 0.18256304\n",
      "2018-04-17 15:49:07 iteration 400/859: loss=0.447724, accuracy=89.075% -0.0011341583 0.103973225 0.344885\n",
      "2018-04-17 15:49:25 iteration 450/859: loss=0.371104, accuracy=93.225% -0.009078092 0.0730051 0.30717748\n",
      "2018-04-17 15:49:43 iteration 500/859: loss=0.305374, accuracy=92.065% -0.0031543016 0.0817798 0.22674885\n",
      "2018-04-17 15:50:01 iteration 550/859: loss=0.261167, accuracy=93.188% -0.009244603 0.06967982 0.20073226\n",
      "2018-04-17 15:50:19 iteration 600/859: loss=0.251027, accuracy=93.958% -0.01395308 0.06520933 0.19977055\n",
      "2018-04-17 15:50:36 iteration 650/859: loss=0.107653, accuracy=97.278% -0.020247133 0.03592377 0.091976844\n",
      "2018-04-17 15:50:54 iteration 700/859: loss=0.349032, accuracy=90.857% -0.0013612845 0.08902732 0.26136577\n",
      "2018-04-17 15:51:12 iteration 750/859: loss=0.193862, accuracy=94.739% -0.021545496 0.05890218 0.15650536\n",
      "2018-04-17 15:51:30 iteration 800/859: loss=0.356410, accuracy=91.138% -0.002932385 0.08627685 0.273066\n",
      "2018-04-17 15:51:48 iteration 850/859: loss=0.226142, accuracy=94.263% -0.017303012 0.061923705 0.1815212\n",
      "2018-04-17 15:51:51 iteration 859/859: loss=0.311812, accuracy=92.188% -0.005434614 0.07434259 0.242904\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 15:52:02 end epoch 9/50: acc_train=92.844% acc_val=93.237% acc_test=93.479%\n",
      "2018-04-17 15:52:02 start epoch 10/50\n",
      "2018-04-17 15:52:03 iteration 1/859: loss=0.218781, accuracy=94.629% -0.020791072 0.057394437 0.18217799\n",
      "2018-04-17 15:52:20 iteration 50/859: loss=0.414233, accuracy=89.539% 0.0019831308 0.097674266 0.31457543\n",
      "2018-04-17 15:52:38 iteration 100/859: loss=0.299567, accuracy=92.847% -0.0045206756 0.07438614 0.22970197\n",
      "2018-04-17 15:52:56 iteration 150/859: loss=0.273942, accuracy=92.993% -0.014343346 0.068592615 0.21969318\n",
      "2018-04-17 15:53:14 iteration 200/859: loss=0.190463, accuracy=95.129% -0.013608845 0.053405393 0.15066658\n",
      "2018-04-17 15:53:32 iteration 250/859: loss=0.226882, accuracy=94.507% -0.008786121 0.057381094 0.17828748\n",
      "2018-04-17 15:53:49 iteration 300/859: loss=0.209506, accuracy=94.324% -0.013059095 0.06094272 0.1616228\n",
      "2018-04-17 15:54:07 iteration 350/859: loss=0.325910, accuracy=91.663% -0.004574003 0.07907875 0.25140524\n",
      "2018-04-17 15:54:25 iteration 400/859: loss=0.242901, accuracy=93.677% -0.007827732 0.064939655 0.18578887\n",
      "2018-04-17 15:54:43 iteration 450/859: loss=0.373828, accuracy=91.150% 0.0012168398 0.08470234 0.2879088\n",
      "2018-04-17 15:55:01 iteration 500/859: loss=0.271082, accuracy=93.298% -0.009545658 0.0638025 0.2168251\n",
      "2018-04-17 15:55:19 iteration 550/859: loss=0.116083, accuracy=97.119% -0.026288027 0.037574336 0.104797184\n",
      "2018-04-17 15:55:37 iteration 600/859: loss=0.212991, accuracy=94.287% -0.009356195 0.057386972 0.16496004\n",
      "2018-04-17 15:55:54 iteration 650/859: loss=0.362929, accuracy=91.028% 0.0010824569 0.08357231 0.27827442\n",
      "2018-04-17 15:56:13 iteration 700/859: loss=0.232832, accuracy=93.713% -0.008092229 0.0647761 0.17614827\n",
      "2018-04-17 15:56:31 iteration 750/859: loss=0.335971, accuracy=90.649% 0.0024574683 0.08915846 0.24435517\n",
      "2018-04-17 15:56:49 iteration 800/859: loss=0.331260, accuracy=91.382% -0.0031779103 0.0816872 0.25275105\n",
      "2018-04-17 15:57:07 iteration 850/859: loss=0.235998, accuracy=93.835% -0.0060471306 0.062941566 0.17910312\n",
      "2018-04-17 15:57:10 iteration 859/859: loss=0.290322, accuracy=93.152% -0.008230948 0.069797665 0.22875535\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 15:57:21 end epoch 10/50: acc_train=93.634% acc_val=94.665% acc_test=93.792%\n",
      "2018-04-17 15:57:21 start epoch 11/50\n",
      "2018-04-17 15:57:21 iteration 1/859: loss=0.275706, accuracy=92.603% -0.008325567 0.0735934 0.21043822\n",
      "2018-04-17 15:57:38 iteration 50/859: loss=0.187706, accuracy=95.264% -0.014309399 0.05084483 0.15117037\n",
      "2018-04-17 15:57:56 iteration 100/859: loss=0.204750, accuracy=94.580% -0.017302155 0.055857647 0.16619419\n",
      "2018-04-17 15:58:14 iteration 150/859: loss=0.284933, accuracy=93.274% -0.009037359 0.06572994 0.22824049\n",
      "2018-04-17 15:58:33 iteration 200/859: loss=0.152934, accuracy=95.776% -0.018913155 0.046102233 0.12574491\n",
      "2018-04-17 15:58:50 iteration 250/859: loss=0.474184, accuracy=91.846% -0.0031788175 0.08026443 0.397098\n",
      "2018-04-17 15:59:08 iteration 300/859: loss=0.295434, accuracy=92.065% 0.0002591607 0.07501848 0.2201566\n",
      "2018-04-17 15:59:26 iteration 350/859: loss=0.120492, accuracy=96.826% -0.01716631 0.036872625 0.10078618\n",
      "2018-04-17 15:59:44 iteration 400/859: loss=0.326616, accuracy=92.139% 0.0060953465 0.076706976 0.243814\n",
      "2018-04-17 16:00:01 iteration 450/859: loss=0.372592, accuracy=89.795% 0.0054338975 0.096340366 0.27081764\n",
      "2018-04-17 16:00:19 iteration 500/859: loss=0.214774, accuracy=95.044% -0.013730213 0.0531119 0.17539214\n",
      "2018-04-17 16:00:37 iteration 550/859: loss=0.219531, accuracy=94.568% -0.0058553903 0.054677375 0.17070931\n",
      "2018-04-17 16:00:55 iteration 600/859: loss=0.180805, accuracy=95.056% -0.014769954 0.051528107 0.14404672\n",
      "2018-04-17 16:01:13 iteration 650/859: loss=0.214902, accuracy=95.923% -0.013191706 0.04400435 0.18408903\n",
      "2018-04-17 16:01:31 iteration 700/859: loss=0.200883, accuracy=95.105% -0.007278727 0.05195806 0.1562035\n",
      "2018-04-17 16:01:48 iteration 750/859: loss=0.118836, accuracy=96.692% -0.018276136 0.037314177 0.099797845\n",
      "2018-04-17 16:02:06 iteration 800/859: loss=0.239538, accuracy=93.616% -0.0057987 0.06183725 0.1834998\n",
      "2018-04-17 16:02:24 iteration 850/859: loss=0.295377, accuracy=92.224% 0.0033616014 0.07304699 0.21896823\n",
      "2018-04-17 16:02:27 iteration 859/859: loss=0.171744, accuracy=96.252% -0.009408536 0.03941338 0.1417391\n",
      "2018-04-17 16:02:37 end epoch 11/50: acc_train=94.122% acc_val=94.085% acc_test=94.135%\n",
      "2018-04-17 16:02:37 start epoch 12/50\n",
      "2018-04-17 16:02:38 iteration 1/859: loss=0.065656, accuracy=98.254% -0.019469088 0.024806824 0.06031865\n",
      "2018-04-17 16:02:56 iteration 50/859: loss=0.184274, accuracy=95.215% -0.015612145 0.050852455 0.14903381\n",
      "2018-04-17 16:03:14 iteration 100/859: loss=0.287761, accuracy=94.556% -0.012454929 0.053963203 0.24625301\n",
      "2018-04-17 16:03:32 iteration 150/859: loss=0.280012, accuracy=92.920% -0.0007919289 0.06817123 0.21263307\n",
      "2018-04-17 16:03:50 iteration 200/859: loss=0.205083, accuracy=94.885% -0.006007481 0.051843066 0.15924776\n",
      "2018-04-17 16:04:08 iteration 250/859: loss=0.340479, accuracy=91.479% 0.007883035 0.07871901 0.25387737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 16:04:26 iteration 300/859: loss=0.092633, accuracy=97.595% -0.019584548 0.029074436 0.08314271\n",
      "2018-04-17 16:04:44 iteration 350/859: loss=0.129500, accuracy=96.741% -0.013135159 0.03609615 0.10653862\n",
      "2018-04-17 16:05:02 iteration 400/859: loss=0.278365, accuracy=92.212% -0.0013980841 0.07384599 0.20591679\n",
      "2018-04-17 16:05:20 iteration 450/859: loss=0.078579, accuracy=98.181% -0.023847105 0.025153602 0.07727218\n",
      "2018-04-17 16:05:38 iteration 500/859: loss=0.368782, accuracy=91.956% 0.0073391674 0.07578392 0.2856586\n",
      "2018-04-17 16:05:57 iteration 550/859: loss=0.383883, accuracy=91.846% -0.00014324347 0.07744484 0.3065818\n",
      "2018-04-17 16:06:14 iteration 600/859: loss=0.163587, accuracy=95.947% -0.0064475113 0.042127334 0.12790756\n",
      "2018-04-17 16:06:32 iteration 650/859: loss=0.179185, accuracy=95.911% -0.0074442485 0.042187233 0.14444175\n",
      "2018-04-17 16:06:51 iteration 700/859: loss=0.169182, accuracy=95.752% -0.013151211 0.043789208 0.13854384\n",
      "2018-04-17 16:07:09 iteration 750/859: loss=0.209196, accuracy=94.727% -0.007490074 0.051977538 0.16470838\n",
      "2018-04-17 16:07:27 iteration 800/859: loss=0.404833, accuracy=91.553% 0.0037989456 0.081993066 0.31904072\n",
      "2018-04-17 16:07:45 iteration 850/859: loss=0.273348, accuracy=92.859% 0.0005797196 0.06847985 0.20428863\n",
      "2018-04-17 16:07:48 iteration 859/859: loss=0.181107, accuracy=95.068% -0.0040188627 0.050679106 0.13444658\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 16:07:59 end epoch 12/50: acc_train=94.288% acc_val=94.911% acc_test=94.781%\n",
      "2018-04-17 16:07:59 start epoch 13/50\n",
      "2018-04-17 16:07:59 iteration 1/859: loss=0.234299, accuracy=94.006% -0.0077470466 0.058962964 0.18308315\n",
      "2018-04-17 16:08:18 iteration 50/859: loss=0.172073, accuracy=95.557% -0.011395253 0.045713905 0.13775434\n",
      "2018-04-17 16:08:36 iteration 100/859: loss=0.318092, accuracy=91.907% 0.002360848 0.078144714 0.23758617\n",
      "2018-04-17 16:08:54 iteration 150/859: loss=0.122909, accuracy=96.838% -0.015122101 0.03466729 0.10336374\n",
      "2018-04-17 16:09:12 iteration 200/859: loss=0.154319, accuracy=95.703% -0.012316983 0.04535643 0.12127979\n",
      "2018-04-17 16:09:30 iteration 250/859: loss=0.199766, accuracy=94.568% -0.005914393 0.05477815 0.15090257\n",
      "2018-04-17 16:09:48 iteration 300/859: loss=0.271705, accuracy=92.822% 0.002189499 0.068756856 0.20075908\n",
      "2018-04-17 16:10:06 iteration 350/859: loss=0.216138, accuracy=94.043% -0.0022854123 0.05901196 0.1594113\n",
      "2018-04-17 16:10:23 iteration 400/859: loss=0.151095, accuracy=95.801% -0.0090581905 0.04310637 0.11704668\n",
      "2018-04-17 16:10:41 iteration 450/859: loss=0.108130, accuracy=97.510% -0.013968202 0.027990397 0.09410778\n",
      "2018-04-17 16:10:59 iteration 500/859: loss=0.262440, accuracy=93.347% 0.0031491579 0.06492259 0.19436827\n",
      "2018-04-17 16:11:17 iteration 550/859: loss=0.212528, accuracy=94.568% -0.0053255237 0.053929377 0.16392365\n",
      "2018-04-17 16:11:35 iteration 600/859: loss=0.225014, accuracy=93.994% -0.00021468829 0.059003275 0.16622531\n",
      "2018-04-17 16:11:53 iteration 650/859: loss=0.230685, accuracy=94.299% -0.0054869717 0.05593717 0.18023512\n",
      "2018-04-17 16:12:11 iteration 700/859: loss=0.142434, accuracy=96.240% -0.016964644 0.03993109 0.11946736\n",
      "2018-04-17 16:12:29 iteration 750/859: loss=0.161549, accuracy=95.618% -0.009893624 0.045470513 0.12597246\n",
      "2018-04-17 16:12:47 iteration 800/859: loss=0.093720, accuracy=97.571% -0.01833964 0.028880978 0.08317872\n",
      "2018-04-17 16:13:05 iteration 850/859: loss=0.093066, accuracy=97.668% -0.017474528 0.027433569 0.083106816\n",
      "2018-04-17 16:13:08 iteration 859/859: loss=0.146640, accuracy=96.008% -0.014585078 0.04141301 0.11981183\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 16:13:19 end epoch 13/50: acc_train=95.122% acc_val=95.692% acc_test=95.271%\n",
      "2018-04-17 16:13:19 start epoch 14/50\n",
      "2018-04-17 16:13:19 iteration 1/859: loss=0.078274, accuracy=98.047% -0.018118942 0.023838943 0.07255356\n",
      "2018-04-17 16:13:37 iteration 50/859: loss=0.187342, accuracy=94.934% -0.008508933 0.050118487 0.14573276\n",
      "2018-04-17 16:13:55 iteration 100/859: loss=0.150022, accuracy=96.082% -0.011354331 0.041545406 0.11983104\n",
      "2018-04-17 16:14:13 iteration 150/859: loss=0.151910, accuracy=96.497% -0.00832075 0.03741836 0.12281194\n",
      "2018-04-17 16:14:31 iteration 200/859: loss=0.273994, accuracy=93.127% 0.0006789545 0.067386724 0.20592812\n",
      "2018-04-17 16:14:49 iteration 250/859: loss=0.328389, accuracy=92.578% 0.0073647825 0.070762694 0.2502617\n",
      "2018-04-17 16:15:07 iteration 300/859: loss=0.163663, accuracy=95.471% -0.012396211 0.04542952 0.13062993\n",
      "2018-04-17 16:15:25 iteration 350/859: loss=0.067096, accuracy=98.230% -0.016470846 0.022334967 0.061231747\n",
      "2018-04-17 16:15:44 iteration 400/859: loss=0.106653, accuracy=96.802% -0.012471497 0.03189589 0.08722848\n",
      "2018-04-17 16:16:02 iteration 450/859: loss=0.289079, accuracy=91.858% 0.006964897 0.07548983 0.20662384\n",
      "2018-04-17 16:16:20 iteration 500/859: loss=0.251274, accuracy=93.433% -0.0019762465 0.06403074 0.18921934\n",
      "2018-04-17 16:16:38 iteration 550/859: loss=0.049980, accuracy=98.779% -0.017899085 0.016718904 0.05115971\n",
      "2018-04-17 16:16:56 iteration 600/859: loss=0.253477, accuracy=93.225% -6.490583e-05 0.064850345 0.18869135\n",
      "2018-04-17 16:17:14 iteration 650/859: loss=0.232715, accuracy=94.604% 0.0003346846 0.053937472 0.17844254\n",
      "2018-04-17 16:17:32 iteration 700/859: loss=0.231425, accuracy=94.568% -0.0010529782 0.051359296 0.18111852\n",
      "2018-04-17 16:17:50 iteration 750/859: loss=0.147693, accuracy=96.118% -0.011533378 0.03936591 0.11986016\n",
      "2018-04-17 16:18:08 iteration 800/859: loss=0.110457, accuracy=97.046% -0.010278893 0.032163948 0.08857198\n",
      "2018-04-17 16:18:26 iteration 850/859: loss=0.122461, accuracy=96.826% -0.009599442 0.033298712 0.09876205\n",
      "2018-04-17 16:18:29 iteration 859/859: loss=0.302375, accuracy=92.114% 0.008214144 0.075148016 0.21901335\n",
      "2018-04-17 16:18:41 end epoch 14/50: acc_train=95.138% acc_val=95.134% acc_test=95.396%\n",
      "2018-04-17 16:18:41 start epoch 15/50\n",
      "2018-04-17 16:18:41 iteration 1/859: loss=0.169350, accuracy=95.496% -0.009757373 0.04542104 0.13368621\n",
      "2018-04-17 16:18:59 iteration 50/859: loss=0.285682, accuracy=92.737% 0.0037118967 0.068984054 0.21298605\n",
      "2018-04-17 16:19:16 iteration 100/859: loss=0.195528, accuracy=95.227% -0.0053073606 0.0454969 0.1553382\n",
      "2018-04-17 16:19:34 iteration 150/859: loss=0.102111, accuracy=97.070% -0.008796686 0.03100251 0.07990506\n",
      "2018-04-17 16:19:52 iteration 200/859: loss=0.254327, accuracy=93.225% 0.0006243839 0.06488629 0.18881589\n",
      "2018-04-17 16:20:10 iteration 250/859: loss=0.292611, accuracy=92.847% 0.0029247182 0.066655606 0.2230303\n",
      "2018-04-17 16:20:28 iteration 300/859: loss=0.192651, accuracy=95.093% -0.0029550113 0.048525583 0.14708087\n",
      "2018-04-17 16:20:46 iteration 350/859: loss=0.069730, accuracy=98.096% -0.012318169 0.022307966 0.05974038\n",
      "2018-04-17 16:21:03 iteration 400/859: loss=0.134737, accuracy=96.606% -0.0077196397 0.03533699 0.10711994\n",
      "2018-04-17 16:21:21 iteration 450/859: loss=0.216422, accuracy=94.495% -0.0019789585 0.052626353 0.16577482\n",
      "2018-04-17 16:21:39 iteration 500/859: loss=0.371312, accuracy=92.407% 0.0058874786 0.07217058 0.29325414\n",
      "2018-04-17 16:21:57 iteration 550/859: loss=0.414625, accuracy=90.967% 0.012917563 0.08505591 0.31665176\n",
      "2018-04-17 16:22:15 iteration 600/859: loss=0.215674, accuracy=94.507% -0.005049979 0.053124867 0.16759922\n",
      "2018-04-17 16:22:33 iteration 650/859: loss=0.092782, accuracy=97.473% -0.014810422 0.027418155 0.08017437\n",
      "2018-04-17 16:22:51 iteration 700/859: loss=0.166893, accuracy=96.460% -0.00807596 0.035934776 0.13903394\n",
      "2018-04-17 16:23:11 iteration 750/859: loss=0.186441, accuracy=95.850% -0.0061895126 0.04087107 0.15175933\n",
      "2018-04-17 16:23:29 iteration 800/859: loss=0.196520, accuracy=94.653% -0.004034195 0.051912535 0.14864215\n",
      "2018-04-17 16:23:47 iteration 850/859: loss=0.131060, accuracy=96.716% -0.008386698 0.034475304 0.10497177\n",
      "2018-04-17 16:23:50 iteration 859/859: loss=0.115462, accuracy=96.948% -0.009775703 0.032152556 0.09308493\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 16:24:00 end epoch 15/50: acc_train=95.566% acc_val=96.384% acc_test=95.531%\n",
      "2018-04-17 16:24:00 start epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 16:24:01 iteration 1/859: loss=0.185699, accuracy=95.190% -0.003582172 0.047905933 0.14137505\n",
      "2018-04-17 16:24:19 iteration 50/859: loss=0.176229, accuracy=95.227% -0.0048415964 0.04665704 0.13441336\n",
      "2018-04-17 16:24:36 iteration 100/859: loss=0.115315, accuracy=96.997% -0.012667211 0.03129887 0.096683234\n",
      "2018-04-17 16:24:55 iteration 150/859: loss=0.161541, accuracy=95.642% -0.007199387 0.043027423 0.12571329\n",
      "2018-04-17 16:25:12 iteration 200/859: loss=0.108467, accuracy=97.241% -0.010634301 0.029257065 0.08984411\n",
      "2018-04-17 16:25:32 iteration 250/859: loss=0.195742, accuracy=94.885% -0.0013254129 0.049733724 0.14733417\n",
      "2018-04-17 16:25:50 iteration 300/859: loss=0.101793, accuracy=97.266% -0.010646713 0.029910177 0.08253002\n",
      "2018-04-17 16:26:07 iteration 350/859: loss=0.171293, accuracy=96.594% -0.0061693033 0.03452704 0.14293545\n",
      "2018-04-17 16:26:25 iteration 400/859: loss=0.243409, accuracy=94.360% 0.00022350908 0.053040214 0.19014502\n",
      "2018-04-17 16:26:43 iteration 450/859: loss=0.216777, accuracy=94.604% -0.0025055506 0.052685022 0.16659778\n",
      "2018-04-17 16:27:01 iteration 500/859: loss=0.183920, accuracy=95.105% -0.004767844 0.046397474 0.14229065\n",
      "2018-04-17 16:27:19 iteration 550/859: loss=0.134804, accuracy=96.423% -0.007618944 0.036335707 0.106086895\n",
      "2018-04-17 16:27:37 iteration 600/859: loss=0.369795, accuracy=91.235% 0.0147468895 0.080791876 0.27425647\n",
      "2018-04-17 16:27:55 iteration 650/859: loss=0.241984, accuracy=93.616% 0.0025365264 0.05984167 0.17960617\n",
      "2018-04-17 16:28:13 iteration 700/859: loss=0.172106, accuracy=95.251% -0.0003639143 0.04651864 0.12595168\n",
      "2018-04-17 16:28:31 iteration 750/859: loss=0.222260, accuracy=94.592% 0.0023314161 0.051760364 0.16816868\n",
      "2018-04-17 16:28:48 iteration 800/859: loss=0.352471, accuracy=90.698% 0.012607562 0.08477863 0.2550848\n",
      "2018-04-17 16:29:06 iteration 850/859: loss=0.178079, accuracy=96.887% -0.0075689927 0.032090046 0.15355764\n",
      "2018-04-17 16:29:09 iteration 859/859: loss=0.162164, accuracy=95.715% -0.005249431 0.042850416 0.12456317\n",
      "2018-04-17 16:29:20 end epoch 16/50: acc_train=95.556% acc_val=95.938% acc_test=95.385%\n",
      "2018-04-17 16:29:20 start epoch 17/50\n",
      "2018-04-17 16:29:21 iteration 1/859: loss=0.220119, accuracy=94.458% 0.0012779671 0.054198153 0.16464268\n",
      "2018-04-17 16:29:38 iteration 50/859: loss=0.269615, accuracy=92.920% 0.0068885065 0.06609205 0.19663464\n",
      "2018-04-17 16:29:56 iteration 100/859: loss=0.173893, accuracy=95.215% -0.0053399177 0.047275968 0.13195686\n",
      "2018-04-17 16:30:14 iteration 150/859: loss=0.353524, accuracy=92.627% 0.011225807 0.07019143 0.27210718\n",
      "2018-04-17 16:30:32 iteration 200/859: loss=0.273089, accuracy=93.591% 0.0028877705 0.060331643 0.20986918\n",
      "2018-04-17 16:30:50 iteration 250/859: loss=0.363423, accuracy=91.150% 0.013145152 0.08313813 0.2671393\n",
      "2018-04-17 16:31:08 iteration 300/859: loss=0.140404, accuracy=96.313% -0.0041032387 0.037028212 0.10747926\n",
      "2018-04-17 16:31:26 iteration 350/859: loss=0.071699, accuracy=98.071% -0.014399746 0.021885028 0.06421405\n",
      "2018-04-17 16:31:44 iteration 400/859: loss=0.037131, accuracy=99.097% -0.016711563 0.012886274 0.040956676\n",
      "2018-04-17 16:32:02 iteration 450/859: loss=0.084689, accuracy=97.961% -0.013187525 0.022555074 0.07532134\n",
      "2018-04-17 16:32:20 iteration 500/859: loss=0.212973, accuracy=94.714% -0.0003786859 0.050360322 0.16299163\n",
      "2018-04-17 16:32:38 iteration 550/859: loss=0.398362, accuracy=90.540% 0.0125717 0.08740647 0.298384\n",
      "2018-04-17 16:32:58 iteration 600/859: loss=0.264380, accuracy=94.104% 0.0033383952 0.05706356 0.20397821\n",
      "2018-04-17 16:33:16 iteration 650/859: loss=0.177924, accuracy=94.934% 0.0028037545 0.049721126 0.12539865\n",
      "2018-04-17 16:33:34 iteration 700/859: loss=0.196397, accuracy=95.044% -0.00083016336 0.04770852 0.14951825\n",
      "2018-04-17 16:33:52 iteration 750/859: loss=0.295295, accuracy=93.408% 0.002930802 0.06132837 0.23103562\n",
      "2018-04-17 16:34:10 iteration 800/859: loss=0.241648, accuracy=94.751% 0.0027529816 0.050896633 0.18799818\n",
      "2018-04-17 16:34:28 iteration 850/859: loss=0.129692, accuracy=96.460% -0.0070628026 0.035308044 0.10144697\n",
      "2018-04-17 16:34:31 iteration 859/859: loss=0.124973, accuracy=96.753% -0.0061570653 0.032308735 0.09882155\n",
      "2018-04-17 16:34:42 end epoch 17/50: acc_train=95.694% acc_val=95.558% acc_test=95.844%\n",
      "2018-04-17 16:34:42 start epoch 18/50\n",
      "2018-04-17 16:34:42 iteration 1/859: loss=0.282028, accuracy=92.981% 0.007300228 0.06482001 0.20990805\n",
      "2018-04-17 16:35:00 iteration 50/859: loss=0.159075, accuracy=96.008% -0.001502208 0.038575936 0.122001275\n",
      "2018-04-17 16:35:18 iteration 100/859: loss=0.070772, accuracy=98.230% -0.013461369 0.02004152 0.06419204\n",
      "2018-04-17 16:35:37 iteration 150/859: loss=0.148255, accuracy=96.436% -0.006399513 0.035680857 0.11897379\n",
      "2018-04-17 16:35:54 iteration 200/859: loss=0.127347, accuracy=97.107% -0.004522816 0.029320974 0.102548376\n",
      "2018-04-17 16:36:12 iteration 250/859: loss=0.248710, accuracy=93.567% 0.0047558052 0.061517578 0.18243635\n",
      "2018-04-17 16:36:30 iteration 300/859: loss=0.178176, accuracy=95.837% -0.0009741057 0.04044943 0.13870107\n",
      "2018-04-17 16:36:48 iteration 350/859: loss=0.078185, accuracy=98.022% -0.010997148 0.02196289 0.06721898\n",
      "2018-04-17 16:37:06 iteration 400/859: loss=0.190127, accuracy=95.288% -0.0011212036 0.046598904 0.14464921\n",
      "2018-04-17 16:37:24 iteration 450/859: loss=0.399628, accuracy=89.966% 0.020625776 0.09221417 0.28678817\n",
      "2018-04-17 16:37:42 iteration 500/859: loss=0.151084, accuracy=95.911% -0.003866867 0.040985443 0.11396526\n",
      "2018-04-17 16:38:00 iteration 550/859: loss=0.146213, accuracy=96.008% -0.004471616 0.03916657 0.11151783\n",
      "2018-04-17 16:38:18 iteration 600/859: loss=0.137106, accuracy=96.667% -0.0074142143 0.033055864 0.11146412\n",
      "2018-04-17 16:38:36 iteration 650/859: loss=0.163197, accuracy=95.667% -0.0064796703 0.04191003 0.12776661\n",
      "2018-04-17 16:38:54 iteration 700/859: loss=0.176638, accuracy=95.398% -0.0031329081 0.044161055 0.1356098\n",
      "2018-04-17 16:39:11 iteration 750/859: loss=0.169399, accuracy=95.923% -0.0019130625 0.040581487 0.13073009\n",
      "2018-04-17 16:39:29 iteration 800/859: loss=0.113517, accuracy=97.119% -0.009626513 0.03014415 0.09299933\n",
      "2018-04-17 16:39:47 iteration 850/859: loss=0.076483, accuracy=97.852% -0.011361453 0.023179177 0.064665176\n",
      "2018-04-17 16:39:50 iteration 859/859: loss=0.319906, accuracy=91.663% 0.009342429 0.07653508 0.23402853\n",
      "2018-04-17 16:40:01 end epoch 18/50: acc_train=95.350% acc_val=95.201% acc_test=95.885%\n",
      "2018-04-17 16:40:01 start epoch 19/50\n",
      "2018-04-17 16:40:02 iteration 1/859: loss=0.097509, accuracy=97.229% -0.012829876 0.028344119 0.08199457\n",
      "2018-04-17 16:40:20 iteration 50/859: loss=0.129765, accuracy=96.631% -0.007101699 0.034090675 0.10277636\n",
      "2018-04-17 16:40:38 iteration 100/859: loss=0.059159, accuracy=98.474% -0.012600559 0.01804995 0.05370934\n",
      "2018-04-17 16:40:56 iteration 150/859: loss=0.173483, accuracy=95.862% -0.0019615623 0.04025931 0.13518488\n",
      "2018-04-17 16:41:13 iteration 200/859: loss=0.138536, accuracy=97.009% -0.0057979105 0.030307269 0.11402631\n",
      "2018-04-17 16:41:32 iteration 250/859: loss=0.134207, accuracy=96.228% -0.0053925635 0.03742669 0.10217301\n",
      "2018-04-17 16:41:49 iteration 300/859: loss=0.164584, accuracy=95.471% -0.0015683209 0.043504532 0.122647785\n",
      "2018-04-17 16:42:08 iteration 350/859: loss=0.184079, accuracy=94.836% -0.0012300287 0.049351603 0.13595742\n",
      "2018-04-17 16:42:26 iteration 400/859: loss=0.085903, accuracy=97.729% -0.007891632 0.024176192 0.069618076\n",
      "2018-04-17 16:42:44 iteration 450/859: loss=0.253576, accuracy=93.311% 0.007093489 0.06206718 0.18441562\n",
      "2018-04-17 16:43:02 iteration 500/859: loss=0.279339, accuracy=93.311% 0.005011049 0.06399015 0.21033776\n",
      "2018-04-17 16:43:20 iteration 550/859: loss=0.185957, accuracy=94.897% -0.003687663 0.048068237 0.14157654\n",
      "2018-04-17 16:43:38 iteration 600/859: loss=0.225036, accuracy=94.897% 0.0012503292 0.04894346 0.17484252\n",
      "2018-04-17 16:43:56 iteration 650/859: loss=0.084566, accuracy=97.705% -0.008975975 0.023500087 0.0700418\n",
      "2018-04-17 16:44:15 iteration 700/859: loss=0.163267, accuracy=95.544% 0.0020575405 0.042458486 0.118750535\n",
      "2018-04-17 16:44:33 iteration 750/859: loss=0.229433, accuracy=94.336% 0.0057298094 0.054747485 0.16895613\n",
      "2018-04-17 16:44:51 iteration 800/859: loss=0.210417, accuracy=94.189% 0.007589479 0.054451745 0.1483762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 16:45:09 iteration 850/859: loss=0.138582, accuracy=96.289% -0.007369435 0.03655238 0.10939917\n",
      "2018-04-17 16:45:12 iteration 859/859: loss=0.049313, accuracy=98.633% -0.014891909 0.015817706 0.048386738\n",
      "2018-04-17 16:45:23 end epoch 19/50: acc_train=95.822% acc_val=96.228% acc_test=95.958%\n",
      "2018-04-17 16:45:23 start epoch 20/50\n",
      "2018-04-17 16:45:24 iteration 1/859: loss=0.136229, accuracy=96.167% -0.006624622 0.037628002 0.10522591\n",
      "2018-04-17 16:45:41 iteration 50/859: loss=0.076211, accuracy=98.022% -0.009404884 0.021574771 0.064041115\n",
      "2018-04-17 16:45:59 iteration 100/859: loss=0.237023, accuracy=94.629% 0.00039781444 0.05054579 0.18607894\n",
      "2018-04-17 16:46:17 iteration 150/859: loss=0.197008, accuracy=95.093% -5.0116865e-05 0.04732661 0.14973152\n",
      "2018-04-17 16:46:35 iteration 200/859: loss=0.355820, accuracy=91.113% 0.016436605 0.083143376 0.25623986\n",
      "2018-04-17 16:46:53 iteration 250/859: loss=0.049554, accuracy=98.657% -0.014485866 0.015878687 0.048161484\n",
      "2018-04-17 16:47:11 iteration 300/859: loss=0.253779, accuracy=94.055% 0.0022917655 0.055481255 0.19600555\n",
      "2018-04-17 16:47:28 iteration 350/859: loss=0.072939, accuracy=98.083% -0.011053569 0.020797474 0.0631954\n",
      "2018-04-17 16:47:46 iteration 400/859: loss=0.298669, accuracy=93.103% 0.008554491 0.06350622 0.22660847\n",
      "2018-04-17 16:48:04 iteration 450/859: loss=0.259123, accuracy=94.275% 0.0022232472 0.053638853 0.20326091\n",
      "2018-04-17 16:48:22 iteration 500/859: loss=0.189573, accuracy=95.178% 0.0023958038 0.045916196 0.14126083\n",
      "2018-04-17 16:48:41 iteration 550/859: loss=0.048380, accuracy=98.706% -0.010775629 0.014687903 0.044467717\n",
      "2018-04-17 16:48:59 iteration 600/859: loss=0.199276, accuracy=95.093% 0.0017884246 0.047064483 0.15042321\n",
      "2018-04-17 16:49:17 iteration 650/859: loss=0.169591, accuracy=95.618% 0.0011064417 0.042725712 0.12575924\n",
      "2018-04-17 16:49:35 iteration 700/859: loss=0.040271, accuracy=99.023% -0.012656975 0.012257178 0.040670555\n",
      "2018-04-17 16:49:53 iteration 750/859: loss=0.162623, accuracy=95.667% -0.0020759236 0.0417291 0.122969486\n",
      "2018-04-17 16:50:11 iteration 800/859: loss=0.124872, accuracy=96.472% -0.004718414 0.034581535 0.0950086\n",
      "2018-04-17 16:50:30 iteration 850/859: loss=0.138643, accuracy=96.155% -0.00048525567 0.037692443 0.10143541\n",
      "2018-04-17 16:50:33 iteration 859/859: loss=0.112262, accuracy=97.144% -0.005527393 0.02833696 0.08945246\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 16:50:43 end epoch 20/50: acc_train=96.469% acc_val=97.031% acc_test=96.583%\n",
      "2018-04-17 16:50:43 start epoch 21/50\n",
      "2018-04-17 16:50:44 iteration 1/859: loss=0.194565, accuracy=94.788% 0.0019894484 0.050239325 0.14233631\n",
      "2018-04-17 16:51:01 iteration 50/859: loss=0.149825, accuracy=96.375% -0.00027408527 0.034496337 0.11560232\n",
      "2018-04-17 16:51:19 iteration 100/859: loss=0.205690, accuracy=94.910% 0.0017485243 0.047935657 0.15600583\n",
      "2018-04-17 16:51:37 iteration 150/859: loss=0.106723, accuracy=97.119% -0.005038578 0.02847001 0.08329173\n",
      "2018-04-17 16:51:55 iteration 200/859: loss=0.130906, accuracy=96.509% -0.008010087 0.034172826 0.10474372\n",
      "2018-04-17 16:52:13 iteration 250/859: loss=0.146147, accuracy=95.984% -0.00037878385 0.038612384 0.10791376\n",
      "2018-04-17 16:52:31 iteration 300/859: loss=0.079497, accuracy=97.986% -0.008510093 0.021040611 0.06696678\n",
      "2018-04-17 16:52:50 iteration 350/859: loss=0.135649, accuracy=96.570% -0.0023519606 0.0346447 0.10335612\n",
      "2018-04-17 16:53:08 iteration 400/859: loss=0.279883, accuracy=93.713% 0.008811998 0.058875453 0.21219575\n",
      "2018-04-17 16:53:26 iteration 450/859: loss=0.231898, accuracy=94.678% -0.00012512875 0.050308127 0.18171528\n",
      "2018-04-17 16:53:44 iteration 500/859: loss=0.121021, accuracy=96.606% -0.0013356281 0.032544293 0.08981237\n",
      "2018-04-17 16:54:02 iteration 550/859: loss=0.237894, accuracy=94.214% 0.0031419906 0.05503111 0.17972127\n",
      "2018-04-17 16:54:20 iteration 600/859: loss=0.154321, accuracy=96.313% -0.0042840936 0.035995033 0.12260978\n",
      "2018-04-17 16:54:37 iteration 650/859: loss=0.131122, accuracy=96.973% -0.0046556545 0.030257065 0.105520375\n",
      "2018-04-17 16:54:55 iteration 700/859: loss=0.106067, accuracy=97.144% -0.0030169196 0.028290758 0.08079269\n",
      "2018-04-17 16:55:14 iteration 750/859: loss=0.166806, accuracy=95.618% -0.003023782 0.041897725 0.12793176\n",
      "2018-04-17 16:55:32 iteration 800/859: loss=0.082015, accuracy=98.218% -0.0108585255 0.01904783 0.07382578\n",
      "2018-04-17 16:55:50 iteration 850/859: loss=0.057593, accuracy=98.389% -0.010588209 0.017230371 0.05095115\n",
      "2018-04-17 16:55:53 iteration 859/859: loss=0.104294, accuracy=97.180% -0.0054600546 0.028540188 0.08121391\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 16:56:04 end epoch 21/50: acc_train=96.762% acc_val=97.121% acc_test=96.875%\n",
      "2018-04-17 16:56:04 start epoch 22/50\n",
      "2018-04-17 16:56:04 iteration 1/859: loss=0.141765, accuracy=95.996% 0.00034717616 0.04002453 0.10139346\n",
      "2018-04-17 16:56:22 iteration 50/859: loss=0.114481, accuracy=96.948% -0.005499609 0.03009836 0.08988233\n",
      "2018-04-17 16:56:40 iteration 100/859: loss=0.137389, accuracy=96.350% -0.0008107703 0.03549473 0.1027052\n",
      "2018-04-17 16:56:58 iteration 150/859: loss=0.050435, accuracy=98.669% -0.008811531 0.014732638 0.044513516\n",
      "2018-04-17 16:57:17 iteration 200/859: loss=0.116938, accuracy=96.790% -0.0053511127 0.03150919 0.09078028\n",
      "2018-04-17 16:57:35 iteration 250/859: loss=0.112586, accuracy=97.034% -0.005648946 0.028359305 0.08987531\n",
      "2018-04-17 16:57:53 iteration 300/859: loss=0.130608, accuracy=96.570% -0.0019685514 0.034084857 0.098492146\n",
      "2018-04-17 16:58:11 iteration 350/859: loss=0.149573, accuracy=95.972% -0.003903648 0.03880941 0.11466704\n",
      "2018-04-17 16:58:29 iteration 400/859: loss=0.068804, accuracy=97.937% -0.010730774 0.021636402 0.05789791\n",
      "2018-04-17 16:58:47 iteration 450/859: loss=0.256688, accuracy=93.872% 0.002849556 0.05919767 0.19464105\n",
      "2018-04-17 16:59:05 iteration 500/859: loss=0.142441, accuracy=96.350% -0.0030436805 0.035375632 0.110109136\n",
      "2018-04-17 16:59:24 iteration 550/859: loss=0.080486, accuracy=97.681% -0.008143761 0.024017427 0.0646127\n",
      "2018-04-17 16:59:42 iteration 600/859: loss=0.137953, accuracy=96.387% -0.0017516126 0.03459544 0.10510937\n",
      "2018-04-17 17:00:00 iteration 650/859: loss=0.087382, accuracy=97.668% -0.0026855743 0.022655765 0.06741141\n",
      "2018-04-17 17:00:19 iteration 700/859: loss=0.093060, accuracy=97.400% -0.004933307 0.026067775 0.071925715\n",
      "2018-04-17 17:00:36 iteration 750/859: loss=0.110087, accuracy=97.034% -0.008288411 0.029242948 0.08913273\n",
      "2018-04-17 17:00:54 iteration 800/859: loss=0.198991, accuracy=95.142% 0.004149791 0.046372008 0.14846936\n",
      "2018-04-17 17:01:12 iteration 850/859: loss=0.032261, accuracy=99.146% -0.007796237 0.010241829 0.029815743\n",
      "2018-04-17 17:01:16 iteration 859/859: loss=0.118810, accuracy=97.131% -0.0046092467 0.02851237 0.09490678\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 17:01:26 end epoch 22/50: acc_train=96.641% acc_val=97.232% acc_test=96.625%\n",
      "2018-04-17 17:01:26 start epoch 23/50\n",
      "2018-04-17 17:01:26 iteration 1/859: loss=0.039631, accuracy=98.804% -0.010472802 0.013565242 0.036538705\n",
      "2018-04-17 17:01:45 iteration 50/859: loss=0.132417, accuracy=96.643% -0.002283547 0.03265057 0.10205021\n",
      "2018-04-17 17:02:03 iteration 100/859: loss=0.116446, accuracy=96.875% -0.0032435947 0.030731846 0.08895756\n",
      "2018-04-17 17:02:21 iteration 150/859: loss=0.127690, accuracy=96.472% -0.004858731 0.034290366 0.09825825\n",
      "2018-04-17 17:02:39 iteration 200/859: loss=0.158153, accuracy=95.618% 0.0031664334 0.042136975 0.11284995\n",
      "2018-04-17 17:02:57 iteration 250/859: loss=0.133309, accuracy=96.484% -0.00055937667 0.0346327 0.09923567\n",
      "2018-04-17 17:03:15 iteration 300/859: loss=0.058403, accuracy=98.584% -0.0074107954 0.015652562 0.05016107\n",
      "2018-04-17 17:03:33 iteration 350/859: loss=0.100363, accuracy=97.302% -0.0015629644 0.026590804 0.07533559\n",
      "2018-04-17 17:03:51 iteration 400/859: loss=0.060176, accuracy=98.364% -0.0071779313 0.017480534 0.049873352\n",
      "2018-04-17 17:04:09 iteration 450/859: loss=0.265976, accuracy=93.372% 0.008094244 0.06192095 0.19596104\n",
      "2018-04-17 17:04:27 iteration 500/859: loss=0.109537, accuracy=97.009% -0.004133699 0.029568935 0.08410206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 17:04:45 iteration 550/859: loss=0.069532, accuracy=98.120% -0.010681436 0.019709494 0.06050405\n",
      "2018-04-17 17:05:04 iteration 600/859: loss=0.055640, accuracy=98.718% -0.01241338 0.014255084 0.053797834\n",
      "2018-04-17 17:05:22 iteration 650/859: loss=0.160164, accuracy=96.289% -0.0005340746 0.03567175 0.12502675\n",
      "2018-04-17 17:05:39 iteration 700/859: loss=0.136731, accuracy=97.217% -0.002503772 0.027488953 0.11174627\n",
      "2018-04-17 17:05:57 iteration 750/859: loss=0.183000, accuracy=95.593% -7.066897e-05 0.043484844 0.13958587\n",
      "2018-04-17 17:06:15 iteration 800/859: loss=0.040273, accuracy=98.792% -0.0085507855 0.012934771 0.03588903\n",
      "2018-04-17 17:06:33 iteration 850/859: loss=0.147531, accuracy=96.460% -0.0007532185 0.03409337 0.11419082\n",
      "2018-04-17 17:06:36 iteration 859/859: loss=0.208838, accuracy=95.227% 0.004682095 0.045611195 0.15854435\n",
      "2018-04-17 17:06:47 end epoch 23/50: acc_train=96.834% acc_val=97.188% acc_test=97.229%\n",
      "2018-04-17 17:06:47 start epoch 24/50\n",
      "2018-04-17 17:06:48 iteration 1/859: loss=0.074684, accuracy=98.230% -0.009056917 0.018215988 0.06552528\n",
      "2018-04-17 17:07:06 iteration 50/859: loss=0.176516, accuracy=95.886% 0.003296905 0.0402892 0.13293013\n",
      "2018-04-17 17:07:24 iteration 100/859: loss=0.091619, accuracy=97.449% -0.0027778656 0.025152255 0.06924456\n",
      "2018-04-17 17:07:42 iteration 150/859: loss=0.118982, accuracy=96.790% -0.004709674 0.031043664 0.09264758\n",
      "2018-04-17 17:07:59 iteration 200/859: loss=0.157322, accuracy=96.008% 1.2837816e-05 0.038554907 0.11875476\n",
      "2018-04-17 17:08:18 iteration 250/859: loss=0.208230, accuracy=95.325% 0.0013610463 0.044472925 0.16239613\n",
      "2018-04-17 17:08:36 iteration 300/859: loss=0.218777, accuracy=94.946% 0.0025371783 0.048214953 0.16802484\n",
      "2018-04-17 17:08:54 iteration 350/859: loss=0.200455, accuracy=94.812% 0.0062804283 0.04936683 0.14480779\n",
      "2018-04-17 17:09:12 iteration 400/859: loss=0.112682, accuracy=97.363% -0.0038932743 0.026094371 0.090480804\n",
      "2018-04-17 17:09:30 iteration 450/859: loss=0.095618, accuracy=97.815% -0.0060924054 0.02188887 0.079822004\n",
      "2018-04-17 17:09:48 iteration 500/859: loss=0.157451, accuracy=96.094% -0.0010433684 0.038077146 0.12041761\n",
      "2018-04-17 17:10:07 iteration 550/859: loss=0.106039, accuracy=97.168% -0.0017452458 0.027542105 0.08024222\n",
      "2018-04-17 17:10:25 iteration 600/859: loss=0.177937, accuracy=95.154% 0.0016830929 0.045048185 0.13120592\n",
      "2018-04-17 17:10:43 iteration 650/859: loss=0.170491, accuracy=95.618% 0.0035691268 0.042529173 0.12439279\n",
      "2018-04-17 17:11:01 iteration 700/859: loss=0.208294, accuracy=95.154% 0.005578056 0.046286337 0.15642972\n",
      "2018-04-17 17:11:20 iteration 750/859: loss=0.286524, accuracy=92.676% 0.015119215 0.06763897 0.20376566\n",
      "2018-04-17 17:11:37 iteration 800/859: loss=0.103774, accuracy=97.327% -0.0035570138 0.026506925 0.08082412\n",
      "2018-04-17 17:11:55 iteration 850/859: loss=0.102300, accuracy=97.424% 0.00069831865 0.025627151 0.07597485\n",
      "2018-04-17 17:11:58 iteration 859/859: loss=0.204394, accuracy=95.068% 0.004671227 0.046376575 0.15334615\n",
      "2018-04-17 17:12:09 end epoch 24/50: acc_train=96.916% acc_val=97.098% acc_test=96.906%\n",
      "2018-04-17 17:12:09 start epoch 25/50\n",
      "2018-04-17 17:12:09 iteration 1/859: loss=0.134493, accuracy=97.107% -0.0019183013 0.028920079 0.107491225\n",
      "2018-04-17 17:12:27 iteration 50/859: loss=0.172923, accuracy=95.337% 0.004230107 0.044650845 0.124042496\n",
      "2018-04-17 17:12:45 iteration 100/859: loss=0.038722, accuracy=99.060% -0.011467717 0.010925397 0.039264467\n",
      "2018-04-17 17:13:04 iteration 150/859: loss=0.072589, accuracy=98.206% -0.0061143516 0.018281594 0.06042201\n",
      "2018-04-17 17:13:22 iteration 200/859: loss=0.211741, accuracy=96.375% -0.00041836192 0.03467787 0.17748137\n",
      "2018-04-17 17:13:41 iteration 250/859: loss=0.145477, accuracy=96.570% -0.002280408 0.032456495 0.11530103\n",
      "2018-04-17 17:13:59 iteration 300/859: loss=0.034080, accuracy=99.097% -0.01263833 0.010620263 0.03609845\n",
      "2018-04-17 17:14:17 iteration 350/859: loss=0.106292, accuracy=97.021% -0.0032032537 0.029143829 0.08035174\n",
      "2018-04-17 17:14:35 iteration 400/859: loss=0.060021, accuracy=98.376% -0.00788608 0.01669849 0.051208176\n",
      "2018-04-17 17:14:53 iteration 450/859: loss=0.267744, accuracy=93.494% 0.011534579 0.061911874 0.19429742\n",
      "2018-04-17 17:15:11 iteration 500/859: loss=0.073964, accuracy=98.145% -0.0064036013 0.019437063 0.060930397\n",
      "2018-04-17 17:15:30 iteration 550/859: loss=0.256419, accuracy=94.006% 0.011379795 0.058134954 0.1869039\n",
      "2018-04-17 17:15:49 iteration 600/859: loss=0.052759, accuracy=98.572% -0.0068813115 0.014840166 0.04480014\n",
      "2018-04-17 17:16:06 iteration 650/859: loss=0.150494, accuracy=95.911% 0.0005054689 0.039336674 0.11065169\n",
      "2018-04-17 17:16:24 iteration 700/859: loss=0.194031, accuracy=95.300% 0.004640112 0.045224972 0.14416584\n",
      "2018-04-17 17:16:43 iteration 750/859: loss=0.114233, accuracy=96.814% -0.00529408 0.030227626 0.08929949\n",
      "2018-04-17 17:17:01 iteration 800/859: loss=0.020563, accuracy=99.426% -0.01093415 0.0072888387 0.024208473\n",
      "2018-04-17 17:17:19 iteration 850/859: loss=0.110024, accuracy=97.290% -0.0041288147 0.026525378 0.08762773\n",
      "2018-04-17 17:17:23 iteration 859/859: loss=0.047248, accuracy=98.547% -0.008223763 0.015198821 0.04027271\n",
      "2018-04-17 17:17:34 end epoch 25/50: acc_train=97.319% acc_val=97.165% acc_test=97.177%\n",
      "2018-04-17 17:17:34 start epoch 26/50\n",
      "2018-04-17 17:17:35 iteration 1/859: loss=0.324770, accuracy=92.822% 0.011421636 0.06711485 0.24623317\n",
      "2018-04-17 17:17:53 iteration 50/859: loss=0.116941, accuracy=97.009% -0.004637604 0.02881825 0.09276011\n",
      "2018-04-17 17:18:11 iteration 100/859: loss=0.055012, accuracy=98.608% -0.0072079645 0.015021372 0.047198955\n",
      "2018-04-17 17:18:29 iteration 150/859: loss=0.098482, accuracy=97.339% -0.0027401135 0.026726687 0.07449585\n",
      "2018-04-17 17:18:47 iteration 200/859: loss=0.153423, accuracy=96.057% 0.00036212607 0.03792639 0.11513419\n",
      "2018-04-17 17:19:05 iteration 250/859: loss=0.133387, accuracy=96.790% -0.0022378706 0.031545777 0.104078755\n",
      "2018-04-17 17:19:23 iteration 300/859: loss=0.099508, accuracy=97.424% 0.0022429929 0.024620067 0.07264491\n",
      "2018-04-17 17:19:42 iteration 350/859: loss=0.080555, accuracy=97.656% -0.0035676465 0.022927713 0.061195184\n",
      "2018-04-17 17:20:00 iteration 400/859: loss=0.198046, accuracy=95.557% 0.0001754062 0.04244711 0.15542331\n",
      "2018-04-17 17:20:19 iteration 450/859: loss=0.046771, accuracy=98.645% -0.008049186 0.014446622 0.04037337\n",
      "2018-04-17 17:20:38 iteration 500/859: loss=0.052528, accuracy=98.669% -0.00871001 0.01372354 0.047514442\n",
      "2018-04-17 17:20:56 iteration 550/859: loss=0.139837, accuracy=96.082% 0.0013902177 0.03789023 0.10055676\n",
      "2018-04-17 17:21:14 iteration 600/859: loss=0.020009, accuracy=99.475% -0.011535684 0.006755906 0.024788328\n",
      "2018-04-17 17:21:32 iteration 650/859: loss=0.059186, accuracy=98.376% -0.00590285 0.016430993 0.048657537\n",
      "2018-04-17 17:21:50 iteration 700/859: loss=0.154343, accuracy=95.630% 0.0034681796 0.041555516 0.10931886\n",
      "2018-04-17 17:22:08 iteration 750/859: loss=0.182790, accuracy=95.874% 0.00091141585 0.039834976 0.14204404\n",
      "2018-04-17 17:22:27 iteration 800/859: loss=0.157383, accuracy=96.338% -0.0005029015 0.03443111 0.12345488\n",
      "2018-04-17 17:22:45 iteration 850/859: loss=0.095506, accuracy=97.534% -0.0060420707 0.024169413 0.077378765\n",
      "2018-04-17 17:22:48 iteration 859/859: loss=0.073787, accuracy=98.083% -0.0050169243 0.019375533 0.05942796\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 17:23:00 end epoch 26/50: acc_train=97.172% acc_val=97.522% acc_test=97.375%\n",
      "2018-04-17 17:23:00 start epoch 27/50\n",
      "2018-04-17 17:23:00 iteration 1/859: loss=0.086478, accuracy=97.766% -0.0047391653 0.02228538 0.06893145\n",
      "2018-04-17 17:23:19 iteration 50/859: loss=0.157835, accuracy=96.179% -0.0025520169 0.03615448 0.12423206\n",
      "2018-04-17 17:23:37 iteration 100/859: loss=0.038369, accuracy=98.901% -0.007398163 0.011895414 0.033871733\n",
      "2018-04-17 17:23:56 iteration 150/859: loss=0.084947, accuracy=97.571% -0.0047147768 0.024033263 0.06562896\n",
      "2018-04-17 17:24:14 iteration 200/859: loss=0.031944, accuracy=99.097% -0.007814793 0.009931872 0.029826611\n",
      "2018-04-17 17:24:32 iteration 250/859: loss=0.134083, accuracy=96.838% 0.00066413294 0.031171134 0.10224777\n",
      "2018-04-17 17:24:50 iteration 300/859: loss=0.040709, accuracy=98.938% -0.0093055125 0.01133158 0.03868244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 17:25:08 iteration 350/859: loss=0.038810, accuracy=98.816% -0.0059594894 0.012427645 0.032341585\n",
      "2018-04-17 17:25:26 iteration 400/859: loss=0.122498, accuracy=97.131% -0.0020866871 0.027480325 0.09710396\n",
      "2018-04-17 17:25:45 iteration 450/859: loss=0.033271, accuracy=99.023% -0.009494679 0.010623482 0.032141965\n",
      "2018-04-17 17:26:03 iteration 500/859: loss=0.030834, accuracy=99.207% -0.009090029 0.009052093 0.030871874\n",
      "2018-04-17 17:26:21 iteration 550/859: loss=0.171544, accuracy=95.325% 0.0024973415 0.044605874 0.124440834\n",
      "2018-04-17 17:26:39 iteration 600/859: loss=0.095786, accuracy=97.607% -0.00418483 0.02348258 0.076488234\n",
      "2018-04-17 17:26:57 iteration 650/859: loss=0.091756, accuracy=97.546% -0.004285938 0.024322195 0.071719944\n",
      "2018-04-17 17:27:16 iteration 700/859: loss=0.067074, accuracy=98.267% -0.0057019503 0.017552402 0.055223923\n",
      "2018-04-17 17:27:34 iteration 750/859: loss=0.177632, accuracy=96.228% 0.0010603548 0.03619931 0.14037208\n",
      "2018-04-17 17:27:53 iteration 800/859: loss=0.070458, accuracy=98.279% -0.007147932 0.017109573 0.060496554\n",
      "2018-04-17 17:28:11 iteration 850/859: loss=0.078136, accuracy=97.974% -0.008377042 0.020291373 0.066221595\n",
      "2018-04-17 17:28:15 iteration 859/859: loss=0.160065, accuracy=95.898% 0.0054326532 0.03984189 0.11479091\n",
      "2018-04-17 17:28:25 end epoch 27/50: acc_train=96.241% acc_val=96.741% acc_test=96.302%\n",
      "2018-04-17 17:28:25 start epoch 28/50\n",
      "2018-04-17 17:28:25 iteration 1/859: loss=0.123331, accuracy=96.826% -0.001417055 0.03101626 0.09373191\n",
      "2018-04-17 17:28:43 iteration 50/859: loss=0.107823, accuracy=97.192% -0.0042947014 0.027356684 0.08476149\n",
      "2018-04-17 17:29:02 iteration 100/859: loss=0.205536, accuracy=95.227% 0.0046244366 0.045909323 0.15500215\n",
      "2018-04-17 17:29:20 iteration 150/859: loss=0.193259, accuracy=96.497% -0.0008000117 0.033666898 0.16039205\n",
      "2018-04-17 17:29:38 iteration 200/859: loss=0.127748, accuracy=96.362% -0.00029849279 0.03460476 0.09344216\n",
      "2018-04-17 17:29:56 iteration 250/859: loss=0.090742, accuracy=97.314% -0.0040444853 0.026411999 0.06837477\n",
      "2018-04-17 17:30:16 iteration 300/859: loss=0.113371, accuracy=97.107% -0.0019559343 0.028568555 0.086758375\n",
      "2018-04-17 17:30:34 iteration 350/859: loss=0.033823, accuracy=99.036% -0.007890898 0.01055253 0.031160979\n",
      "2018-04-17 17:30:52 iteration 400/859: loss=0.167099, accuracy=95.398% 0.003690964 0.043857086 0.11955052\n",
      "2018-04-17 17:31:10 iteration 450/859: loss=0.048490, accuracy=98.718% -0.005561358 0.013238432 0.040813353\n",
      "2018-04-17 17:31:28 iteration 500/859: loss=0.043262, accuracy=98.950% -0.0073836534 0.011206007 0.03943934\n",
      "2018-04-17 17:31:46 iteration 550/859: loss=0.088988, accuracy=97.852% -5.2639585e-05 0.021351572 0.06768957\n",
      "2018-04-17 17:32:04 iteration 600/859: loss=0.235578, accuracy=95.239% 0.005136302 0.04563374 0.18480834\n",
      "2018-04-17 17:32:22 iteration 650/859: loss=0.107173, accuracy=96.985% -0.0024195164 0.029418712 0.08017383\n",
      "2018-04-17 17:32:40 iteration 700/859: loss=0.182698, accuracy=95.276% 0.0050960905 0.044125468 0.13347626\n",
      "2018-04-17 17:32:58 iteration 750/859: loss=0.052546, accuracy=98.657% -0.0078171985 0.013985118 0.046378426\n",
      "2018-04-17 17:33:16 iteration 800/859: loss=0.081370, accuracy=97.864% -0.0031883232 0.020778764 0.06377964\n",
      "2018-04-17 17:33:34 iteration 850/859: loss=0.109234, accuracy=96.887% -0.0033618584 0.029986074 0.08261017\n",
      "2018-04-17 17:33:38 iteration 859/859: loss=0.078643, accuracy=97.876% -0.0044575883 0.021116797 0.061983585\n",
      "2018-04-17 17:33:49 end epoch 28/50: acc_train=97.050% acc_val=97.076% acc_test=97.146%\n",
      "2018-04-17 17:33:49 start epoch 29/50\n",
      "2018-04-17 17:33:49 iteration 1/859: loss=0.136393, accuracy=96.240% 0.0011164729 0.036469746 0.098806486\n",
      "2018-04-17 17:34:07 iteration 50/859: loss=0.318091, accuracy=93.115% 0.014402087 0.06529874 0.23839006\n",
      "2018-04-17 17:34:25 iteration 100/859: loss=0.088072, accuracy=97.571% -0.00413929 0.023722133 0.06848952\n",
      "2018-04-17 17:34:43 iteration 150/859: loss=0.074187, accuracy=98.096% -0.0035645005 0.018915972 0.058835667\n",
      "2018-04-17 17:35:02 iteration 200/859: loss=0.104105, accuracy=97.388% -0.0045443056 0.026200434 0.08244933\n",
      "2018-04-17 17:35:20 iteration 250/859: loss=0.150632, accuracy=96.716% 0.0026387821 0.031291734 0.11670196\n",
      "2018-04-17 17:35:38 iteration 300/859: loss=0.152410, accuracy=95.923% 0.0019815557 0.0393716 0.111057326\n",
      "2018-04-17 17:35:56 iteration 350/859: loss=0.171643, accuracy=95.117% 0.0019643127 0.04619726 0.12348172\n",
      "2018-04-17 17:36:14 iteration 400/859: loss=0.080173, accuracy=97.852% -0.004882198 0.02142058 0.06363482\n",
      "2018-04-17 17:36:32 iteration 450/859: loss=0.126667, accuracy=96.753% -0.003076377 0.031429734 0.09831393\n",
      "2018-04-17 17:36:50 iteration 500/859: loss=0.083687, accuracy=97.949% -0.004050106 0.020411858 0.06732543\n",
      "2018-04-17 17:37:08 iteration 550/859: loss=0.065197, accuracy=98.291% -0.006701806 0.017345171 0.054553434\n",
      "2018-04-17 17:37:27 iteration 600/859: loss=0.104645, accuracy=97.083% 0.00027783326 0.028357828 0.07600971\n",
      "2018-04-17 17:37:46 iteration 650/859: loss=0.023031, accuracy=99.341% -0.010212397 0.0076108514 0.025632434\n",
      "2018-04-17 17:38:05 iteration 700/859: loss=0.273358, accuracy=94.189% 0.01023078 0.054454885 0.20867184\n",
      "2018-04-17 17:38:23 iteration 750/859: loss=0.068570, accuracy=98.193% -0.0069599673 0.018217376 0.05731292\n",
      "2018-04-17 17:38:42 iteration 800/859: loss=0.027274, accuracy=99.341% -0.006338955 0.0074273883 0.02618552\n",
      "2018-04-17 17:39:00 iteration 850/859: loss=0.098826, accuracy=97.241% -0.0014738058 0.026974529 0.073325485\n",
      "2018-04-17 17:39:03 iteration 859/859: loss=0.030106, accuracy=99.133% -0.0066179624 0.009163795 0.027560242\n",
      "2018-04-17 17:39:13 end epoch 29/50: acc_train=97.316% acc_val=97.433% acc_test=97.198%\n",
      "2018-04-17 17:39:13 start epoch 30/50\n",
      "2018-04-17 17:39:13 iteration 1/859: loss=0.171014, accuracy=95.569% 0.0032848374 0.042431146 0.12529829\n",
      "2018-04-17 17:39:31 iteration 50/859: loss=0.167301, accuracy=95.557% 0.0016461121 0.041675262 0.123980016\n",
      "2018-04-17 17:39:49 iteration 100/859: loss=0.026870, accuracy=99.231% -0.007849023 0.008524217 0.026195299\n",
      "2018-04-17 17:40:08 iteration 150/859: loss=0.113892, accuracy=97.119% -0.002678972 0.027860245 0.08871031\n",
      "2018-04-17 17:40:27 iteration 200/859: loss=0.139119, accuracy=96.326% 0.004944923 0.034816436 0.099357866\n",
      "2018-04-17 17:40:45 iteration 250/859: loss=0.058798, accuracy=98.486% -0.0059985546 0.015155191 0.04964163\n",
      "2018-04-17 17:41:03 iteration 300/859: loss=0.039698, accuracy=98.962% -0.008058972 0.010933464 0.036823623\n",
      "2018-04-17 17:41:22 iteration 350/859: loss=0.118856, accuracy=96.863% -0.0018415872 0.02966184 0.091035314\n",
      "2018-04-17 17:41:40 iteration 400/859: loss=0.131091, accuracy=96.545% 0.006138134 0.033146717 0.0918061\n",
      "2018-04-17 17:41:58 iteration 450/859: loss=0.152188, accuracy=95.862% 0.00030161292 0.039414123 0.11247237\n",
      "2018-04-17 17:42:17 iteration 500/859: loss=0.058187, accuracy=98.621% -0.0058106254 0.013886687 0.050111163\n",
      "2018-04-17 17:42:35 iteration 550/859: loss=0.125710, accuracy=96.863% 0.00057443 0.030248368 0.09488737\n",
      "2018-04-17 17:42:53 iteration 600/859: loss=0.132000, accuracy=96.753% -4.8731767e-05 0.031014109 0.10103463\n",
      "2018-04-17 17:43:11 iteration 650/859: loss=0.182494, accuracy=95.691% 0.004660552 0.04121452 0.136619\n",
      "2018-04-17 17:43:29 iteration 700/859: loss=0.055841, accuracy=98.499% -0.0059604747 0.015203374 0.046598464\n",
      "2018-04-17 17:43:47 iteration 750/859: loss=0.205653, accuracy=94.861% 0.004919669 0.04915777 0.15157527\n",
      "2018-04-17 17:44:05 iteration 800/859: loss=0.132987, accuracy=97.253% 0.0014462812 0.026443124 0.10509744\n",
      "2018-04-17 17:44:24 iteration 850/859: loss=0.037218, accuracy=99.011% -0.006561783 0.010431153 0.03334871\n",
      "2018-04-17 17:44:27 iteration 859/859: loss=0.108335, accuracy=97.131% -0.0045040315 0.027740914 0.08509809\n",
      "2018-04-17 17:44:38 end epoch 30/50: acc_train=97.213% acc_val=97.098% acc_test=97.135%\n",
      "2018-04-17 17:44:38 start epoch 31/50\n",
      "2018-04-17 17:44:38 iteration 1/859: loss=0.124162, accuracy=96.814% -0.00043886527 0.030321915 0.094279245\n",
      "2018-04-17 17:44:56 iteration 50/859: loss=0.030187, accuracy=99.194% -0.008723755 0.008782664 0.030127887\n",
      "2018-04-17 17:45:14 iteration 100/859: loss=0.034520, accuracy=99.231% -0.0058141397 0.008265739 0.032068085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 17:45:33 iteration 150/859: loss=0.136088, accuracy=97.229% 0.001802243 0.026898265 0.10738721\n",
      "2018-04-17 17:45:51 iteration 200/859: loss=0.068976, accuracy=98.108% -0.003868966 0.018197099 0.05464827\n",
      "2018-04-17 17:46:08 iteration 250/859: loss=0.059987, accuracy=98.315% -0.0042247823 0.016714785 0.047497097\n",
      "2018-04-17 17:46:27 iteration 300/859: loss=0.058182, accuracy=98.438% -0.0076171756 0.016015308 0.049783967\n",
      "2018-04-17 17:46:45 iteration 350/859: loss=0.082626, accuracy=97.815% -0.0028093085 0.021457763 0.06397714\n",
      "2018-04-17 17:47:04 iteration 400/859: loss=0.194528, accuracy=97.009% -0.0007502711 0.02983357 0.16544506\n",
      "2018-04-17 17:47:22 iteration 450/859: loss=0.061464, accuracy=98.242% -0.004082558 0.01748038 0.0480662\n",
      "2018-04-17 17:47:42 iteration 500/859: loss=0.139463, accuracy=96.802% 0.0007971901 0.030937454 0.1077282\n",
      "2018-04-17 17:48:00 iteration 550/859: loss=0.088171, accuracy=97.839% -0.0021482662 0.021378856 0.068940714\n",
      "2018-04-17 17:48:18 iteration 600/859: loss=0.166313, accuracy=95.447% 0.0049257344 0.043203544 0.11818391\n",
      "2018-04-17 17:48:36 iteration 650/859: loss=0.090686, accuracy=97.632% -0.0027487543 0.023309764 0.0701253\n",
      "2018-04-17 17:48:55 iteration 700/859: loss=0.119040, accuracy=97.009% 0.00083302363 0.028870981 0.089336336\n",
      "2018-04-17 17:49:13 iteration 750/859: loss=0.056233, accuracy=98.425% -0.007902169 0.015899684 0.048235275\n",
      "2018-04-17 17:49:31 iteration 800/859: loss=0.061858, accuracy=98.328% -0.0061662467 0.016720016 0.05130472\n",
      "2018-04-17 17:49:49 iteration 850/859: loss=0.068514, accuracy=98.279% -0.0062003494 0.01722802 0.057486482\n",
      "2018-04-17 17:49:53 iteration 859/859: loss=0.163935, accuracy=97.339% -0.006209054 0.02645435 0.14368954\n",
      "2018-04-17 17:50:04 end epoch 31/50: acc_train=97.362% acc_val=97.433% acc_test=97.271%\n",
      "2018-04-17 17:50:04 start epoch 32/50\n",
      "2018-04-17 17:50:04 iteration 1/859: loss=0.063169, accuracy=98.267% -0.0063443976 0.017138597 0.05237493\n",
      "2018-04-17 17:50:22 iteration 50/859: loss=0.171955, accuracy=95.386% 0.0066055865 0.044356048 0.12099309\n",
      "2018-04-17 17:50:40 iteration 100/859: loss=0.051095, accuracy=98.645% -0.0045852973 0.013672845 0.042007122\n",
      "2018-04-17 17:50:58 iteration 150/859: loss=0.122631, accuracy=96.716% 0.0004302077 0.030702025 0.091498286\n",
      "2018-04-17 17:51:16 iteration 200/859: loss=0.074580, accuracy=97.998% -0.00278968 0.019959444 0.057409786\n",
      "2018-04-17 17:51:35 iteration 250/859: loss=0.050435, accuracy=98.706% -0.0036017552 0.012914134 0.041122377\n",
      "2018-04-17 17:51:53 iteration 300/859: loss=0.147362, accuracy=96.106% 0.0021950067 0.037318073 0.10784881\n",
      "2018-04-17 17:52:11 iteration 350/859: loss=0.267436, accuracy=93.652% 0.014002526 0.05977012 0.19366354\n",
      "2018-04-17 17:52:29 iteration 400/859: loss=0.069340, accuracy=98.022% -0.003813963 0.019390857 0.05376272\n",
      "2018-04-17 17:52:48 iteration 450/859: loss=0.050222, accuracy=98.499% -0.0050026374 0.014974602 0.040250227\n",
      "2018-04-17 17:53:06 iteration 500/859: loss=0.103454, accuracy=97.705% -0.00387468 0.022296254 0.085032046\n",
      "2018-04-17 17:53:24 iteration 550/859: loss=0.047238, accuracy=98.853% -0.005512075 0.011665289 0.0410849\n",
      "2018-04-17 17:53:43 iteration 600/859: loss=0.177325, accuracy=95.752% 0.0029929776 0.040694796 0.13363747\n",
      "2018-04-17 17:54:01 iteration 650/859: loss=0.070231, accuracy=98.169% -0.003546659 0.018171925 0.05560597\n",
      "2018-04-17 17:54:19 iteration 700/859: loss=0.043383, accuracy=98.804% -0.0051122876 0.0121320225 0.036363073\n",
      "2018-04-17 17:54:37 iteration 750/859: loss=0.020153, accuracy=99.524% -0.010216248 0.005889314 0.024479557\n",
      "2018-04-17 17:54:56 iteration 800/859: loss=0.059242, accuracy=98.535% -0.006259306 0.014794713 0.050706323\n",
      "2018-04-17 17:55:15 iteration 850/859: loss=0.155529, accuracy=95.667% 0.0022677563 0.041297786 0.11196372\n",
      "2018-04-17 17:55:18 iteration 859/859: loss=0.113960, accuracy=97.131% 3.0537438e-05 0.02760294 0.08632627\n",
      "2018-04-17 17:55:28 end epoch 32/50: acc_train=97.672% acc_val=97.455% acc_test=97.542%\n",
      "2018-04-17 17:55:28 start epoch 33/50\n",
      "2018-04-17 17:55:28 iteration 1/859: loss=0.081409, accuracy=97.986% -0.00078288885 0.019909346 0.06228291\n",
      "2018-04-17 17:55:47 iteration 50/859: loss=0.150128, accuracy=96.216% 0.0041285567 0.03619907 0.10980071\n",
      "2018-04-17 17:56:05 iteration 100/859: loss=0.014653, accuracy=99.585% -0.0076848892 0.0050300644 0.01730793\n",
      "2018-04-17 17:56:24 iteration 150/859: loss=0.073081, accuracy=97.986% -0.0049562873 0.019887581 0.05814934\n",
      "2018-04-17 17:56:42 iteration 200/859: loss=0.137404, accuracy=96.484% 0.0037518987 0.033262905 0.1003887\n",
      "2018-04-17 17:57:00 iteration 250/859: loss=0.045853, accuracy=98.901% -0.0036953813 0.011110611 0.038438253\n",
      "2018-04-17 17:57:19 iteration 300/859: loss=0.072101, accuracy=98.035% -0.0021329543 0.019176114 0.05505736\n",
      "2018-04-17 17:57:38 iteration 350/859: loss=0.180634, accuracy=95.813% 0.009985901 0.04049061 0.13015759\n",
      "2018-04-17 17:57:56 iteration 400/859: loss=0.064440, accuracy=98.450% -0.0050165826 0.015390897 0.054065388\n",
      "2018-04-17 17:58:15 iteration 450/859: loss=0.167347, accuracy=95.911% 0.0040102047 0.038891383 0.12444527\n",
      "2018-04-17 17:58:33 iteration 500/859: loss=0.230380, accuracy=94.019% 0.010792593 0.056616236 0.16297165\n",
      "2018-04-17 17:58:51 iteration 550/859: loss=0.060842, accuracy=98.181% -0.003624163 0.017961286 0.04650519\n",
      "2018-04-17 17:59:09 iteration 600/859: loss=0.151936, accuracy=95.837% 0.004740298 0.03912283 0.10807332\n",
      "2018-04-17 17:59:27 iteration 650/859: loss=0.070192, accuracy=98.181% -0.0027626068 0.017998565 0.054956436\n",
      "2018-04-17 17:59:46 iteration 700/859: loss=0.011385, accuracy=99.695% -0.008190775 0.0038123317 0.01576367\n",
      "2018-04-17 18:00:04 iteration 750/859: loss=0.222645, accuracy=94.287% 0.010647396 0.05397732 0.15802073\n",
      "2018-04-17 18:00:22 iteration 800/859: loss=0.148740, accuracy=95.752% 0.0066545545 0.04087041 0.10121511\n",
      "2018-04-17 18:00:41 iteration 850/859: loss=0.080955, accuracy=98.059% -0.0028427579 0.019095529 0.06470212\n",
      "2018-04-17 18:00:44 iteration 859/859: loss=0.152206, accuracy=95.984% 0.0043002693 0.038794912 0.10911104\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 18:00:56 end epoch 33/50: acc_train=97.988% acc_val=97.589% acc_test=97.542%\n",
      "2018-04-17 18:00:56 start epoch 34/50\n",
      "2018-04-17 18:00:56 iteration 1/859: loss=0.069849, accuracy=98.010% -0.0039474303 0.01926118 0.054535568\n",
      "2018-04-17 18:01:14 iteration 50/859: loss=0.061400, accuracy=98.303% 0.00014661886 0.016626555 0.044627078\n",
      "2018-04-17 18:01:32 iteration 100/859: loss=0.165230, accuracy=95.642% 0.0082198605 0.041938074 0.115072206\n",
      "2018-04-17 18:01:50 iteration 150/859: loss=0.040045, accuracy=98.938% -0.006349288 0.010867564 0.03552656\n",
      "2018-04-17 18:02:09 iteration 200/859: loss=0.190691, accuracy=95.190% 0.009522833 0.045546323 0.13562223\n",
      "2018-04-17 18:02:27 iteration 250/859: loss=0.092879, accuracy=97.571% -0.0011331736 0.023457315 0.07055459\n",
      "2018-04-17 18:02:46 iteration 300/859: loss=0.039276, accuracy=99.011% -0.0069795167 0.010097711 0.0361576\n",
      "2018-04-17 18:03:04 iteration 350/859: loss=0.098777, accuracy=97.437% -0.00071051385 0.024778217 0.07470912\n",
      "2018-04-17 18:03:22 iteration 400/859: loss=0.112978, accuracy=97.424% 0.0004110246 0.025182694 0.08738421\n",
      "2018-04-17 18:03:40 iteration 450/859: loss=0.112145, accuracy=96.851% 0.0017448461 0.030095965 0.0803038\n",
      "2018-04-17 18:03:59 iteration 500/859: loss=0.089511, accuracy=97.656% -0.003176966 0.022903506 0.069784336\n",
      "2018-04-17 18:04:16 iteration 550/859: loss=0.091392, accuracy=97.437% -0.0039321277 0.02451608 0.07080827\n",
      "2018-04-17 18:04:35 iteration 600/859: loss=0.098519, accuracy=97.766% 0.00078318716 0.021548044 0.07618823\n",
      "2018-04-17 18:04:53 iteration 650/859: loss=0.270360, accuracy=94.385% 0.011550752 0.052922092 0.20588732\n",
      "2018-04-17 18:05:12 iteration 700/859: loss=0.061206, accuracy=98.450% -0.0063530216 0.015353884 0.05220518\n",
      "2018-04-17 18:05:30 iteration 750/859: loss=0.052585, accuracy=98.596% -0.0058791526 0.014033521 0.044431053\n",
      "2018-04-17 18:05:48 iteration 800/859: loss=0.126325, accuracy=96.777% 0.001478021 0.030734068 0.09411245\n",
      "2018-04-17 18:06:06 iteration 850/859: loss=0.186188, accuracy=95.520% 0.006693866 0.04312864 0.13636538\n",
      "2018-04-17 18:06:10 iteration 859/859: loss=0.020406, accuracy=99.438% -0.007672624 0.00631406 0.021764623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 18:06:20 end epoch 34/50: acc_train=97.853% acc_val=97.500% acc_test=97.844%\n",
      "2018-04-17 18:06:20 start epoch 35/50\n",
      "2018-04-17 18:06:20 iteration 1/859: loss=0.116842, accuracy=97.668% -0.0008278385 0.022319663 0.09534985\n",
      "2018-04-17 18:06:38 iteration 50/859: loss=0.113368, accuracy=97.070% 0.0005633718 0.028272295 0.08453277\n",
      "2018-04-17 18:06:56 iteration 100/859: loss=0.193887, accuracy=95.581% 0.009434494 0.04176092 0.14269137\n",
      "2018-04-17 18:07:15 iteration 150/859: loss=0.181990, accuracy=95.642% 0.004929104 0.04102394 0.13603738\n",
      "2018-04-17 18:07:33 iteration 200/859: loss=0.069895, accuracy=98.096% -0.0028457567 0.018432943 0.054307394\n",
      "2018-04-17 18:07:52 iteration 250/859: loss=0.038545, accuracy=99.084% -0.005333265 0.0094925 0.03438562\n",
      "2018-04-17 18:08:10 iteration 300/859: loss=0.157671, accuracy=95.984% 0.0049909577 0.03818698 0.11449304\n",
      "2018-04-17 18:08:28 iteration 350/859: loss=0.075519, accuracy=97.913% -0.00015249258 0.020270472 0.055400535\n",
      "2018-04-17 18:08:46 iteration 400/859: loss=0.071929, accuracy=98.010% -5.0674727e-05 0.019656425 0.052323185\n",
      "2018-04-17 18:09:04 iteration 450/859: loss=0.045533, accuracy=98.730% -0.0035950406 0.0128899785 0.036237925\n",
      "2018-04-17 18:09:22 iteration 500/859: loss=0.063122, accuracy=98.401% -0.0051295618 0.0159657 0.05228609\n",
      "2018-04-17 18:09:40 iteration 550/859: loss=0.151289, accuracy=96.704% 0.0037224279 0.032044686 0.11552164\n",
      "2018-04-17 18:09:58 iteration 600/859: loss=0.076762, accuracy=97.986% -0.0043685576 0.020247832 0.060882792\n",
      "2018-04-17 18:10:17 iteration 650/859: loss=0.041698, accuracy=98.950% -0.0052611446 0.010575195 0.036383457\n",
      "2018-04-17 18:10:35 iteration 700/859: loss=0.097570, accuracy=97.437% -0.0008738462 0.025084818 0.073359415\n",
      "2018-04-17 18:10:53 iteration 750/859: loss=0.111958, accuracy=96.777% 0.0024500112 0.031006128 0.07850166\n",
      "2018-04-17 18:11:12 iteration 800/859: loss=0.041627, accuracy=98.901% -0.005632299 0.011178387 0.036080733\n",
      "2018-04-17 18:11:30 iteration 850/859: loss=0.025713, accuracy=99.329% -0.007462412 0.0073689534 0.025806554\n",
      "2018-04-17 18:11:33 iteration 859/859: loss=0.062844, accuracy=98.499% -0.004300382 0.014875799 0.052268572\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 18:11:44 end epoch 35/50: acc_train=97.844% acc_val=97.991% acc_test=97.865%\n",
      "2018-04-17 18:11:44 start epoch 36/50\n",
      "2018-04-17 18:11:45 iteration 1/859: loss=0.033842, accuracy=99.158% -0.0064367997 0.008767849 0.03151092\n",
      "2018-04-17 18:12:03 iteration 50/859: loss=0.045511, accuracy=98.608% -0.003594181 0.013946402 0.03515874\n",
      "2018-04-17 18:12:22 iteration 100/859: loss=0.064152, accuracy=98.181% -0.0041876407 0.017916461 0.050423153\n",
      "2018-04-17 18:12:40 iteration 150/859: loss=0.098177, accuracy=97.803% 0.00017119163 0.021331228 0.07667502\n",
      "2018-04-17 18:12:58 iteration 200/859: loss=0.022221, accuracy=99.402% -0.0076033063 0.006541738 0.023282297\n",
      "2018-04-17 18:13:16 iteration 250/859: loss=0.093194, accuracy=98.010% -5.1993084e-05 0.019714668 0.07353105\n",
      "2018-04-17 18:13:35 iteration 300/859: loss=0.081338, accuracy=98.071% -9.594761e-05 0.01905195 0.062381808\n",
      "2018-04-17 18:13:53 iteration 350/859: loss=0.182035, accuracy=95.923% 0.005905862 0.03862949 0.13749945\n",
      "2018-04-17 18:14:11 iteration 400/859: loss=0.016032, accuracy=99.561% -0.00718347 0.0049528764 0.018262701\n",
      "2018-04-17 18:14:30 iteration 450/859: loss=0.058928, accuracy=98.389% -0.0036563613 0.016160835 0.046424016\n",
      "2018-04-17 18:14:48 iteration 500/859: loss=0.056509, accuracy=98.340% -0.002270085 0.016337192 0.042442095\n",
      "2018-04-17 18:15:06 iteration 550/859: loss=0.047896, accuracy=98.511% -0.0043282397 0.0148193035 0.037404444\n",
      "2018-04-17 18:15:26 iteration 600/859: loss=0.019532, accuracy=99.475% -0.0073055252 0.0057093343 0.021128345\n",
      "2018-04-17 18:15:44 iteration 650/859: loss=0.102242, accuracy=97.461% 0.00035070314 0.024447871 0.07744293\n",
      "2018-04-17 18:16:02 iteration 700/859: loss=0.145178, accuracy=96.472% 0.0037186693 0.034097813 0.10736126\n",
      "2018-04-17 18:16:20 iteration 750/859: loss=0.031382, accuracy=99.084% -0.008108967 0.009396201 0.030094536\n",
      "2018-04-17 18:16:39 iteration 800/859: loss=0.038665, accuracy=99.011% -0.0044238353 0.009869389 0.03321932\n",
      "2018-04-17 18:16:57 iteration 850/859: loss=0.051217, accuracy=98.608% -0.0044528316 0.013589039 0.04208047\n",
      "2018-04-17 18:17:00 iteration 859/859: loss=0.006676, accuracy=99.878% -0.00887381 0.0020982265 0.013451711\n",
      "2018-04-17 18:17:10 end epoch 36/50: acc_train=97.500% acc_val=97.812% acc_test=97.635%\n",
      "2018-04-17 18:17:10 start epoch 37/50\n",
      "2018-04-17 18:17:10 iteration 1/859: loss=0.088193, accuracy=97.693% -0.0009949243 0.02253771 0.066650055\n",
      "2018-04-17 18:17:29 iteration 50/859: loss=0.024697, accuracy=99.329% -0.006281088 0.007072271 0.0239054\n",
      "2018-04-17 18:17:48 iteration 100/859: loss=0.056231, accuracy=98.413% -0.0070610433 0.016059086 0.047233265\n",
      "2018-04-17 18:18:06 iteration 150/859: loss=0.039395, accuracy=98.962% -0.0048006093 0.010540103 0.03365515\n",
      "2018-04-17 18:18:25 iteration 200/859: loss=0.096554, accuracy=97.437% -0.00015607153 0.0248586 0.07185133\n",
      "2018-04-17 18:18:43 iteration 250/859: loss=0.027323, accuracy=99.304% -0.006361489 0.007339856 0.026344433\n",
      "2018-04-17 18:19:01 iteration 300/859: loss=0.016334, accuracy=99.524% -0.0057974705 0.005137049 0.016994888\n",
      "2018-04-17 18:19:20 iteration 350/859: loss=0.091295, accuracy=97.510% -0.00012095664 0.024001371 0.06741424\n",
      "2018-04-17 18:19:38 iteration 400/859: loss=0.036945, accuracy=98.914% -0.0055176485 0.011007371 0.03145569\n",
      "2018-04-17 18:19:56 iteration 450/859: loss=0.149846, accuracy=96.838% 0.0024012362 0.030439746 0.11700465\n",
      "2018-04-17 18:20:14 iteration 500/859: loss=0.032645, accuracy=99.146% -0.00742239 0.00884406 0.03122367\n",
      "2018-04-17 18:20:33 iteration 550/859: loss=0.211396, accuracy=95.105% 0.00988936 0.045997806 0.15550926\n",
      "2018-04-17 18:20:51 iteration 600/859: loss=0.090977, accuracy=97.717% 0.000395872 0.022252936 0.06832794\n",
      "2018-04-17 18:21:09 iteration 650/859: loss=0.119442, accuracy=97.021% 0.0015269332 0.02848205 0.0894334\n",
      "2018-04-17 18:21:28 iteration 700/859: loss=0.229250, accuracy=95.044% 0.009749903 0.047020063 0.1724796\n",
      "2018-04-17 18:21:46 iteration 750/859: loss=0.165830, accuracy=95.789% 0.005029471 0.039545156 0.12125507\n",
      "2018-04-17 18:22:04 iteration 800/859: loss=0.112244, accuracy=96.814% 0.0023288887 0.030139768 0.07977511\n",
      "2018-04-17 18:22:23 iteration 850/859: loss=0.136586, accuracy=96.375% 0.0015627906 0.034324937 0.10069837\n",
      "2018-04-17 18:22:26 iteration 859/859: loss=0.032949, accuracy=99.023% -0.005626548 0.009895676 0.02868018\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 18:22:38 end epoch 37/50: acc_train=98.078% acc_val=98.036% acc_test=98.135%\n",
      "2018-04-17 18:22:38 start epoch 38/50\n",
      "2018-04-17 18:22:39 iteration 1/859: loss=0.125173, accuracy=97.083% 0.0036346633 0.028059203 0.093479395\n",
      "2018-04-17 18:22:57 iteration 50/859: loss=0.047838, accuracy=98.560% -0.004331103 0.0144474255 0.03772133\n",
      "2018-04-17 18:23:15 iteration 100/859: loss=0.129978, accuracy=96.826% 0.003267344 0.030600915 0.09610973\n",
      "2018-04-17 18:23:34 iteration 150/859: loss=0.080557, accuracy=97.791% -0.0021413458 0.02177434 0.0609238\n",
      "2018-04-17 18:23:52 iteration 200/859: loss=0.033001, accuracy=99.231% -0.007743685 0.008041637 0.03270284\n",
      "2018-04-17 18:24:11 iteration 250/859: loss=0.047916, accuracy=98.706% -0.0046405196 0.012959999 0.039596386\n",
      "2018-04-17 18:24:29 iteration 300/859: loss=0.055461, accuracy=98.523% -0.003752382 0.014507314 0.044706043\n",
      "2018-04-17 18:24:48 iteration 350/859: loss=0.050796, accuracy=98.633% -0.0038769816 0.013461011 0.04121194\n",
      "2018-04-17 18:25:07 iteration 400/859: loss=0.166708, accuracy=95.862% 0.0071344166 0.03920601 0.12036723\n",
      "2018-04-17 18:25:25 iteration 450/859: loss=0.018180, accuracy=99.573% -0.0065888823 0.004793114 0.019975781\n",
      "2018-04-17 18:25:44 iteration 500/859: loss=0.234471, accuracy=95.227% 0.006836577 0.04521308 0.18242091\n",
      "2018-04-17 18:26:02 iteration 550/859: loss=0.115222, accuracy=97.107% 0.001340867 0.027722394 0.08615863\n",
      "2018-04-17 18:26:20 iteration 600/859: loss=0.076699, accuracy=97.754% -0.00047115036 0.021575155 0.05559504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 18:26:38 iteration 650/859: loss=0.040136, accuracy=99.097% -0.0042264224 0.009474603 0.03488793\n",
      "2018-04-17 18:26:56 iteration 700/859: loss=0.027951, accuracy=99.377% -0.0071415007 0.006649027 0.028443363\n",
      "2018-04-17 18:27:15 iteration 750/859: loss=0.068174, accuracy=98.169% -0.0017162736 0.017879684 0.052010253\n",
      "2018-04-17 18:27:33 iteration 800/859: loss=0.165260, accuracy=95.715% 0.0067072934 0.04086591 0.1176866\n",
      "2018-04-17 18:27:51 iteration 850/859: loss=0.033708, accuracy=99.109% -0.0062521347 0.009119974 0.03084046\n",
      "2018-04-17 18:27:54 iteration 859/859: loss=0.030278, accuracy=99.341% -0.008481493 0.007411778 0.03134814\n",
      "2018-04-17 18:28:05 end epoch 38/50: acc_train=97.913% acc_val=97.924% acc_test=97.802%\n",
      "2018-04-17 18:28:05 start epoch 39/50\n",
      "2018-04-17 18:28:05 iteration 1/859: loss=0.015153, accuracy=99.573% -0.007374841 0.004872028 0.01765602\n",
      "2018-04-17 18:28:23 iteration 50/859: loss=0.121534, accuracy=97.949% -0.0006394277 0.020433305 0.10174049\n",
      "2018-04-17 18:28:42 iteration 100/859: loss=0.033782, accuracy=99.036% -0.0053410693 0.009948919 0.029174186\n",
      "2018-04-17 18:29:00 iteration 150/859: loss=0.175848, accuracy=95.996% 0.0054645105 0.03799861 0.13238484\n",
      "2018-04-17 18:29:18 iteration 200/859: loss=0.134747, accuracy=96.436% 0.004363374 0.034125753 0.09625782\n",
      "2018-04-17 18:29:36 iteration 250/859: loss=0.045909, accuracy=98.792% -0.005872722 0.012181295 0.039600246\n",
      "2018-04-17 18:29:55 iteration 300/859: loss=0.056698, accuracy=98.596% -0.005054985 0.013782698 0.04796988\n",
      "2018-04-17 18:30:13 iteration 350/859: loss=0.048358, accuracy=98.718% -0.0034187748 0.012839997 0.038936637\n",
      "2018-04-17 18:30:31 iteration 400/859: loss=0.293822, accuracy=96.130% 0.005332505 0.03822175 0.25026822\n",
      "2018-04-17 18:30:49 iteration 450/859: loss=0.109141, accuracy=96.973% 0.0028963774 0.029073644 0.077171385\n",
      "2018-04-17 18:31:08 iteration 500/859: loss=0.022055, accuracy=99.353% -0.0063732057 0.006822446 0.021605382\n",
      "2018-04-17 18:31:26 iteration 550/859: loss=0.082831, accuracy=97.620% -0.0014704184 0.023125317 0.06117578\n",
      "2018-04-17 18:31:45 iteration 600/859: loss=0.190769, accuracy=96.021% 0.0070860335 0.038376413 0.14530635\n",
      "2018-04-17 18:32:03 iteration 650/859: loss=0.129886, accuracy=96.729% 0.0039026595 0.031651743 0.094331294\n",
      "2018-04-17 18:32:21 iteration 700/859: loss=0.068401, accuracy=98.193% -0.0031575009 0.017509455 0.05404914\n",
      "2018-04-17 18:32:39 iteration 750/859: loss=0.054305, accuracy=98.572% -0.003413299 0.013822508 0.043895528\n",
      "2018-04-17 18:32:58 iteration 800/859: loss=0.071538, accuracy=98.096% -0.002763171 0.018588077 0.055712793\n",
      "2018-04-17 18:33:16 iteration 850/859: loss=0.102623, accuracy=97.803% 0.00054872467 0.021476155 0.080598496\n",
      "2018-04-17 18:33:19 iteration 859/859: loss=0.050058, accuracy=98.706% -0.0032369285 0.012913254 0.04038162\n",
      "2018-04-17 18:33:30 end epoch 39/50: acc_train=97.866% acc_val=97.567% acc_test=97.708%\n",
      "2018-04-17 18:33:30 start epoch 40/50\n",
      "2018-04-17 18:33:31 iteration 1/859: loss=0.038569, accuracy=98.999% -0.0046194866 0.0102406405 0.03294775\n",
      "2018-04-17 18:33:48 iteration 50/859: loss=0.103461, accuracy=97.925% 0.0006335089 0.0203125 0.082515076\n",
      "2018-04-17 18:34:07 iteration 100/859: loss=0.086982, accuracy=97.815% 0.0011515834 0.021251818 0.064578414\n",
      "2018-04-17 18:34:25 iteration 150/859: loss=0.024986, accuracy=99.377% -0.0060902997 0.006559623 0.024517013\n",
      "2018-04-17 18:34:43 iteration 200/859: loss=0.153765, accuracy=95.947% 0.005243386 0.039016623 0.10950457\n",
      "2018-04-17 18:35:01 iteration 250/859: loss=0.046581, accuracy=99.060% -0.0057673794 0.00963396 0.042714514\n",
      "2018-04-17 18:35:20 iteration 300/859: loss=0.010267, accuracy=99.695% -0.0074157296 0.0035737108 0.014109066\n",
      "2018-04-17 18:35:38 iteration 350/859: loss=0.051543, accuracy=98.694% -0.0038791033 0.012810221 0.04261201\n",
      "2018-04-17 18:35:56 iteration 400/859: loss=0.031748, accuracy=99.158% -0.0054128887 0.008543765 0.028617106\n",
      "2018-04-17 18:36:14 iteration 450/859: loss=0.058050, accuracy=98.413% -0.0027768484 0.01548669 0.045340084\n",
      "2018-04-17 18:36:32 iteration 500/859: loss=0.038027, accuracy=98.999% -0.0062440448 0.010174355 0.03409671\n",
      "2018-04-17 18:36:50 iteration 550/859: loss=0.126868, accuracy=96.826% 0.0036624472 0.030923927 0.09228135\n",
      "2018-04-17 18:37:08 iteration 600/859: loss=0.064790, accuracy=98.206% -0.0015798475 0.01734355 0.049025908\n",
      "2018-04-17 18:37:26 iteration 650/859: loss=0.082872, accuracy=97.900% -0.0017503552 0.020286633 0.06433527\n",
      "2018-04-17 18:37:44 iteration 700/859: loss=0.073903, accuracy=98.096% -0.002580506 0.018002003 0.0584814\n",
      "2018-04-17 18:38:03 iteration 750/859: loss=0.152732, accuracy=96.680% 0.0028334172 0.032010682 0.11788793\n",
      "2018-04-17 18:38:21 iteration 800/859: loss=0.017055, accuracy=99.573% -0.0061324663 0.004625222 0.018562067\n",
      "2018-04-17 18:38:39 iteration 850/859: loss=0.080512, accuracy=98.071% -0.001481721 0.019175183 0.062818415\n",
      "2018-04-17 18:38:42 iteration 859/859: loss=0.085111, accuracy=97.827% -6.59663e-05 0.020656776 0.064520195\n",
      "2018-04-17 18:38:53 end epoch 40/50: acc_train=97.953% acc_val=97.478% acc_test=97.865%\n",
      "2018-04-17 18:38:53 start epoch 41/50\n",
      "2018-04-17 18:38:54 iteration 1/859: loss=0.022240, accuracy=99.329% -0.0074956315 0.0070165037 0.022719586\n",
      "2018-04-17 18:39:12 iteration 50/859: loss=0.148517, accuracy=96.411% 0.006521286 0.03467095 0.10732506\n",
      "2018-04-17 18:39:31 iteration 100/859: loss=0.092666, accuracy=97.290% 0.0021475325 0.02585629 0.064662024\n",
      "2018-04-17 18:39:49 iteration 150/859: loss=0.047107, accuracy=98.865% -0.004826839 0.01133413 0.040599987\n",
      "2018-04-17 18:40:07 iteration 200/859: loss=0.070791, accuracy=98.035% -0.0018559195 0.019005612 0.05364153\n",
      "2018-04-17 18:40:25 iteration 250/859: loss=0.216446, accuracy=95.374% 0.008314917 0.043079324 0.16505176\n",
      "2018-04-17 18:40:43 iteration 300/859: loss=0.111718, accuracy=97.815% -0.0011587264 0.02132812 0.09154893\n",
      "2018-04-17 18:41:01 iteration 350/859: loss=0.028636, accuracy=99.133% -0.006046587 0.008775903 0.025907036\n",
      "2018-04-17 18:41:20 iteration 400/859: loss=0.169237, accuracy=96.021% 0.00628853 0.037864912 0.1250838\n",
      "2018-04-17 18:41:38 iteration 450/859: loss=0.041692, accuracy=98.865% -0.0049084504 0.011450223 0.035150256\n",
      "2018-04-17 18:41:57 iteration 500/859: loss=0.068551, accuracy=98.022% -0.00075467874 0.019298146 0.050007563\n",
      "2018-04-17 18:42:15 iteration 550/859: loss=0.024600, accuracy=99.280% -0.0053452672 0.0073682955 0.022577196\n",
      "2018-04-17 18:42:33 iteration 600/859: loss=0.075940, accuracy=98.035% -0.0019633304 0.019168153 0.058735315\n",
      "2018-04-17 18:42:53 iteration 650/859: loss=0.185472, accuracy=95.667% 0.008110292 0.04080686 0.13655454\n",
      "2018-04-17 18:43:11 iteration 700/859: loss=0.091025, accuracy=98.145% 1.821064e-05 0.018035684 0.07297083\n",
      "2018-04-17 18:43:29 iteration 750/859: loss=0.029806, accuracy=99.268% -0.007048829 0.007649365 0.029205142\n",
      "2018-04-17 18:43:47 iteration 800/859: loss=0.026610, accuracy=99.341% -0.0081419675 0.0071659833 0.027585598\n",
      "2018-04-17 18:44:06 iteration 850/859: loss=0.104438, accuracy=97.314% 0.0023743804 0.02588132 0.07618259\n",
      "2018-04-17 18:44:09 iteration 859/859: loss=0.063229, accuracy=98.315% -0.0036708359 0.016518395 0.05038141\n",
      "2018-04-17 18:44:19 end epoch 41/50: acc_train=98.303% acc_val=98.013% acc_test=98.083%\n",
      "2018-04-17 18:44:19 start epoch 42/50\n",
      "2018-04-17 18:44:19 iteration 1/859: loss=0.154602, accuracy=96.167% 0.0031049203 0.03685638 0.11464045\n",
      "2018-04-17 18:44:38 iteration 50/859: loss=0.037710, accuracy=99.011% -0.0033272237 0.010009574 0.031027414\n",
      "2018-04-17 18:44:56 iteration 100/859: loss=0.029856, accuracy=99.133% -0.006339547 0.008839053 0.02735643\n",
      "2018-04-17 18:45:15 iteration 150/859: loss=0.156664, accuracy=97.095% 0.0024346695 0.028396148 0.12583297\n",
      "2018-04-17 18:45:33 iteration 200/859: loss=0.098182, accuracy=97.278% 0.0017903566 0.026305575 0.070085585\n",
      "2018-04-17 18:45:51 iteration 250/859: loss=0.008401, accuracy=99.744% -0.006784156 0.0029814453 0.012203675\n",
      "2018-04-17 18:46:09 iteration 300/859: loss=0.105730, accuracy=96.948% 0.0011804432 0.029336482 0.07521263\n",
      "2018-04-17 18:46:27 iteration 350/859: loss=0.027144, accuracy=99.243% -0.0052710613 0.007714738 0.024699906\n",
      "2018-04-17 18:46:45 iteration 400/859: loss=0.084061, accuracy=98.022% 0.00031861718 0.019385837 0.06435696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 18:47:04 iteration 450/859: loss=0.051471, accuracy=98.682% -0.0055657574 0.013199928 0.043837253\n",
      "2018-04-17 18:47:22 iteration 500/859: loss=0.098205, accuracy=97.388% 0.0005796623 0.024826536 0.07279885\n",
      "2018-04-17 18:47:41 iteration 550/859: loss=0.016223, accuracy=99.561% -0.006390025 0.004792548 0.017820043\n",
      "2018-04-17 18:47:59 iteration 600/859: loss=0.035442, accuracy=99.109% -0.0052144886 0.009104947 0.031551704\n",
      "2018-04-17 18:48:17 iteration 650/859: loss=0.132077, accuracy=96.326% 0.0043703415 0.03510364 0.092602864\n",
      "2018-04-17 18:48:35 iteration 700/859: loss=0.098174, accuracy=97.278% 0.0029200993 0.026288068 0.06896628\n",
      "2018-04-17 18:48:54 iteration 750/859: loss=0.074032, accuracy=97.974% -0.001051511 0.01922197 0.055861674\n",
      "2018-04-17 18:49:12 iteration 800/859: loss=0.044305, accuracy=98.865% -0.0048658326 0.011248166 0.037922964\n",
      "2018-04-17 18:49:30 iteration 850/859: loss=0.078342, accuracy=97.913% -0.0012072844 0.02048263 0.059066325\n",
      "2018-04-17 18:49:33 iteration 859/859: loss=0.014349, accuracy=99.622% -0.0068547456 0.0042488542 0.016955081\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 18:49:46 end epoch 42/50: acc_train=98.100% acc_val=98.125% acc_test=97.948%\n",
      "2018-04-17 18:49:46 start epoch 43/50\n",
      "2018-04-17 18:49:46 iteration 1/859: loss=0.128383, accuracy=97.339% -0.00060344324 0.025877679 0.10310906\n",
      "2018-04-17 18:50:04 iteration 50/859: loss=0.063521, accuracy=98.254% -0.0016641729 0.01720785 0.047977142\n",
      "2018-04-17 18:50:22 iteration 100/859: loss=0.083237, accuracy=97.827% -0.0019884768 0.021014014 0.06421164\n",
      "2018-04-17 18:50:40 iteration 150/859: loss=0.024968, accuracy=99.280% -0.0054763705 0.007378281 0.023066238\n",
      "2018-04-17 18:50:59 iteration 200/859: loss=0.064458, accuracy=98.120% -0.0027016164 0.018735368 0.048424236\n",
      "2018-04-17 18:51:17 iteration 250/859: loss=0.037491, accuracy=98.999% -0.005395123 0.010130045 0.032755774\n",
      "2018-04-17 18:51:35 iteration 300/859: loss=0.030716, accuracy=99.170% -0.006270671 0.008448123 0.028538309\n",
      "2018-04-17 18:51:54 iteration 350/859: loss=0.047722, accuracy=98.767% -0.0036599934 0.012301447 0.03908073\n",
      "2018-04-17 18:52:12 iteration 400/859: loss=0.070675, accuracy=98.071% -0.0010548921 0.019060493 0.052669547\n",
      "2018-04-17 18:52:31 iteration 450/859: loss=0.107792, accuracy=98.267% -0.0019385184 0.01707982 0.09265071\n",
      "2018-04-17 18:52:49 iteration 500/859: loss=0.161260, accuracy=95.850% 0.006493152 0.03919319 0.115573786\n",
      "2018-04-17 18:53:09 iteration 550/859: loss=0.031022, accuracy=99.048% -0.006146391 0.009670999 0.027497506\n",
      "2018-04-17 18:53:27 iteration 600/859: loss=0.139517, accuracy=96.594% 0.004995185 0.03262206 0.10189969\n",
      "2018-04-17 18:53:45 iteration 650/859: loss=0.134655, accuracy=97.388% 0.0012098416 0.02526781 0.10817741\n",
      "2018-04-17 18:54:04 iteration 700/859: loss=0.066109, accuracy=98.328% -0.003856094 0.016372543 0.05359263\n",
      "2018-04-17 18:54:22 iteration 750/859: loss=0.016134, accuracy=99.585% -0.0071858354 0.0046689166 0.018650819\n",
      "2018-04-17 18:54:40 iteration 800/859: loss=0.033014, accuracy=99.133% -0.005539046 0.008805571 0.029747743\n",
      "2018-04-17 18:54:58 iteration 850/859: loss=0.019339, accuracy=99.365% -0.004394035 0.0063659297 0.017367067\n",
      "2018-04-17 18:55:02 iteration 859/859: loss=0.021587, accuracy=99.500% -0.0048626075 0.00524241 0.02120671\n",
      "2018-04-17 18:55:12 end epoch 43/50: acc_train=98.119% acc_val=97.701% acc_test=97.760%\n",
      "2018-04-17 18:55:12 start epoch 44/50\n",
      "2018-04-17 18:55:13 iteration 1/859: loss=0.070107, accuracy=98.108% 0.0001068687 0.018235004 0.05176477\n",
      "2018-04-17 18:55:31 iteration 50/859: loss=0.034605, accuracy=99.011% -0.005037629 0.010253998 0.02938882\n",
      "2018-04-17 18:55:49 iteration 100/859: loss=0.084323, accuracy=97.815% -0.0015333798 0.02110737 0.06474924\n",
      "2018-04-17 18:56:08 iteration 150/859: loss=0.113823, accuracy=97.351% 0.0018190593 0.025456306 0.08654773\n",
      "2018-04-17 18:56:26 iteration 200/859: loss=0.034423, accuracy=98.950% -0.0045971973 0.010549992 0.028470468\n",
      "2018-04-17 18:56:44 iteration 250/859: loss=0.053940, accuracy=98.608% -0.0038764079 0.013612384 0.04420366\n",
      "2018-04-17 18:57:02 iteration 300/859: loss=0.048846, accuracy=98.669% -0.0028024132 0.013143879 0.038504444\n",
      "2018-04-17 18:57:21 iteration 350/859: loss=0.062207, accuracy=98.376% -0.00047982103 0.0157899 0.04689651\n",
      "2018-04-17 18:57:39 iteration 400/859: loss=0.045483, accuracy=98.914% -0.0026163217 0.0107753575 0.03732443\n",
      "2018-04-17 18:57:58 iteration 450/859: loss=0.032098, accuracy=99.194% -0.006093882 0.008193748 0.029997762\n",
      "2018-04-17 18:58:16 iteration 500/859: loss=0.006912, accuracy=99.780% -0.006201201 0.0025878265 0.010525726\n",
      "2018-04-17 18:58:34 iteration 550/859: loss=0.016526, accuracy=99.524% -0.0049197148 0.0050786473 0.01636702\n",
      "2018-04-17 18:58:52 iteration 600/859: loss=0.028495, accuracy=99.243% -0.006088952 0.007841666 0.026742436\n",
      "2018-04-17 18:59:11 iteration 650/859: loss=0.102418, accuracy=98.108% 0.00020839616 0.018499788 0.08370994\n",
      "2018-04-17 18:59:29 iteration 700/859: loss=0.078424, accuracy=97.791% 0.0008181065 0.021618731 0.055986915\n",
      "2018-04-17 18:59:48 iteration 750/859: loss=0.064589, accuracy=98.181% -0.00016579978 0.01756296 0.04719198\n",
      "2018-04-17 19:00:06 iteration 800/859: loss=0.062224, accuracy=98.230% -0.00064699876 0.017321896 0.04554934\n",
      "2018-04-17 19:00:24 iteration 850/859: loss=0.116351, accuracy=97.388% 0.0017867954 0.024864906 0.08969952\n",
      "2018-04-17 19:00:28 iteration 859/859: loss=0.079258, accuracy=97.913% -0.0011356944 0.020304162 0.060089774\n",
      "2018-04-17 19:00:39 end epoch 44/50: acc_train=97.919% acc_val=97.991% acc_test=97.865%\n",
      "2018-04-17 19:00:39 start epoch 45/50\n",
      "2018-04-17 19:00:39 iteration 1/859: loss=0.240333, accuracy=95.276% 0.00879215 0.04461683 0.18692422\n",
      "2018-04-17 19:00:58 iteration 50/859: loss=0.014689, accuracy=99.634% -0.0063869343 0.003996037 0.017080083\n",
      "2018-04-17 19:01:16 iteration 100/859: loss=0.021056, accuracy=99.512% -0.0045593367 0.0050897887 0.020525988\n",
      "2018-04-17 19:01:34 iteration 150/859: loss=0.106558, accuracy=97.009% 0.004057276 0.028575089 0.07392575\n",
      "2018-04-17 19:01:52 iteration 200/859: loss=0.163261, accuracy=95.959% 0.007347078 0.03761505 0.11829892\n",
      "2018-04-17 19:02:10 iteration 250/859: loss=0.094730, accuracy=97.498% 0.0026998466 0.024135217 0.067894995\n",
      "2018-04-17 19:02:29 iteration 300/859: loss=0.027267, accuracy=99.133% -0.004139918 0.008756939 0.022650467\n",
      "2018-04-17 19:02:47 iteration 350/859: loss=0.077261, accuracy=97.827% -0.0003369258 0.020779388 0.056818258\n",
      "2018-04-17 19:03:06 iteration 400/859: loss=0.068257, accuracy=98.022% -0.000421377 0.01875335 0.049925067\n",
      "2018-04-17 19:03:25 iteration 450/859: loss=0.092604, accuracy=97.839% -0.0006676782 0.021144826 0.07212677\n",
      "2018-04-17 19:03:43 iteration 500/859: loss=0.039042, accuracy=98.987% -0.004416308 0.010306131 0.033152573\n",
      "2018-04-17 19:04:01 iteration 550/859: loss=0.083513, accuracy=97.729% 0.00022444548 0.02230021 0.0609881\n",
      "2018-04-17 19:04:20 iteration 600/859: loss=0.041470, accuracy=98.901% -0.0033420187 0.0108588915 0.033953354\n",
      "2018-04-17 19:04:39 iteration 650/859: loss=0.077779, accuracy=97.888% 0.001038728 0.020043436 0.05669706\n",
      "2018-04-17 19:04:57 iteration 700/859: loss=0.028586, accuracy=99.194% -0.0045332904 0.00815458 0.024964385\n",
      "2018-04-17 19:05:16 iteration 750/859: loss=0.006156, accuracy=99.780% -0.007091074 0.002701871 0.010545658\n",
      "2018-04-17 19:05:34 iteration 800/859: loss=0.058334, accuracy=98.340% -0.0026066483 0.016248168 0.044692144\n",
      "2018-04-17 19:05:52 iteration 850/859: loss=0.084907, accuracy=97.668% 0.00051870366 0.022402126 0.061985802\n",
      "2018-04-17 19:05:55 iteration 859/859: loss=0.069329, accuracy=98.242% -0.0009961195 0.017190792 0.053133946\n",
      "2018-04-17 19:06:06 end epoch 45/50: acc_train=98.288% acc_val=98.103% acc_test=98.312%\n",
      "2018-04-17 19:06:06 start epoch 46/50\n",
      "2018-04-17 19:06:07 iteration 1/859: loss=0.061886, accuracy=98.230% -0.0024393138 0.016979879 0.0473456\n",
      "2018-04-17 19:06:25 iteration 50/859: loss=0.033248, accuracy=99.072% -0.005171868 0.009249576 0.029170107\n",
      "2018-04-17 19:06:43 iteration 100/859: loss=0.033766, accuracy=99.060% -0.0016732246 0.009350714 0.026088946\n",
      "2018-04-17 19:07:01 iteration 150/859: loss=0.153378, accuracy=96.802% 0.0036230641 0.029871156 0.119883776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17 19:07:20 iteration 200/859: loss=0.063862, accuracy=98.291% -0.0021998163 0.016769964 0.049292244\n",
      "2018-04-17 19:07:38 iteration 250/859: loss=0.150656, accuracy=96.472% 0.005088339 0.03357551 0.11199214\n",
      "2018-04-17 19:07:56 iteration 300/859: loss=0.022119, accuracy=99.365% -0.0026848598 0.006426192 0.018377531\n",
      "2018-04-17 19:08:15 iteration 350/859: loss=0.025684, accuracy=99.280% -0.0045591705 0.007174374 0.023068676\n",
      "2018-04-17 19:08:33 iteration 400/859: loss=0.081141, accuracy=97.888% -6.155996e-06 0.020478653 0.060668007\n",
      "2018-04-17 19:08:51 iteration 450/859: loss=0.079722, accuracy=97.717% 0.0007062127 0.022248188 0.056767363\n",
      "2018-04-17 19:09:09 iteration 500/859: loss=0.102876, accuracy=97.803% -0.00078705215 0.021464394 0.08219904\n",
      "2018-04-17 19:09:28 iteration 550/859: loss=0.056408, accuracy=98.535% -0.0038389072 0.014435617 0.04581098\n",
      "2018-04-17 19:09:46 iteration 600/859: loss=0.137066, accuracy=96.533% 0.006566502 0.033209763 0.097290196\n",
      "2018-04-17 19:10:04 iteration 650/859: loss=0.014337, accuracy=99.597% -0.004492843 0.004312653 0.014517421\n",
      "2018-04-17 19:10:23 iteration 700/859: loss=0.038447, accuracy=99.060% -0.0049922685 0.009674647 0.033764273\n",
      "2018-04-17 19:10:42 iteration 750/859: loss=0.048355, accuracy=98.645% -0.0034637568 0.013314279 0.03850489\n",
      "2018-04-17 19:11:00 iteration 800/859: loss=0.042090, accuracy=98.804% -0.0032362088 0.011838675 0.033487834\n",
      "2018-04-17 19:11:19 iteration 850/859: loss=0.085258, accuracy=97.888% -0.0008468501 0.020657327 0.06544705\n",
      "2018-04-17 19:11:22 iteration 859/859: loss=0.024866, accuracy=99.329% -0.00308043 0.006800973 0.021145666\n",
      "2018-04-17 19:11:32 end epoch 46/50: acc_train=98.284% acc_val=97.634% acc_test=98.083%\n",
      "2018-04-17 19:11:32 start epoch 47/50\n",
      "2018-04-17 19:11:33 iteration 1/859: loss=0.055252, accuracy=98.511% -0.0023652944 0.014598104 0.043019526\n",
      "2018-04-17 19:11:52 iteration 50/859: loss=0.082837, accuracy=97.754% -0.0004193235 0.0218145 0.061441988\n",
      "2018-04-17 19:12:10 iteration 100/859: loss=0.126913, accuracy=97.876% -9.013282e-05 0.020625932 0.10637702\n",
      "2018-04-17 19:12:28 iteration 150/859: loss=0.020637, accuracy=99.390% -0.0044851014 0.0063159787 0.018806318\n",
      "2018-04-17 19:12:47 iteration 200/859: loss=0.059656, accuracy=98.389% -0.0028893228 0.015809603 0.04673603\n",
      "2018-04-17 19:13:06 iteration 250/859: loss=0.042609, accuracy=98.889% -0.0031336707 0.011005813 0.0347372\n",
      "2018-04-17 19:13:25 iteration 300/859: loss=0.149878, accuracy=96.826% 0.0035878986 0.030618051 0.11567165\n",
      "2018-04-17 19:13:43 iteration 350/859: loss=0.031949, accuracy=99.182% -0.0043538534 0.008231783 0.028071392\n",
      "2018-04-17 19:14:02 iteration 400/859: loss=0.070267, accuracy=97.998% -0.0009457942 0.019120518 0.052091792\n",
      "2018-04-17 19:14:21 iteration 450/859: loss=0.086554, accuracy=97.803% -0.0005328483 0.021274056 0.06581277\n",
      "2018-04-17 19:14:39 iteration 500/859: loss=0.051853, accuracy=98.682% -0.0036036472 0.013144265 0.042311974\n",
      "2018-04-17 19:14:57 iteration 550/859: loss=0.039163, accuracy=99.011% -0.0032308255 0.009853049 0.032540523\n",
      "2018-04-17 19:15:15 iteration 600/859: loss=0.091967, accuracy=97.668% 0.0027523944 0.022415753 0.06679935\n",
      "2018-04-17 19:15:34 iteration 650/859: loss=0.085820, accuracy=97.900% -0.0006597802 0.020134473 0.066345125\n",
      "2018-04-17 19:15:52 iteration 700/859: loss=0.015491, accuracy=99.561% -0.007073785 0.0047137174 0.017851427\n",
      "2018-04-17 19:16:10 iteration 750/859: loss=0.109331, accuracy=97.217% 0.00510948 0.026972488 0.077249035\n",
      "2018-04-17 19:16:28 iteration 800/859: loss=0.009787, accuracy=99.719% -0.005751066 0.003129144 0.012408437\n",
      "2018-04-17 19:16:47 iteration 850/859: loss=0.103549, accuracy=97.229% 0.004689298 0.026954561 0.071905494\n",
      "2018-04-17 19:16:50 iteration 859/859: loss=0.044092, accuracy=98.914% -0.0021358342 0.010726455 0.035501353\n",
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 19:17:01 end epoch 47/50: acc_train=98.322% acc_val=98.237% acc_test=98.198%\n",
      "2018-04-17 19:17:01 start epoch 48/50\n",
      "2018-04-17 19:17:02 iteration 1/859: loss=0.009382, accuracy=99.780% -0.0051770164 0.002480193 0.0120787695\n",
      "2018-04-17 19:17:20 iteration 50/859: loss=0.186305, accuracy=95.740% 0.0063189166 0.04021269 0.13977325\n",
      "2018-04-17 19:17:39 iteration 100/859: loss=0.061321, accuracy=98.389% 0.00086871843 0.015794942 0.044657584\n",
      "2018-04-17 19:17:57 iteration 150/859: loss=0.050954, accuracy=98.535% -0.0016638081 0.014416392 0.03820135\n",
      "2018-04-17 19:18:16 iteration 200/859: loss=0.030547, accuracy=99.255% -0.004701636 0.0075200796 0.027728599\n",
      "2018-04-17 19:18:34 iteration 250/859: loss=0.041079, accuracy=98.853% -0.0036769756 0.011534244 0.033221826\n",
      "2018-04-17 19:18:52 iteration 300/859: loss=0.036463, accuracy=99.060% -0.0026446194 0.009354915 0.029752217\n",
      "2018-04-17 19:19:10 iteration 350/859: loss=0.014340, accuracy=99.646% -0.0050544 0.0037771952 0.015617184\n",
      "2018-04-17 19:19:29 iteration 400/859: loss=0.085092, accuracy=97.766% 0.0016467 0.021764878 0.061680004\n",
      "2018-04-17 19:19:47 iteration 450/859: loss=0.090930, accuracy=97.729% 0.0017044988 0.021870023 0.06735526\n",
      "2018-04-17 19:20:05 iteration 500/859: loss=0.044799, accuracy=98.853% -0.005140522 0.011426425 0.038513027\n",
      "2018-04-17 19:20:23 iteration 550/859: loss=0.170041, accuracy=95.935% 0.0071690707 0.038433235 0.124438964\n",
      "2018-04-17 19:20:43 iteration 600/859: loss=0.052854, accuracy=98.596% -0.002720079 0.013714129 0.04185979\n",
      "2018-04-17 19:21:01 iteration 650/859: loss=0.048956, accuracy=98.596% -0.0015328355 0.013665271 0.036823735\n",
      "2018-04-17 19:21:19 iteration 700/859: loss=0.014482, accuracy=99.622% -0.0050528096 0.0040782145 0.015456533\n",
      "2018-04-17 19:21:38 iteration 750/859: loss=0.118035, accuracy=97.229% 0.0030808866 0.02686655 0.088087894\n",
      "2018-04-17 19:21:56 iteration 800/859: loss=0.012863, accuracy=99.658% -0.004080481 0.0035652653 0.013378285\n",
      "2018-04-17 19:22:14 iteration 850/859: loss=0.022543, accuracy=99.475% -0.0043236124 0.0053645354 0.02150186\n",
      "2018-04-17 19:22:17 iteration 859/859: loss=0.135409, accuracy=96.765% 0.0046484084 0.03146594 0.099294946\n",
      "2018-04-17 19:22:27 end epoch 48/50: acc_train=98.091% acc_val=98.013% acc_test=97.990%\n",
      "2018-04-17 19:22:27 start epoch 49/50\n",
      "2018-04-17 19:22:28 iteration 1/859: loss=0.057483, accuracy=98.438% -0.0011253776 0.015430167 0.043178637\n",
      "2018-04-17 19:22:45 iteration 50/859: loss=0.107014, accuracy=96.997% 0.0041106725 0.028922036 0.07398106\n",
      "2018-04-17 19:23:05 iteration 100/859: loss=0.021214, accuracy=99.500% -0.0057036253 0.0053388374 0.021578524\n",
      "2018-04-17 19:23:24 iteration 150/859: loss=0.148982, accuracy=97.083% 0.004655431 0.02827456 0.116051875\n",
      "2018-04-17 19:23:42 iteration 200/859: loss=0.081795, accuracy=97.974% 0.0006525635 0.019491244 0.061650842\n",
      "2018-04-17 19:24:00 iteration 250/859: loss=0.083305, accuracy=97.754% -0.0015867002 0.021881152 0.06301078\n",
      "2018-04-17 19:24:18 iteration 300/859: loss=0.096128, accuracy=97.485% 0.0018422309 0.02431485 0.06997043\n",
      "2018-04-17 19:24:37 iteration 350/859: loss=0.119494, accuracy=97.363% 0.003018917 0.025591249 0.09088425\n",
      "2018-04-17 19:24:55 iteration 400/859: loss=0.089179, accuracy=97.766% -0.00076823647 0.021601941 0.06834572\n",
      "2018-04-17 19:25:14 iteration 450/859: loss=0.066531, accuracy=98.120% -0.00020448168 0.01846321 0.04827181\n",
      "2018-04-17 19:25:32 iteration 500/859: loss=0.028220, accuracy=99.255% -0.0048405803 0.007582006 0.025478935\n",
      "2018-04-17 19:25:51 iteration 550/859: loss=0.058465, accuracy=98.364% -0.0005999507 0.015684564 0.0433807\n",
      "2018-04-17 19:26:09 iteration 600/859: loss=0.027117, accuracy=99.231% -0.0033822511 0.007833811 0.022665197\n",
      "2018-04-17 19:26:28 iteration 650/859: loss=0.109960, accuracy=98.035% 0.0013995157 0.019282099 0.08927815\n",
      "2018-04-17 19:26:46 iteration 700/859: loss=0.212621, accuracy=95.825% 0.007895874 0.04015889 0.16456589\n",
      "2018-04-17 19:27:04 iteration 750/859: loss=0.048080, accuracy=98.718% -0.0021121944 0.0126917465 0.037500158\n",
      "2018-04-17 19:27:22 iteration 800/859: loss=0.037578, accuracy=98.950% -0.0026121337 0.0103120245 0.029877977\n",
      "2018-04-17 19:27:41 iteration 850/859: loss=0.129823, accuracy=96.716% 0.004273299 0.031861313 0.09368819\n",
      "2018-04-17 19:27:44 iteration 859/859: loss=0.131316, accuracy=97.961% 0.00031025158 0.019901898 0.111103736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently maximum accuracy on validation set, model saved in path: model/RAM/RAM.ckpt\n",
      "2018-04-17 19:27:56 end epoch 49/50: acc_train=98.356% acc_val=98.371% acc_test=98.219%\n",
      "2018-04-17 19:27:56 start epoch 50/50\n",
      "2018-04-17 19:27:56 iteration 1/859: loss=0.061641, accuracy=98.291% -0.0010041995 0.016382977 0.0462623\n",
      "2018-04-17 19:28:14 iteration 50/859: loss=0.026769, accuracy=99.329% -0.004373179 0.0068955496 0.024246445\n",
      "2018-04-17 19:28:33 iteration 100/859: loss=0.189541, accuracy=95.898% 0.00923934 0.039353956 0.1409479\n",
      "2018-04-17 19:28:51 iteration 150/859: loss=0.140295, accuracy=96.423% 0.0063244463 0.03468219 0.09928829\n",
      "2018-04-17 19:29:09 iteration 200/859: loss=0.021219, accuracy=99.438% -0.0044347383 0.00585267 0.019801373\n",
      "2018-04-17 19:29:27 iteration 250/859: loss=0.101203, accuracy=97.217% 0.003770433 0.026413357 0.0710192\n",
      "2018-04-17 19:29:46 iteration 300/859: loss=0.020935, accuracy=99.451% -0.004236947 0.0056339214 0.019538473\n",
      "2018-04-17 19:30:04 iteration 350/859: loss=0.126049, accuracy=96.826% 0.0060133645 0.030451125 0.08958461\n",
      "2018-04-17 19:30:22 iteration 400/859: loss=0.023568, accuracy=99.390% -0.0037089274 0.0062432913 0.021033764\n",
      "2018-04-17 19:30:41 iteration 450/859: loss=0.058526, accuracy=98.352% 0.00022521331 0.016010383 0.042290423\n",
      "2018-04-17 19:31:00 iteration 500/859: loss=0.046768, accuracy=98.743% -0.0015979838 0.012317726 0.036048718\n",
      "2018-04-17 19:31:18 iteration 550/859: loss=0.025284, accuracy=99.280% -0.004255157 0.007365516 0.022173913\n",
      "2018-04-17 19:31:36 iteration 600/859: loss=0.020284, accuracy=99.500% -0.004938885 0.0052859634 0.019937165\n",
      "2018-04-17 19:31:55 iteration 650/859: loss=0.049696, accuracy=98.682% -0.0006675775 0.0130173685 0.03734654\n",
      "2018-04-17 19:32:13 iteration 700/859: loss=0.136518, accuracy=96.216% 0.005371673 0.036360275 0.094785765\n",
      "2018-04-17 19:32:32 iteration 750/859: loss=0.087496, accuracy=97.937% 0.0019221066 0.019833738 0.06574005\n",
      "2018-04-17 19:32:50 iteration 800/859: loss=0.010956, accuracy=99.670% -0.005513617 0.0035856888 0.012883494\n",
      "2018-04-17 19:33:08 iteration 850/859: loss=0.034385, accuracy=99.072% -0.004335114 0.0091507025 0.029569604\n",
      "2018-04-17 19:33:11 iteration 859/859: loss=0.014591, accuracy=99.609% -0.0066044875 0.004214267 0.01698134\n",
      "2018-04-17 19:33:22 end epoch 50/50: acc_train=98.184% acc_val=98.371% acc_test=98.250%\n"
     ]
    }
   ],
   "source": [
    "acc_train_his=[]\n",
    "acc_val_his=[]\n",
    "acc_test_his=[]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     print(tf.global_variables())\n",
    "    tf.global_variables_initializer().run()\n",
    "    max_acc=None\n",
    "    for epoch in range(max_epoch):\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'start epoch %d/%d' % (epoch+1,max_epoch))\n",
    "        train()\n",
    "        loss_train,acc_train=eval(mnist.train,500)\n",
    "        loss_val,acc_val=eval(mnist.validation,70)\n",
    "        loss_test,acc_test=eval(mnist.test,150)\n",
    "        acc_train_his.append(acc_train)\n",
    "        acc_val_his.append(acc_val)\n",
    "        acc_test_his.append(acc_test)\n",
    "        \n",
    "        if max_acc==None or acc_val>max_acc:\n",
    "            max_acc=acc_val\n",
    "            save_path = saver.save(sess, \"model/RAM/RAM.ckpt\")\n",
    "            print(\"Currently maximum accuracy on validation set, model saved in path: %s\" % save_path)\n",
    "        \n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())),'end epoch %d/%d:' % (epoch+1,max_epoch),\n",
    "             'acc_train=%.3f%% acc_val=%.3f%% acc_test=%.3f%%' % (acc_train*100.0,acc_val*100.0,acc_test*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(2)\n",
    "ptr,=plt.plot(range(max_epoch),acc_train_his,'r-')\n",
    "pva,=plt.plot(range(max_epoch),acc_val_his,'b-')\n",
    "pte,=plt.plot(range(max_epoch),acc_test_his,'y-')\n",
    "plt.xlabel('training epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('accuracy on three sets')\n",
    "plt.legend((ptr,pva,pte),('train','validation','test'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/RAM/RAM.ckpt\n",
      "Accuracy on training set is 98.474%\n",
      "Accuracy on validation set is 98.017%\n",
      "Accuracy on testing set is 98.107%\n"
     ]
    }
   ],
   "source": [
    "saver=tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, \"model/RAM/RAM.ckpt\")\n",
    "    _,acc_train=eval(mnist.train,num_train//batch_size)\n",
    "    _,acc_val=eval(mnist.validation,num_val//batch_size)\n",
    "    _,acc_test=eval(mnist.test,num_test//batch_size)\n",
    "    print('Accuracy on training set is %.3f%%' % (acc_train*100.0))\n",
    "    print('Accuracy on validation set is %.3f%%' % (acc_val*100.0))\n",
    "    print('Accuracy on testing set is %.3f%%' % (acc_test*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean=tf.zeros((100,2),dtype=tf.float32)\n",
    "# std=tf.constant([1,1],dtype=tf.float32)\n",
    "# gaussian=tf.distributions.Normal(loc=mean,scale=std)\n",
    "# rand=tf.random_normal(shape=(100,2),mean=0,stddev=1)\n",
    "# sampled=mean+rand\n",
    "# prob=-gaussian.log_prob(sampled)\n",
    "# prob=tf.reduce_mean(tf.reduce_sum(prob,1))\n",
    "# with tf.Session() as sess:\n",
    "#     out=sess.run([prob])\n",
    "#     print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
